{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    \n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_subsumptions(filename):\n",
    "    subsumptions = []\n",
    "\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            subsumptions.append((row[0], row[1]))\n",
    "\n",
    "    return subsumptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_train = read_subsumptions('subsumptions-train.txt.orig')\n",
    "train = read_subsumptions('subsumptions-train.txt')\n",
    "orig_test = read_subsumptions('subsumptions-test.txt.orig')\n",
    "orig_validation = read_subsumptions('subsumptions-validation.txt.orig')\n",
    "\n",
    "all_train = read_subsumptions('all_hyper_relations.tsv')\n",
    "all_train = list(set(all_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9762 4374\n"
     ]
    }
   ],
   "source": [
    "print len(train), len(orig_train)\n",
    "#print len([(x,y)for x, y in train if x == 'alcohol'])\n",
    "#print [(x,y)for x, y in orig_train if x == 'alcohol']\n",
    "\n",
    "orig_d = defaultdict(list)\n",
    "train_d = defaultdict(list)\n",
    "orig_test_d = defaultdict(list)\n",
    "all_train_d = defaultdict(list)\n",
    "orig_valid_d = defaultdict(list)\n",
    "\n",
    "for x, y in orig_train:\n",
    "    orig_d[x].append(y)\n",
    "\n",
    "for x, y in train:\n",
    "    train_d[x].append(y)\n",
    "    \n",
    "for x, y in orig_test:\n",
    "    orig_test_d[x].append(y)\n",
    "\n",
    "for x, y in all_train:\n",
    "    all_train_d[x].append(y)\n",
    "    \n",
    "for x, y in orig_validation:\n",
    "    orig_valid_d[x].append(y)    \n",
    "\n",
    "# convert to read-only use now    \n",
    "for dic in [orig_d, train_d, orig_test_d, all_train_d, orig_valid_d]:\n",
    "    dic.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409 1355\n",
      "1409\n",
      "1355\n"
     ]
    }
   ],
   "source": [
    "print len(train_d), len(orig_d)\n",
    "\n",
    "print len(train_d.keys())\n",
    "print len(orig_d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409\n",
      "1355\n"
     ]
    }
   ],
   "source": [
    "print len(set(train_d.keys()))\n",
    "print len(set(orig_d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['placental',\n",
       " 'mammal',\n",
       " 'animal',\n",
       " 'vertebrate',\n",
       " 'chordate',\n",
       " 'carnivore',\n",
       " 'canine',\n",
       " 'pet']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the original data is of much higher quality\n",
    "# for instance the hypernyms of dog in the original data set are:\n",
    "# placental, mammal, animal, vertebrate, chordate, carnivore, canine, pet\n",
    "\n",
    "# in the larger dataset the following inaccurate hypernyms are also included:\n",
    "# animals, pets, species, mammals, predator, thing, creature and predators\n",
    "orig_d['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['placental',\n",
       " 'mammal',\n",
       " 'animal',\n",
       " 'vertebrate',\n",
       " 'chordate',\n",
       " 'carnivore',\n",
       " 'canine',\n",
       " 'pet',\n",
       " 'animals',\n",
       " 'pets',\n",
       " 'species',\n",
       " 'mammals',\n",
       " 'predator',\n",
       " 'thing',\n",
       " 'creature',\n",
       " 'predators']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_d_keys = [x for x, y in train_d.iteritems() if len(y) > 0]\n",
    "orig_d_keys = [x for x, y in orig_d.iteritems() if len(y) > 0]\n",
    "\n",
    "new_terms = list(set(train_d_keys) - set(orig_d_keys))\n",
    "\n",
    "#for t in new_terms:\n",
    " #   print t, '\\t', \"\\t\".join(train_d[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n",
      "265\n"
     ]
    }
   ],
   "source": [
    "# replace hypernyms \n",
    "test_terms = list(set([x[0] for x in orig_test]))\n",
    "\n",
    "# we have 601 new test terms\n",
    "print len(new_terms)\n",
    "# ...of which 265 exist in the test set\n",
    "print len(set(train_d_keys).intersection(set(test_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "# since we need lexical split to avoid the lexical memorisation \n",
    "# problem we can only embellish our training set with 336 new terms\n",
    "# this should increase the training set by ~ 1140 training relations\n",
    "new_terms = list(set(new_terms)- set(test_terms))\n",
    "print len(new_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4374\n",
      "5527\n"
     ]
    }
   ],
   "source": [
    "# iterate over new_terms, look for relations in all_train_d\n",
    "# unpack list and add relation to orig_train\n",
    "print len(orig_train)\n",
    "for term in new_terms:    \n",
    "    try:\n",
    "        hypernyms = all_train_d[term]\n",
    "        train_tuples = zip([term] * len(hypernyms), hypernyms)\n",
    "        orig_train.extend(train_tuples)\n",
    "    except KeyError:    \n",
    "        print term, ' not found'\n",
    "        pass\n",
    "\n",
    "#all_train_d['flask']\n",
    "print len(orig_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally \n",
    "# subsumptions-more-train.txt\n",
    "with open('subsumptions-more-train.txt', 'wb') as csvfile:\n",
    "    train_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    for relation in orig_train:    \n",
    "        train_writer.writerow(relation)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('roebuck.n.01') - [u'roebuck']\n",
      "Hypernyms:  Synset('roe_deer.n.01') - [u'roe_deer', u'Capreolus_capreolus']\n",
      "[(Synset('ruminant.n.01'), 3), (Synset('deer.n.01'), 2), (Synset('roe_deer.n.01'), 1), (Synset('even-toed_ungulate.n.01'), 4)]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "term = 'roebuck'\n",
    "for syn in wn.synsets(term):\n",
    "    if syn.pos() == 'n':\n",
    "        print syn, '-', syn.lemma_names()\n",
    "        if term in syn.lemma_names():\n",
    "            for hyper in syn.hypernyms():        \n",
    "                print \"Hypernyms: \", hyper, '-', hyper.lemma_names()\n",
    "            print [(x,y) for x, y in syn.hypernym_distances() if 0 < y <= 4 ]\n",
    "            print \"-\"*30\n",
    "#[0].hypernyms()[0].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = wn.synset('caterpillar.n.01')\n",
    "#wn.synsets('partial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('larva.n.01')]\n",
      "set([(Synset('physical_entity.n.01'), 7), (Synset('animal.n.01'), 2), (Synset('object.n.01'), 6), (Synset('larva.n.01'), 1), (Synset('living_thing.n.01'), 4), (Synset('organism.n.01'), 3), (Synset('entity.n.01'), 8), (Synset('whole.n.02'), 5), (Synset('caterpillar.n.01'), 0)])\n"
     ]
    }
   ],
   "source": [
    "print a.hypernyms()\n",
    "print a.hypernym_distances()\n",
    "#[x for x, y in a.hypernym_distances() if y <= 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'whole', u'unit']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whole.n.02').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
