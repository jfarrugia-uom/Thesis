{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained UMBC vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = 'embeddings/1A_en_UMBC_tokenized.vectors.txt'\n",
    "model = KeyedVectors.load_word2vec_format(w2v, binary=False)\n",
    "# pre-compute L2 norms of vectors\\\\\\\\\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219523"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.index2word)\n",
    "\n",
    "# embeddings size is 219523 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'tailgating', 0.26962268352508545),\n",
       " (u'bleacher', 0.25520360469818115),\n",
       " (u'basketball_tournament', 0.2459111511707306),\n",
       " (u'board_meeting', 0.2328670769929886),\n",
       " (u'gridiron', 0.2320120930671692),\n",
       " (u'football_game', 0.22912628948688507),\n",
       " (u'grotto', 0.22740451991558075),\n",
       " (u'tailgater', 0.2264767587184906),\n",
       " (u'tailgate_party', 0.22612455487251282),\n",
       " (u'photo_opportunity', 0.22608546912670135),\n",
       " (u'pep_rally', 0.22473984956741333),\n",
       " (u'commencement_day', 0.2242063581943512),\n",
       " (u'hibachi', 0.21942704916000366),\n",
       " (u'bleachers', 0.21716344356536865),\n",
       " (u'stupe', 0.2161717265844345),\n",
       " (u'football_season', 0.21609929203987122),\n",
       " (u'pregame', 0.2155630886554718),\n",
       " (u'basketball_game', 0.21550601720809937),\n",
       " (u'pre-game', 0.21545638144016266),\n",
       " (u'predawn', 0.21508227288722992)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(u'igeoe', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no word is capitalised in model vocab\n",
    "assert len(list(filter(lambda k: k.istitle(), model.vocab.keys()))) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import codecs\n",
    "import io\n",
    "import os\n",
    "import unicodecsv as csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_subsumptions(filenames):\n",
    "    hypo, hyper = filenames\n",
    "        \n",
    "    data_list, gold_list, subsumptions = [], [], []\n",
    "    \n",
    "    # load data items\n",
    "    with open(hypo, mode='r') as f:        \n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        for row in reader:\n",
    "            data_list.append(row[0])\n",
    "            \n",
    "    with io.open(hyper, mode= 'r') as f:        \n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        for row in reader:            \n",
    "            gold_list.append(row)\n",
    "      \n",
    "    # make sure we have the same number of elements in each list\n",
    "    assert len(data_list) == len(gold_list)\n",
    "    \n",
    "    for data_item, gold_terms in zip(data_list, gold_list):\n",
    "        for gold_item in gold_terms:\n",
    "            data_item = data_item.replace(\" \", \"_\").lower()\n",
    "            gold_item = gold_item.replace(\" \", \"_\").lower()\n",
    "            subsumptions.append((data_item, gold_item))\n",
    "    \n",
    "    return subsumptions\n",
    "\n",
    "def read_vocab(filename):\n",
    "        \n",
    "    vocab = []    \n",
    "    # load data items\n",
    "    with open(filename, mode='r') as f:        \n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "        for row in reader:\n",
    "            vocab_item = row[0].replace(\" \", \"_\").lower()\n",
    "            vocab.append(vocab_item)\n",
    "                              \n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_names = list(map(lambda x: './data/shared_task/1A.english.%s.data.txt'%(x), ['trial', 'test', 'training']))\n",
    "gold_file_names = list(map(lambda x: './data/shared_task/1A.english.%s.gold.txt'%(x), ['trial', 'test', 'training']))\n",
    "vocab_file_name = './data/shared_task/1A.english.vocabulary.txt'\n",
    "\n",
    "file_names = zip(data_file_names, gold_file_names)\n",
    "# 0 = trial; 1 = test; 2 = train\n",
    "valid_subs = read_subsumptions(file_names[0])\n",
    "test_subs = read_subsumptions(file_names[1])\n",
    "train_subs = read_subsumptions(file_names[2])\n",
    "vocabulary = read_vocab(vocab_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensure that model vocab is greater than given vocab\n",
    "assert len(model.index2word) >= len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate training tuples for which no embedding exists\n",
    "from collections import Counter\n",
    "\n",
    "def get_terms_having_vectors(dataset):        \n",
    "    query, hyper = \\\n",
    "    zip(*[(q,h) for q, h in dataset \n",
    "          if q in model and h in model])\n",
    "    \n",
    "    return list(query), list(hyper)\n",
    "\n",
    "\n",
    "train_query, train_hyper = get_terms_having_vectors(train_subs)\n",
    "test_query, test_hyper = get_terms_having_vectors(test_subs)\n",
    "valid_query, valid_hyper = get_terms_having_vectors(valid_subs)\n",
    "vocab = list(filter(lambda w: w in model, vocabulary))\n",
    "\n",
    "assert len(train_query) == len(train_hyper)\n",
    "assert len(test_query) == len(test_hyper)\n",
    "assert len(valid_query) == len(valid_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'person', 310)\n",
      "(u'city', 63)\n",
      "(u'work_of_art', 44)\n",
      "(u'leader', 41)\n",
      "(u'picture', 40)\n",
      "(u'writer', 39)\n",
      "(u'natural_phenomenon', 37)\n",
      "(u'animal', 35)\n",
      "(u'movie', 34)\n",
      "(u'locale', 34)\n",
      "(u'film', 34)\n",
      "(u'technical_specification', 33)\n",
      "(u'moving-picture_show', 33)\n",
      "(u'state', 32)\n",
      "(u'moving_picture', 32)\n",
      "(u'motion_picture', 31)\n",
      "(u'constructed_structure', 31)\n",
      "(u'computer_software', 31)\n",
      "(u'software', 30)\n",
      "(u'software_package', 29)\n",
      "(u'show', 29)\n",
      "(u'plant', 29)\n",
      "(u'computer_programme', 29)\n",
      "(u'computer_program', 29)\n",
      "(u'sport', 28)\n",
      "(u'software_program', 28)\n",
      "(u'picture_show', 28)\n",
      "(u'phenomenon', 28)\n",
      "(u'country', 27)\n",
      "(u'computer_code', 27)\n",
      "(u'company', 27)\n",
      "(u'chief', 27)\n",
      "(u'physical_phenomenon', 26)\n",
      "(u'channel', 26)\n",
      "(u'transportation', 25)\n",
      "(u'town', 25)\n",
      "(u'movement', 25)\n",
      "(u'communication_medium', 25)\n",
      "(u'transmission_channel', 24)\n",
      "(u'medium', 24)\n",
      "(u'transport', 23)\n",
      "(u'social_event', 23)\n",
      "(u'public_building', 23)\n",
      "(u'move', 23)\n",
      "(u'function', 23)\n",
      "(u'politician', 22)\n",
      "(u'musical_work', 22)\n",
      "(u'mechanism', 22)\n",
      "(u'boss', 22)\n",
      "(u'travel', 21)\n",
      "(u'storage_medium', 21)\n",
      "(u'travelling', 20)\n",
      "(u'traveling', 20)\n",
      "(u'transmission', 20)\n",
      "(u'technology', 20)\n",
      "(u'specifications', 20)\n",
      "(u'sickness', 20)\n",
      "(u'piece_of_land', 20)\n",
      "(u'pic', 20)\n",
      "(u'figure', 20)\n",
      "(u'competitor', 20)\n",
      "(u'code', 20)\n",
      "(u'tv_show', 19)\n",
      "(u'tv_program', 19)\n",
      "(u'motion', 19)\n",
      "(u'means_of_transport', 19)\n",
      "(u'enterprise', 19)\n",
      "(u'coding_system', 19)\n",
      "(u'athlete', 19)\n",
      "(u'weather_condition', 18)\n",
      "(u'television_show', 18)\n",
      "(u'television_series', 18)\n",
      "(u'television_program', 18)\n",
      "(u'sportsperson', 18)\n",
      "(u'political_leader', 18)\n",
      "(u'passenger_transport', 18)\n",
      "(u'parcel_of_land', 18)\n",
      "(u'map', 18)\n",
      "(u'illness', 18)\n",
      "(u'disorder', 18)\n",
      "(u'disease', 18)\n",
      "(u'corporation', 18)\n",
      "(u'shape', 17)\n",
      "(u'piece_of_work', 17)\n",
      "(u'mechanical_assembly', 17)\n",
      "(u'government', 17)\n",
      "(u'atmospheric_phenomenon', 17)\n",
      "(u'actor', 17)\n",
      "(u'weather', 16)\n",
      "(u'record_album', 16)\n",
      "(u'piece_of_ground', 16)\n",
      "(u'measure', 16)\n",
      "(u'mass_media', 16)\n",
      "(u'group_event', 16)\n",
      "(u'venture', 15)\n",
      "(u'tv_series', 15)\n",
      "(u'title', 15)\n",
      "(u'natural_elevation', 15)\n",
      "(u'mapmaking', 15)\n",
      "(u'knowledge_organization', 15)\n",
      "(u'equipment', 15)\n",
      "(u'written_document', 14)\n",
      "(u'way', 14)\n",
      "(u'video_game', 14)\n",
      "(u'studio_album', 14)\n",
      "(u'image', 14)\n",
      "(u'facility', 14)\n",
      "(u'written_communication', 13)\n",
      "(u'story', 13)\n",
      "(u'series', 13)\n",
      "(u'olympic_sport', 13)\n",
      "(u'musician', 13)\n",
      "(u'musical_group', 13)\n",
      "(u'musical_composition', 13)\n",
      "(u'music_group', 13)\n",
      "(u'means_of_transportation', 13)\n",
      "(u'mapping', 13)\n",
      "(u'language', 13)\n",
      "(u'fluid', 13)\n",
      "(u'firm', 13)\n",
      "(u'clothing', 13)\n",
      "(u'clothes', 13)\n",
      "(u'cartography', 13)\n",
      "(u'book', 13)\n",
      "(u'body_of_water', 13)\n",
      "(u'atmospheric_condition', 13)\n",
      "(u'album', 13)\n",
      "(u'violent_storm', 12)\n",
      "(u'video', 12)\n",
      "(u'type_of_sport', 12)\n",
      "(u'title_of_respect', 12)\n",
      "(u'teacher', 12)\n",
      "(u'storage', 12)\n",
      "(u'space', 12)\n",
      "(u'social_control', 12)\n",
      "(u'route', 12)\n",
      "(u'political_organization', 12)\n",
      "(u'political_organisation', 12)\n",
      "(u'partnership', 12)\n",
      "(u'olympic_sports', 12)\n",
      "(u'mode_of_transport', 12)\n",
      "(u'land_site', 12)\n",
      "(u'infrastructure', 12)\n",
      "(u'data-storage_medium', 12)\n",
      "(u'current_of_air', 12)\n",
      "(u'contestant', 12)\n",
      "(u'computer_game', 12)\n",
      "(u'body_part', 12)\n",
      "(u'application', 12)\n",
      "(u'written_language', 11)\n",
      "(u'waterbody', 11)\n",
      "(u'videogaming', 11)\n",
      "(u'surface', 11)\n",
      "(u'storm', 11)\n",
      "(u'software_application', 11)\n",
      "(u'site', 11)\n",
      "(u'signal', 11)\n",
      "(u'research', 11)\n",
      "(u'pc_game', 11)\n",
      "(u'measurement', 11)\n",
      "(u'main_road', 11)\n",
      "(u'local_government', 11)\n",
      "(u'form', 11)\n",
      "(u'engineering_science', 11)\n",
      "(u'educator', 11)\n",
      "(u'drama', 11)\n",
      "(u'database', 11)\n",
      "(u'windstorm', 10)\n",
      "(u'wind', 10)\n",
      "(u'war', 10)\n",
      "(u'video_recording', 10)\n",
      "(u'university', 10)\n",
      "(u'tropical_storm', 10)\n",
      "(u'team', 10)\n",
      "(u'rock_group', 10)\n",
      "(u'representational_process', 10)\n",
      "(u'provincial_capital', 10)\n",
      "(u'piece_of_music', 10)\n",
      "(u'observable', 10)\n",
      "(u'natural_process', 10)\n",
      "(u'musical_organization', 10)\n",
      "(u'making', 10)\n",
      "(u'magnitude', 10)\n",
      "(u'investigating', 10)\n",
      "(u'freeway', 10)\n",
      "(u'evidence', 10)\n",
      "(u'engineering_physics', 10)\n",
      "(u'elevation', 10)\n",
      "(u'electronic_media', 10)\n",
      "(u'data_storage_device', 10)\n",
      "(u'controlled-access_highway', 10)\n",
      "(u'computing_machine', 10)\n",
      "(u'computer', 10)\n",
      "(u'competition', 10)\n",
      "(u'capital', 10)\n",
      "(u'business_process', 10)\n",
      "(u'applications_software', 10)\n",
      "(u'applications_programme', 10)\n",
      "(u'app', 10)\n",
      "(u'air_current', 10)\n",
      "(u'worker', 9)\n",
      "(u'warfare', 9)\n",
      "(u'visual_art', 9)\n",
      "(u'video_game_console', 9)\n",
      "(u'transport_infrastructure', 9)\n",
      "(u'track', 9)\n",
      "(u'team_sport', 9)\n",
      "(u'stone', 9)\n",
      "(u'station', 9)\n",
      "(u'scholar', 9)\n",
      "(u'rock_band', 9)\n",
      "(u'physical_system', 9)\n",
      "(u'physical_property', 9)\n",
      "(u'personal_name', 9)\n",
      "(u'periodical_publication', 9)\n",
      "(u'painter', 9)\n",
      "(u'narrative', 9)\n",
      "(u'motorway', 9)\n",
      "(u'military_force', 9)\n",
      "(u'mass_medium', 9)\n",
      "(u'mammal', 9)\n",
      "(u'locus', 9)\n",
      "(u'juridical_person', 9)\n",
      "(u'investigation', 9)\n",
      "(u'instructor', 9)\n",
      "(u'governance', 9)\n",
      "(u'game_console', 9)\n",
      "(u'free_energy', 9)\n",
      "(u'expressway', 9)\n",
      "(u'cost', 9)\n",
      "(u'computing_device', 9)\n",
      "(u'broadcast', 9)\n",
      "(u'bodily_process', 9)\n",
      "(u'band', 9)\n",
      "(u'atmosphere', 9)\n",
      "(u'application_program', 9)\n",
      "(u'vehicle', 8)\n",
      "(u'tracheophyte', 8)\n",
      "(u'township', 8)\n",
      "(u'topological_space', 8)\n",
      "(u'thespian', 8)\n",
      "(u'textfile', 8)\n",
      "(u'television_producer', 8)\n",
      "(u'surname', 8)\n",
      "(u'support', 8)\n",
      "(u'superhighway', 8)\n",
      "(u'state_highway', 8)\n",
      "(u'single-valued_function', 8)\n",
      "(u'signaling', 8)\n",
      "(u'service', 8)\n",
      "(u'school', 8)\n",
      "(u'scholarly_person', 8)\n",
      "(u'rock', 8)\n",
      "(u'periodical_literature', 8)\n",
      "(u'pathological_state', 8)\n",
      "(u'natural_action', 8)\n",
      "(u'mountain', 8)\n",
      "(u'monetary_unit', 8)\n",
      "(u'military_unit', 8)\n",
      "(u'military_man', 8)\n",
      "(u'military_action', 8)\n",
      "(u'jock', 8)\n",
      "(u'inquiry', 8)\n",
      "(u'imaginary_being', 8)\n",
      "(u'icon', 8)\n",
      "(u'hurricane', 8)\n",
      "(u'human_language', 8)\n",
      "(u'gov', 8)\n",
      "(u'form_of_government', 8)\n",
      "(u'flick', 8)\n",
      "(u'fictional_character', 8)\n",
      "(u'engineering', 8)\n",
      "(u'electronic_equipment', 8)\n",
      "(u'educationist', 8)\n",
      "(u'educational_institution', 8)\n",
      "(u'edifice', 8)\n",
      "(u'drawing', 8)\n",
      "(u'designated_place', 8)\n",
      "(u'contest', 8)\n",
      "(u'concern', 8)\n",
      "(u'computer_file', 8)\n",
      "(u'composition', 8)\n",
      "(u'combatant', 8)\n",
      "(u'bookman', 8)\n",
      "(u'body_structure', 8)\n",
      "(u'bodily_structure', 8)\n",
      "(u'binary_relation', 8)\n",
      "(u'agreement', 8)\n",
      "(u'abstract_object', 8)\n",
      "(u'woody_plant', 7)\n",
      "(u'vessel', 7)\n",
      "(u'unit_of_time', 7)\n",
      "(u'u.s._state', 7)\n",
      "(u'tropical_cyclone', 7)\n",
      "(u'theater', 7)\n",
      "(u'textile', 7)\n",
      "(u'termination', 7)\n",
      "(u'task', 7)\n",
      "(u'sweet', 7)\n",
      "(u'sound', 7)\n",
      "(u'social_function', 7)\n",
      "(u'ship', 7)\n",
      "(u'senior_management', 7)\n",
      "(u'season', 7)\n",
      "(u'railroad_terminal', 7)\n",
      "(u'public_administration', 7)\n",
      "(u'producer', 7)\n",
      "(u'political_system', 7)\n",
      "(u'poetess', 7)\n",
      "(u'poet', 7)\n",
      "(u'periodical', 7)\n",
      "(u'period_of_time', 7)\n",
      "(u'perennial', 7)\n",
      "(u'paper', 7)\n",
      "(u'officeholder', 7)\n",
      "(u'obstruction', 7)\n",
      "(u'network_host', 7)\n",
      "(u'musical_style', 7)\n",
      "(u'music_genre', 7)\n",
      "(u'municipal_government', 7)\n",
      "(u'movie_maker', 7)\n",
      "(u'mount', 7)\n",
      "(u'molecular_entity', 7)\n",
      "(u'medicine', 7)\n",
      "(u'mathematical_relation', 7)\n",
      "(u'library_science', 7)\n",
      "(u'judge', 7)\n",
      "(u'installation', 7)\n",
      "(u'h2o', 7)\n",
      "(u'given_name', 7)\n",
      "(u'filmmaker', 7)\n",
      "(u'film_maker', 7)\n",
      "(u'fictitious_character', 7)\n",
      "(u'fiction', 7)\n",
      "(u'fault', 7)\n",
      "(u'family_name', 7)\n",
      "(u'energy', 7)\n",
      "(u'electronic_game', 7)\n",
      "(u'electronic_computer', 7)\n",
      "(u'database_management_system', 7)\n",
      "(u'data_file', 7)\n",
      "(u'cyclone', 7)\n",
      "(u'conveyance', 7)\n",
      "(u'connection', 7)\n",
      "(u'computing_platform', 7)\n",
      "(u'composer', 7)\n",
      "(u'component', 7)\n",
      "(u'chief_of_state', 7)\n",
      "(u'center', 7)\n",
      "(u'business_organization', 7)\n",
      "(u'brotherhood', 7)\n",
      "(u'bodily_function', 7)\n",
      "(u'biomolecule', 7)\n",
      "(u'biological_group', 7)\n",
      "(u'authorities', 7)\n",
      "(u'atmospheric_state', 7)\n",
      "(u'astronomical_object', 7)\n",
      "(u'applied_science', 7)\n",
      "(u'application_software', 7)\n",
      "(u'academy', 7)\n",
      "(u'wrongful_conduct', 6)\n",
      "(u'write-up', 6)\n",
      "(u'weapon_system', 6)\n",
      "(u'weapon', 6)\n",
      "(u'warring', 6)\n",
      "(u'volcano', 6)\n",
      "(u'variable', 6)\n",
      "(u'utility', 6)\n",
      "(u'understanding', 6)\n",
      "(u'transportation_stop', 6)\n",
      "(u'train_station', 6)\n",
      "(u'train_depot', 6)\n",
      "(u'tract', 6)\n",
      "(u'time_period', 6)\n",
      "(u'throughway', 6)\n",
      "(u'thought_process', 6)\n",
      "(u'theatre', 6)\n",
      "(u'terminal', 6)\n",
      "(u'student', 6)\n",
      "(u'structural_member', 6)\n",
      "(u'storm_damage', 6)\n",
      "(u'sportswoman', 6)\n",
      "(u'scheme', 6)\n",
      "(u'religious_person', 6)\n",
      "(u'recording', 6)\n",
      "(u'railway_station', 6)\n",
      "(u'public_press', 6)\n",
      "(u'protected_area', 6)\n",
      "(u'projectile', 6)\n",
      "(u'professor', 6)\n",
      "(u'print_media', 6)\n",
      "(u'press', 6)\n",
      "(u'popular_music_genre', 6)\n",
      "(u'polity', 6)\n",
      "(u'point_of_reference', 6)\n",
      "(u'platform', 6)\n",
      "(u'philosophy', 6)\n",
      "(u'owner', 6)\n",
      "(u'outdoor_game', 6)\n",
      "(u'noun', 6)\n",
      "(u'narration', 6)\n",
      "(u'military_group', 6)\n",
      "(u'military_branch', 6)\n",
      "(u'military', 6)\n",
      "(u'mathematical_function', 6)\n",
      "(u'lithostratigraphic_unit', 6)\n",
      "(u'legislative_body', 6)\n",
      "(u'jurist', 6)\n",
      "(u'intellectual', 6)\n",
      "(u'indication', 6)\n",
      "(u'household_appliance', 6)\n",
      "(u'horse', 6)\n",
      "(u'honorific', 6)\n",
      "(u'home_appliance', 6)\n",
      "(u'head_of_state', 6)\n",
      "(u'government_agency', 6)\n",
      "(u'governing', 6)\n",
      "(u'geometric_shape', 6)\n",
      "(u'games_console', 6)\n",
      "(u'forms_of_energy', 6)\n",
      "(u'film_producer', 6)\n",
      "(u'film_making', 6)\n",
      "(u'film_director', 6)\n",
      "(u'file', 6)\n",
      "(u'employee', 6)\n",
      "(u'electronic_circuit', 6)\n",
      "(u'electrical_device', 6)\n",
      "(u'electrical_appliance', 6)\n",
      "(u'drug', 6)\n",
      "(u'dramatics', 6)\n",
      "(u'dramatic_art', 6)\n",
      "(u'difference_of_opinion', 6)\n",
      "(u'depiction', 6)\n",
      "(u'depicting', 6)\n",
      "(u'denseness', 6)\n",
      "(u'cylinder', 6)\n",
      "(u'cricketer', 6)\n",
      "(u'county', 6)\n",
      "(u'constituent', 6)\n",
      "(u'computing_system', 6)\n",
      "(u'coin', 6)\n",
      "(u'cinematography', 6)\n",
      "(u'celestial_body', 6)\n",
      "(u'cartoon_character', 6)\n",
      "(u'carnivory', 6)\n",
      "(u'calculation', 6)\n",
      "(u'business_organisation', 6)\n",
      "(u'branch_of_science', 6)\n",
      "(u'bird', 6)\n",
      "(u'association', 6)\n",
      "(u'assistant_professor', 6)\n",
      "(u'assets', 6)\n",
      "(u'arrangement', 6)\n",
      "(u'aristocrat', 6)\n",
      "(u'area_of_mathematics', 6)\n",
      "(u'anatomical_structure', 6)\n",
      "(u'american_state', 6)\n",
      "(u'aircraft', 6)\n",
      "(u'administration', 6)\n",
      "(u'written_agreement', 5)\n",
      "(u'workman', 5)\n",
      "(u'working_person', 5)\n",
      "(u'working_man', 5)\n",
      "(u'woman', 5)\n",
      "(u'website', 5)\n",
      "(u'web_site', 5)\n",
      "(u'warrior', 5)\n",
      "(u'utility_program', 5)\n",
      "(u'tubing', 5)\n",
      "(u'tube', 5)\n",
      "(u'trouble', 5)\n",
      "(u'tropical_depression', 5)\n",
      "(u'treat', 5)\n",
      "(u'transferred_property', 5)\n",
      "(u'track_and_field', 5)\n",
      "(u'thruway', 5)\n",
      "(u'text_file', 5)\n",
      "(u'text', 5)\n",
      "(u'terrain', 5)\n",
      "(u'systems_program', 5)\n",
      "(u'system_software', 5)\n",
      "(u'state_of_matter', 5)\n",
      "(u'songster', 5)\n",
      "(u'song', 5)\n",
      "(u'soldier', 5)\n",
      "(u'sexual_activity', 5)\n",
      "(u'sexual_act', 5)\n",
      "(u'sex_activity', 5)\n",
      "(u'server_system', 5)\n",
      "(u'server', 5)\n",
      "(u'serial', 5)\n",
      "(u'reference', 5)\n",
      "(u'railroad_station', 5)\n",
      "(u'race', 5)\n",
      "(u'pure_mathematics', 5)\n",
      "(u'protection', 5)\n",
      "(u'proclivity', 5)\n",
      "(u'predisposition', 5)\n",
      "(u'practice_of_medicine', 5)\n",
      "(u'possession', 5)\n",
      "(u'personnel', 5)\n",
      "(u'party', 5)\n",
      "(u'new_england_town', 5)\n",
      "(u'natural_depression', 5)\n",
      "(u'narrative_mode', 5)\n",
      "(u'money', 5)\n",
      "(u'monarch', 5)\n",
      "(u'misconduct', 5)\n",
      "(u'military_organization', 5)\n",
      "(u'methodology', 5)\n",
      "(u'measuring', 5)\n",
      "(u'literary_work', 5)\n",
      "(u'literary_composition', 5)\n",
      "(u'liquid_state', 5)\n",
      "(u'legislative_assembly', 5)\n",
      "(u'legal_expert', 5)\n",
      "(u'lecturer', 5)\n",
      "(u'king', 5)\n",
      "(u'justice', 5)\n",
      "(u'jurisconsult', 5)\n",
      "(u'island', 5)\n",
      "(u'intellect', 5)\n",
      "(u'installation_art', 5)\n",
      "(u'injury', 5)\n",
      "(u'information_processing_system', 5)\n",
      "(u'imaginary_creature', 5)\n",
      "(u'government_activity', 5)\n",
      "(u'geographic_point', 5)\n",
      "(u'full_name', 5)\n",
      "(u'foodstuff', 5)\n",
      "(u'food_product', 5)\n",
      "(u'fine_arts', 5)\n",
      "(u'filming', 5)\n",
      "(u'film_genre', 5)\n",
      "(u'female_person', 5)\n",
      "(u'female', 5)\n",
      "(u'faculty_member', 5)\n",
      "(u'faculty', 5)\n",
      "(u'fabric', 5)\n",
      "(u'extinct_volcano', 5)\n",
      "(u'executive', 5)\n",
      "(u'enquiry', 5)\n",
      "(u'emblem', 5)\n",
      "(u'electronic_component', 5)\n",
      "(u'dispute', 5)\n",
      "(u'disaster', 5)\n",
      "(u'dimensionless_quantity', 5)\n",
      "(u'devising', 5)\n",
      "(u'depot', 5)\n",
      "(u'decision_maker', 5)\n",
      "(u'dance_orchestra', 5)\n",
      "(u'dance_band', 5)\n",
      "(u'crystal', 5)\n",
      "(u'criminal_offense', 5)\n",
      "(u'crime', 5)\n",
      "(u'conservation_area', 5)\n",
      "(u'confection', 5)\n",
      "(u'conclusion', 5)\n",
      "(u'computer_system', 5)\n",
      "(u'communications_system', 5)\n",
      "(u'coinage', 5)\n",
      "(u'cloth', 5)\n",
      "(u'city_district', 5)\n",
      "(u'circuitry', 5)\n",
      "(u'chemical_action', 5)\n",
      "(u'centre', 5)\n",
      "(u'carnivore', 5)\n",
      "(u'business_executive', 5)\n",
      "(u'brand', 5)\n",
      "(u'body_process', 5)\n",
      "(u'barrier', 5)\n",
      "(u'audio_signal', 5)\n",
      "(u'artifact', 5)\n",
      "(u'affix', 5)\n",
      "(u'administrator', 5)\n",
      "(u'wrongdoing', 4)\n",
      "(u'written_symbol', 4)\n",
      "(u'wild_horse', 4)\n",
      "(u'voluntary_association', 4)\n",
      "(u'visual_communication', 4)\n",
      "(u'visual_arts', 4)\n",
      "(u'video_game_genre', 4)\n",
      "(u'variable_quantity', 4)\n",
      "(u'utility_software', 4)\n",
      "(u'unincorporated_area', 4)\n",
      "(u'unicameral_legislature', 4)\n",
      "(u'time_interval', 4)\n",
      "(u'thoroughfare', 4)\n",
      "(u'telecommunication_system', 4)\n",
      "(u'tale', 4)\n",
      "(u'systems_software', 4)\n",
      "(u'system_program', 4)\n",
      "(u'symbolisation', 4)\n",
      "(u'sweets', 4)\n",
      "(u'structural_element', 4)\n",
      "(u'statesman', 4)\n",
      "(u'squad', 4)\n",
      "(u'spectral_color', 4)\n",
      "(u'songwriter', 4)\n",
      "(u'soil', 4)\n",
      "(u'sociology', 4)\n",
      "(u'social_rejection', 4)\n",
      "(u'social_occasion', 4)\n",
      "(u'social_action', 4)\n",
      "(u'sign', 4)\n",
      "(u'sexual_relations', 4)\n",
      "(u'sexual_practice', 4)\n",
      "(u'service_program', 4)\n",
      "(u'seed', 4)\n",
      "(u'scientific_research', 4)\n",
      "(u'scientific_evidence', 4)\n",
      "(u'sailboat', 4)\n",
      "(u'river', 4)\n",
      "(u'relative', 4)\n",
      "(u'rejection', 4)\n",
      "(u'reaction_mixture', 4)\n",
      "(u'reaction', 4)\n",
      "(u'ratio', 4)\n",
      "(u'quantitative_relation', 4)\n",
      "(u'public_transport', 4)\n",
      "(u'profits', 4)\n",
      "(u'profit', 4)\n",
      "(u'prof', 4)\n",
      "(u'possessor', 4)\n",
      "(u'portraying', 4)\n",
      "(u'portrayal', 4)\n",
      "(u'popular_music', 4)\n",
      "(u'plant_part', 4)\n",
      "(u'photo', 4)\n",
      "(u'philology', 4)\n",
      "(u'pebble', 4)\n",
      "(u'payment', 4)\n",
      "(u'patrician', 4)\n",
      "(u'parkland', 4)\n",
      "(u'opus', 4)\n",
      "(u'operator', 4)\n",
      "(u'opera', 4)\n",
      "(u'offense', 4)\n",
      "(u'novel', 4)\n",
      "(u'note', 4)\n",
      "(u'network', 4)\n",
      "(u'net_earnings', 4)\n",
      "(u'natural_disaster', 4)\n",
      "(u'musical_instrument', 4)\n",
      "(u'municipal_corporation', 4)\n",
      "(u'moral_philosophy', 4)\n",
      "(u'misfortune', 4)\n",
      "(u'military_rank', 4)\n",
      "(u'metrology', 4)\n",
      "(u'metro_station', 4)\n",
      "(u'method', 4)\n",
      "(u'metal_money', 4)\n",
      "(u'merchant', 4)\n",
      "(u'measuring_device', 4)\n",
      "(u'marking', 4)\n",
      "(u'manufacture', 4)\n",
      "(u'malady', 4)\n",
      "(u'lyricist', 4)\n",
      "(u'legislator', 4)\n",
      "(u'legal_name', 4)\n",
      "(u'lector', 4)\n",
      "(u'learned_profession', 4)\n",
      "(u'language_unit', 4)\n",
      "(u'land_vehicle', 4)\n",
      "(u'label', 4)\n",
      "(u'interconnection', 4)\n",
      "(u'intelligence', 4)\n",
      "(u'intellection', 4)\n",
      "(u'impediment', 4)\n",
      "(u'imaginary_place', 4)\n",
      "(u'illustration', 4)\n",
      "(u'human_sexual_activity', 4)\n",
      "(u'herb', 4)\n",
      "(u'heavenly_body', 4)\n",
      "(u'hamlet', 4)\n",
      "(u'governor', 4)\n",
      "(u'goody', 4)\n",
      "(u'geological_time', 4)\n",
      "(u'geological_phenomenon', 4)\n",
      "(u'geological_period', 4)\n",
      "(u'geologic_time', 4)\n",
      "(u'geographical_point', 4)\n",
      "(u'gas', 4)\n",
      "(u'fraternity', 4)\n",
      "(u'forename', 4)\n",
      "(u'flow', 4)\n",
      "(u'fish', 4)\n",
      "(u'film_production', 4)\n",
      "(u'figure_of_speech', 4)\n",
      "(u'feature_film', 4)\n",
      "(u'fashioning', 4)\n",
      "(u'expression', 4)\n",
      "(u'explorer', 4)\n",
      "(u'executive_director', 4)\n",
      "(u'era', 4)\n",
      "(u'episode', 4)\n",
      "(u'environment', 4)\n",
      "(u'ending', 4)\n",
      "(u'electrical_load', 4)\n",
      "(u'earnings', 4)\n",
      "(u'drinkable', 4)\n",
      "(u'drink', 4)\n",
      "(u'dormant_volcano', 4)\n",
      "(u'dish', 4)\n",
      "(u'disagreement', 4)\n",
      "(u'difference', 4)\n",
      "(u'dictionary_entry', 4)\n",
      "(u'deity', 4)\n",
      "(u'definite_quantity', 4)\n",
      "(u'data_processor', 4)\n",
      "(u'criminal_offence', 4)\n",
      "(u'county_town', 4)\n",
      "(u'county_courthouse', 4)\n",
      "(u'corporate_executive', 4)\n",
      "(u'corp', 4)\n",
      "(u'control', 4)\n",
      "(u'computer_network', 4)\n",
      "(u'computation', 4)\n",
      "(u'communication_system', 4)\n",
      "(u'commissioned_officer', 4)\n",
      "(u'clergyman', 4)\n",
      "(u'circuit_card', 4)\n",
      "(u'circuit', 4)\n",
      "(u'chemical_process', 4)\n",
      "(u'chemical_change', 4)\n",
      "(u'ceremonial', 4)\n",
      "(u'candy', 4)\n",
      "(u'business_concern', 4)\n",
      "(u'brand_image', 4)\n",
      "(u'branch', 4)\n",
      "(u'boat', 4)\n",
      "(u'bioscience', 4)\n",
      "(u'baseball_player', 4)\n",
      "(u'ball_player', 4)\n",
      "(u'bad_person', 4)\n",
      "(u'architectural_element', 4)\n",
      "(u'appliance', 4)\n",
      "(u'animal_tissue', 4)\n",
      "(u'angiosperm', 4)\n",
      "(u'anatomy', 4)\n",
      "(u'alcohol', 4)\n",
      "(u'airplane', 4)\n",
      "(u'aeroplane', 4)\n",
      "(u'advocate', 4)\n",
      "(u'adventurer', 4)\n",
      "(u'adult_female', 4)\n",
      "(u'action_game', 4)\n",
      "(u'wrongful_act', 3)\n",
      "(u'wrongdoer', 3)\n",
      "(u'written_record', 3)\n",
      "(u'written_account', 3)\n",
      "(u'world_market', 3)\n",
      "(u'workplace', 3)\n",
      "(u'workingman', 3)\n",
      "(u'webpage', 3)\n",
      "(u'web_page', 3)\n",
      "(u'weaponry', 3)\n",
      "(u'watercraft', 3)\n",
      "(u'union', 3)\n",
      "(u'undertaking', 3)\n",
      "(u'tv', 3)\n",
      "(u'treatment', 3)\n",
      "(u'traveller', 3)\n",
      "(u'traveler', 3)\n",
      "(u'transportation_system', 3)\n",
      "(u'trainer', 3)\n",
      "(u'tragedy', 3)\n",
      "(u'topological_manifold', 3)\n",
      "(u'time_unit', 3)\n",
      "(u'thrower', 3)\n",
      "(u'thermodynamic_system', 3)\n",
      "(u'tender', 3)\n",
      "(u'tendency', 3)\n",
      "(u'television', 3)\n",
      "(u'telecommunication', 3)\n",
      "(u'telecom_equipment', 3)\n",
      "(u'technical_standard', 3)\n",
      "(u'symbolization', 3)\n",
      "(u'surround', 3)\n",
      "(u'substantive', 3)\n",
      "(u'substantial_form', 3)\n",
      "(u'structural_biology', 3)\n",
      "(u'string_of_words', 3)\n",
      "(u'street', 3)\n",
      "(u'straight_line', 3)\n",
      "(u'stop', 3)\n",
      "(u'stimulus', 3)\n",
      "(u'standard', 3)\n",
      "(u'spectral_colour', 3)\n",
      "(u'spatial_relation', 3)\n",
      "(u'sovereign', 3)\n",
      "(u'solid_figure', 3)\n",
      "(u'social_movement', 3)\n",
      "(u'social_club', 3)\n",
      "(u'singer', 3)\n",
      "(u'ship_type', 3)\n",
      "(u'sexual_relation', 3)\n",
      "(u'sex_act', 3)\n",
      "(u'separation_process', 3)\n",
      "(u'sensitivity', 3)\n",
      "(u'sensation', 3)\n",
      "(u'semiconductor', 3)\n",
      "(u'selling', 3)\n",
      "(u'sea', 3)\n",
      "(u'satellite', 3)\n",
      "(u'saint', 3)\n",
      "(u'sailing_vessel', 3)\n",
      "(u'sailing_ship', 3)\n",
      "(u'sailing_boat', 3)\n",
      "(u'sail', 3)\n",
      "(u'rupture', 3)\n",
      "(u'round_shape', 3)\n",
      "(u'rock_music', 3)\n",
      "(u\"rock_'n'_roll\", 3)\n",
      "(u'road_vehicle', 3)\n",
      "(u'risk', 3)\n",
      "(u'rider', 3)\n",
      "(u'retail_price', 3)\n",
      "(u'response', 3)\n",
      "(u'republic', 3)\n",
      "(u'remembering', 3)\n",
      "(u'religious_ritual', 3)\n",
      "(u'redness', 3)\n",
      "(u'record_label', 3)\n",
      "(u'rate', 3)\n",
      "(u'rank', 3)\n",
      "(u'railroad_track', 3)\n",
      "(u'putting_to_death', 3)\n",
      "(u'public_university', 3)\n",
      "(u'protocol', 3)\n",
      "(u'protective_covering', 3)\n",
      "(u'protective_cover', 3)\n",
      "(u'propensity', 3)\n",
      "(u'proneness', 3)\n",
      "(u'programming_language', 3)\n",
      "(u'programing_language', 3)\n",
      "(u'principle', 3)\n",
      "(u'primary_color', 3)\n",
      "(u'price', 3)\n",
      "(u'prerogative', 3)\n",
      "(u'precious_stone', 3)\n",
      "(u'portraiture', 3)\n",
      "(u'population', 3)\n",
      "(u'political_party', 3)\n",
      "(u'political_economy', 3)\n",
      "(u'plug-in', 3)\n",
      "(u'playwright', 3)\n",
      "(u'planned_language', 3)\n",
      "(u'plane_figure', 3)\n",
      "(u'plan_of_action', 3)\n",
      "(u'placement', 3)\n",
      "(u'place_of_business', 3)\n",
      "(u'pitch', 3)\n",
      "(u'photograph', 3)\n",
      "(u'philosophy_of_language', 3)\n",
      "(u'pharmaceutical_drug', 3)\n",
      "(u'performing_arts', 3)\n",
      "(u'perception', 3)\n",
      "(u'penalty', 3)\n",
      "(u'path', 3)\n",
      "(u'passageway', 3)\n",
      "(u'passage', 3)\n",
      "(u'partition', 3)\n",
      "(u'particle', 3)\n",
      "(u'park', 3)\n",
      "(u'parcel', 3)\n",
      "(u'painting', 3)\n",
      "(u'organ', 3)\n",
      "(u'operating_system', 3)\n",
      "(u'occasion', 3)\n",
      "(u'obstructor', 3)\n",
      "(u'noble', 3)\n",
      "(u'newspaper', 3)\n",
      "(u'network_service', 3)\n",
      "(u'net_profit', 3)\n",
      "(u'net_income', 3)\n",
      "(u'net', 3)\n",
      "(u'negotiable_instrument', 3)\n",
      "(u'natural_language', 3)\n",
      "(u'native_plant', 3)\n",
      "(u'national_leader', 3)\n",
      "(u'name_brand', 3)\n",
      "(u'mixture', 3)\n",
      "(u'mistake', 3)\n",
      "(u'military_operation', 3)\n",
      "(u'microcircuit', 3)\n",
      "(u'micro-organism', 3)\n",
      "(u'mention', 3)\n",
      "(u'memory', 3)\n",
      "(u'membership', 3)\n",
      "(u'medicinal_drug', 3)\n",
      "(u'medical_science', 3)\n",
      "(u'medical_research', 3)\n",
      "(u'medical_procedure', 3)\n",
      "(u'media_professional', 3)\n",
      "(u'measuring_system', 3)\n",
      "(u'measuring_instrument', 3)\n",
      "(u'mathematical_process', 3)\n",
      "(u'mathematical_operation', 3)\n",
      "(u'master', 3)\n",
      "(u'mark', 3)\n",
      "(u'manifold', 3)\n",
      "(u'make-up', 3)\n",
      "(u'magnitude_relation', 3)\n",
      "(u'macromolecule', 3)\n",
      "(u'lyrist', 3)\n",
      "(u'lovemaking', 3)\n",
      "(u'love_life', 3)\n",
      "(u'list', 3)\n",
      "(u'liquidity', 3)\n",
      "(u'linguistics', 3)\n",
      "(u'life_science', 3)\n",
      "(u'life', 3)\n",
      "(u'lexicology', 3)\n",
      "(u'law-makers', 3)\n",
      "(u'landed_property', 3)\n",
      "(u'killing', 3)\n",
      "(u'killer', 3)\n",
      "(u'kill', 3)\n",
      "(u'journalist', 3)\n",
      "(u'journal', 3)\n",
      "(u'island_nation', 3)\n",
      "(u'island_country', 3)\n",
      "(u'internet_site', 3)\n",
      "(u'intellectual_property', 3)\n",
      "(u'instrument', 3)\n",
      "(u'insect', 3)\n",
      "(u'innerwear', 3)\n",
      "(u'information_network', 3)\n",
      "(u'inflammatory_disease', 3)\n",
      "(u'inflammation', 3)\n",
      "(u'infection', 3)\n",
      "(u'increase', 3)\n",
      "(u'incorporation', 3)\n",
      "(u'income', 3)\n",
      "(u'inclination', 3)\n",
      "(u'ideal', 3)\n",
      "(u'housing', 3)\n",
      "(u'hostility', 3)\n",
      "(u'host', 3)\n",
      "(u'horsie', 3)\n",
      "(u'hoofed_mammal', 3)\n",
      "(u'holder', 3)\n",
      "(u'historian', 3)\n",
      "(u'hindrance', 3)\n",
      "(u'herbaceous_plant', 3)\n",
      "(u'health_care', 3)\n",
      "(u'hazard', 3)\n",
      "(u'hatred', 3)\n",
      "(u'green_goods', 3)\n",
      "(u'graphic_symbol', 3)\n",
      "(u'granular_material', 3)\n",
      "(u'grammatical_category', 3)\n",
      "(u'godhood', 3)\n",
      "(u'geologic_timescale', 3)\n",
      "(u'fundamental_measure', 3)\n",
      "(u'fund', 3)\n",
      "(u'fracture', 3)\n",
      "(u'forefather', 3)\n",
      "(u'footwear', 3)\n",
      "(u'footballer', 3)\n",
      "(u'flux', 3)\n",
      "(u'flowering_tree', 3)\n",
      "(u'flower', 3)\n",
      "(u'flag', 3)\n",
      "(u'finish', 3)\n",
      "(u'fine_art', 3)\n",
      "(u'financial_organization', 3)\n",
      "(u'field_game', 3)\n",
      "(u'feature', 3)\n",
      "(u'failure', 3)\n",
      "(u'facies', 3)\n",
      "(u'exploration', 3)\n",
      "(u'explanation', 3)\n",
      "(u'expenditure', 3)\n",
      "(u'evening_paper', 3)\n",
      "(u'evening_newspaper', 3)\n",
      "(u'evaluator', 3)\n",
      "(u'entertainment', 3)\n",
      "(u'engineer', 3)\n",
      "(u'end', 3)\n",
      "(u'elementary_particle', 3)\n",
      "(u'electrical_network', 3)\n",
      "(u'electrical_energy', 3)\n",
      "(u'electric_energy', 3)\n",
      "(u'electric_circuit', 3)\n",
      "(u'economics', 3)\n",
      "(u'economic_science', 3)\n",
      "(u'durables', 3)\n",
      "(u'durable_goods', 3)\n",
      "(u'durable_good', 3)\n",
      "(u'dramaturgy', 3)\n",
      "(u'dramatist', 3)\n",
      "(u'dramatic_play', 3)\n",
      "(u'dr.', 3)\n",
      "(u'dog', 3)\n",
      "(u'doctor', 3)\n",
      "(u'distribution', 3)\n",
      "(u'disagreeable_person', 3)\n",
      "(u'dirt', 3)\n",
      "(u'direction', 3)\n",
      "(u'digital_media', 3)\n",
      "(u'difficulty', 3)\n",
      "(u'developed_country', 3)\n",
      "(u'dessert', 3)\n",
      "(u'designer', 3)\n",
      "(u'depression', 3)\n",
      "(u'density', 3)\n",
      "(u'declaration', 3)\n",
      "(u'death', 3)\n",
      "(u'data_format', 3)\n",
      "(u'danger', 3)\n",
      "(u'daily_newspaper', 3)\n",
      "(u'curved_shape', 3)\n",
      "(u'crack', 3)\n",
      "(u'counties_of_england', 3)\n",
      "(u'convenience', 3)\n",
      "(u'controller', 3)\n",
      "(u'console', 3)\n",
      "(u'connexion', 3)\n",
      "(u'conference', 3)\n",
      "(u'confectionery', 3)\n",
      "(u'computer_networking', 3)\n",
      "(u'computer_language', 3)\n",
      "(u'computer_circuit', 3)\n",
      "(u'computer_chip', 3)\n",
      "(u'communication_equipment', 3)\n",
      "(u'combat', 3)\n",
      "(u'college', 3)\n",
      "(u'coach', 3)\n",
      "(u'club', 3)\n",
      "(u'clinical_symptom', 3)\n",
      "(u'cite', 3)\n",
      "(u'circuit_board', 3)\n",
      "(u'child', 3)\n",
      "(u'chemical_series', 3)\n",
      "(u'chemical_phenomenon', 3)\n",
      "(u'chemical_group', 3)\n",
      "(u'cereal', 3)\n",
      "(u'census_place', 3)\n",
      "(u'category', 3)\n",
      "(u'catastrophe', 3)\n",
      "(u'card_game', 3)\n",
      "(u'card', 3)\n",
      "(u'business_establishment', 3)\n",
      "(u'building_material', 3)\n",
      "(u'board', 3)\n",
      "(u'black_and_white', 3)\n",
      "(u'binomial_nomenclature', 3)\n",
      "(u'binomial_name', 3)\n",
      "(u'baseball', 3)\n",
      "(u'base', 3)\n",
      "(u'bandmember', 3)\n",
      "(u'ball_game', 3)\n",
      "(u'ball', 3)\n",
      "(u'bad_luck', 3)\n",
      "(u'atom', 3)\n",
      "(u'atlantic_hurricane', 3)\n",
      "(u'armed_forces', 3)\n",
      "(u'armed_combat', 3)\n",
      "(u'architect', 3)\n",
      "(u'annotation', 3)\n",
      "(u'airfield', 3)\n",
      "(u'aid', 3)\n",
      "(u'admirer', 3)\n",
      "(u'add-in', 3)\n",
      "(u'active_volcano', 3)\n",
      "(u'academician', 3)\n",
      "(u'academic', 3)\n",
      "(u'youngster', 2)\n",
      "(u'young_lady', 2)\n",
      "(u'yell', 2)\n",
      "(u'wrongfulness', 2)\n",
      "(u'world_ocean', 2)\n",
      "(u'word_string', 2)\n",
      "(u'wood', 2)\n",
      "(u'wildflower', 2)\n",
      "(u'whole_number', 2)\n",
      "(u'wheeled_vehicle', 2)\n",
      "(u'weapons_system', 2)\n",
      "(u'waterway', 2)\n",
      "(u'wall', 2)\n",
      "(u'voyage', 2)\n",
      "(u'voluntary_action', 2)\n",
      "(u'volcanic_crater', 2)\n",
      "(u'voice', 2)\n",
      "(u'vocalist', 2)\n",
      "(u'vocal', 2)\n",
      "(u'violence', 2)\n",
      "(u'verbal_description', 2)\n",
      "(u'veggie', 2)\n",
      "(u'vegetable', 2)\n",
      "(u'veg', 2)\n",
      "(u'vasculature', 2)\n",
      "(u'vascular_system', 2)\n",
      "(u'valley', 2)\n",
      "(u'vaginal_sex', 2)\n",
      "(u'utterer', 2)\n",
      "(u'utilization', 2)\n",
      "(u'use', 2)\n",
      "(u'usage', 2)\n",
      "(u'urban_district', 2)\n",
      "(u'upper_class', 2)\n",
      "(u'unpleasant_person', 2)\n",
      "(u'unlawfulness', 2)\n",
      "(u'university_system', 2)\n",
      "(u'university_of_birmingham', 2)\n",
      "(u'unicameralism', 2)\n",
      "(u'underwear', 2)\n",
      "(u'uncovering', 2)\n",
      "(u'uncleanness', 2)\n",
      "(u'type', 2)\n",
      "(u'two-dimensional_space', 2)\n",
      "(u'tv_station', 2)\n",
      "(u'turnpike', 2)\n",
      "(u'turbine', 2)\n",
      "(u'transformation', 2)\n",
      "(u'transferral', 2)\n",
      "(u'transfer', 2)\n",
      "(u'trademark', 2)\n",
      "(u'trade_name', 2)\n",
      "(u'tower', 2)\n",
      "(u'tourist_attraction', 2)\n",
      "(u'three-dimensional_figure', 2)\n",
      "(u'thinking', 2)\n",
      "(u'textbook', 2)\n",
      "(u'terrestrial_planet', 2)\n",
      "(u'terminologist', 2)\n",
      "(u'temporal_property', 2)\n",
      "(u'tempest', 2)\n",
      "(u'temperature_change', 2)\n",
      "(u'television_season', 2)\n",
      "(u'teleost_fish', 2)\n",
      "(u'telecommunication_equipment', 2)\n",
      "(u'telecom_system', 2)\n",
      "(u'telecom', 2)\n",
      "(u'telecasting', 2)\n",
      "(u'tegument', 2)\n",
      "(u'technologist', 2)\n",
      "(u'technics', 2)\n",
      "(u'talker', 2)\n",
      "(u'tactical_maneuver', 2)\n",
      "(u'tableware', 2)\n",
      "(u'tabletop_game', 2)\n",
      "(u'system_of_rules', 2)\n",
      "(u'system_of_measurement', 2)\n",
      "(u'syntactic_category', 2)\n",
      "(u'symbolic_representation', 2)\n",
      "(u'surroundings', 2)\n",
      "(u'surgical_process', 2)\n",
      "(u'surgical_procedure', 2)\n",
      "(u'surgical_operation', 2)\n",
      "(u'surgery', 2)\n",
      "(u'surface_area', 2)\n",
      "(u'supporter', 2)\n",
      "(u'supermolecule', 2)\n",
      "(u'suitableness', 2)\n",
      "(u'suitability', 2)\n",
      "(u'suit', 2)\n",
      "(u'sugar', 2)\n",
      "(u'success', 2)\n",
      "(u'subunit', 2)\n",
      "(u'subatomic_particle', 2)\n",
      "(u'stringed_instrument', 2)\n",
      "(u'string_instrument', 2)\n",
      "(u'strengthener', 2)\n",
      "(u'strength', 2)\n",
      "(u'street_drug', 2)\n",
      "(u'stream_channel', 2)\n",
      "(u'store', 2)\n",
      "(u'storage_device', 2)\n",
      "(u'statue', 2)\n",
      "(u'state_university_system', 2)\n",
      "(u'stage', 2)\n",
      "(u'stadium', 2)\n",
      "(u'sports_stadium', 2)\n",
      "(u'sports_league', 2)\n",
      "(u'sports_club', 2)\n",
      "(u'spheroid', 2)\n",
      "(u'spending', 2)\n",
      "(u'spell', 2)\n",
      "(u'speed', 2)\n",
      "(u'speech_communication', 2)\n",
      "(u'speech', 2)\n",
      "(u'speaker', 2)\n",
      "(u'spaceship', 2)\n",
      "(u'spacecraft', 2)\n",
      "(u'space_vehicle', 2)\n",
      "(u'source_of_illumination', 2)\n",
      "(u'software_engineering', 2)\n",
      "(u'software_component', 2)\n",
      "(u'softness', 2)\n",
      "(u'social_scientist', 2)\n",
      "(u'slot', 2)\n",
      "(u'slave', 2)\n",
      "(u'size', 2)\n",
      "(u'silicate_mineral', 2)\n",
      "(u'sick_person', 2)\n",
      "(u'shot', 2)\n",
      "(u'short_story', 2)\n",
      "(u'shooter_game', 2)\n",
      "(u'shoe', 2)\n",
      "(u'shipping', 2)\n",
      "(u'ship_transport', 2)\n",
      "(u'sheet_of_paper', 2)\n",
      "(u'sheet', 2)\n",
      "(u'shag', 2)\n",
      "(u'sexuality', 2)\n",
      "(u'sexual_penetration', 2)\n",
      "(u'sexual_organ', 2)\n",
      "(u'sexual_love', 2)\n",
      "(u'sexual_intercourse', 2)\n",
      "(u'sexual_congress', 2)\n",
      "(u'sex_position', 2)\n",
      "(u'sex_organ', 2)\n",
      "(u'set_theory', 2)\n",
      "(u'services', 2)\n",
      "(u'serial_publication', 2)\n",
      "(u'separation', 2)\n",
      "(u'separable_space', 2)\n",
      "(u'sensory_faculty', 2)\n",
      "(u'sensitiveness', 2)\n",
      "(u'sensibility', 2)\n",
      "(u'selling_technique', 2)\n",
      "(u'seller', 2)\n",
      "(u'selection', 2)\n",
      "(u'seismic_activity', 2)\n",
      "(u'sediment', 2)\n",
      "(u'secret_society', 2)\n",
      "(u'seaman', 2)\n",
      "(u'seafood', 2)\n",
      "(u'sea_creature', 2)\n",
      "(u'sea_animal', 2)\n",
      "(u'sculpture', 2)\n",
      "(u'sculptor', 2)\n",
      "(u'scientific_method', 2)\n",
      "(u'scientific_journal', 2)\n",
      "(u'scientific_instrument', 2)\n",
      "(u'schoolwork', 2)\n",
      "(u'schoolbook', 2)\n",
      "(u'scholarly_method', 2)\n",
      "(u'scalar', 2)\n",
      "(u'sanitary_condition', 2)\n",
      "(u'salesman', 2)\n",
      "(u'rule', 2)\n",
      "(u'roman_emperor', 2)\n",
      "(u'rock-and-roll', 2)\n",
      "(u'river_transport', 2)\n",
      "(u'righteousness', 2)\n",
      "(u'rhetorical_device', 2)\n",
      "(u'retail_store', 2)\n",
      "(u'retail_outlet', 2)\n",
      "(u'retail', 2)\n",
      "(u'restraint', 2)\n",
      "(u'residence', 2)\n",
      "(u'report', 2)\n",
      "(u'religiousness', 2)\n",
      "(u'religious_ceremony', 2)\n",
      "(u'regular_payment', 2)\n",
      "(u'regime', 2)\n",
      "(u'reference_point', 2)\n",
      "(u'recording_label', 2)\n",
      "(u'rebuff', 2)\n",
      "(u'reasoning', 2)\n",
      "(u'reason', 2)\n",
      "(u'real_property', 2)\n",
      "(u'real_estate', 2)\n",
      "(u'reader', 2)\n",
      "(u'ranged_weapon', 2)\n",
      "(u'range_of_mountains', 2)\n",
      "(u'railway_system', 2)\n",
      "(u'railway_line', 2)\n",
      "(u'railroad_line', 2)\n",
      "(u'quake', 2)\n",
      "(u'punctuation_mark', 2)\n",
      "(u'public_transit', 2)\n",
      "(u'public_park', 2)\n",
      "(u'province', 2)\n",
      "(u'protein', 2)\n",
      "(u'proscription', 2)\n",
      "(u'prophet', 2)\n",
      "(u'project', 2)\n",
      "(u'program_line', 2)\n",
      "(u'progenitor', 2)\n",
      "(u'processing', 2)\n",
      "(u'proceedings', 2)\n",
      "(u'problem_solving', 2)\n",
      "(u'primary_colour', 2)\n",
      "(u'priest', 2)\n",
      "(u'president', 2)\n",
      "(u'preservation', 2)\n",
      "(u'prediction', 2)\n",
      "(u'precipitation', 2)\n",
      "(u'precinct', 2)\n",
      "(u'potentate', 2)\n",
      "(u'post', 2)\n",
      "(u'polygonal_shape', 2)\n",
      "(u'polygon', 2)\n",
      "(u'pollution', 2)\n",
      "(u'political_ideology', 2)\n",
      "(u'point_of_entry', 2)\n",
      "(u'plucked_string_instrument', 2)\n",
      "(u'plot_of_land', 2)\n",
      "(u'pleading', 2)\n",
      "(u'playwrighting', 2)\n",
      "(u'plate', 2)\n",
      "(u'plant_substance', 2)\n",
      "(u'plant_structure', 2)\n",
      "(u'plant_material', 2)\n",
      "(u'planning_board', 2)\n",
      "(u'planet', 2)\n",
      "(u'plane_section', 2)\n",
      "(u'place_of_worship', 2)\n",
      "(u'pitcher', 2)\n",
      "(u'piloting', 2)\n",
      "(u'pike', 2)\n",
      "(u'pig', 2)\n",
      "(u'piece_of_paper', 2)\n",
      "(u'pictorial_representation', 2)\n",
      "(u'physics', 2)\n",
      "(u'physicist', 2)\n",
      "(u'physician', 2)\n",
      "(u'photography', 2)\n",
      "(u'photographer', 2)\n",
      "(u'philosopher', 2)\n",
      "(u'perspective', 2)\n",
      "(u'peril', 2)\n",
      "(u'performance', 2)\n",
      "(u'perceptual_experience', 2)\n",
      "(u'peninsula', 2)\n",
      "(u'penal_institution', 2)\n",
      "(u'pc_board', 2)\n",
      "(u'part_of_speech', 2)\n",
      "(u'parcelling', 2)\n",
      "(u'parceling', 2)\n",
      "(u'packing_material', 2)\n",
      "(u'outlet', 2)\n",
      "(u'organic_food', 2)\n",
      "(u'organic_fertiliser', 2)\n",
      "(u'ordinary_language', 2)\n",
      "(u'orbiter', 2)\n",
      "(u'orbit_period', 2)\n",
      "(u'orb', 2)\n",
      "(u'oral_communication', 2)\n",
      "(u'optical_phenomenon', 2)\n",
      "(u'optical_device', 2)\n",
      "(u'opening', 2)\n",
      "(u'onomastics', 2)\n",
      "(u'offensive_activity', 2)\n",
      "(u'offence', 2)\n",
      "(u'objection', 2)\n",
      "(u'north_american_nation', 2)\n",
      "(u'north_american_country', 2)\n",
      "(u'nonachievement', 2)\n",
      "(u'newspaper_columnist', 2)\n",
      "(u'needlecraft', 2)\n",
      "(u'necromancy', 2)\n",
      "(u'navigator', 2)\n",
      "(u'natural_numbers', 2)\n",
      "(u'natural_number', 2)\n",
      "(u'natural_law', 2)\n",
      "(u'native_metal', 2)\n",
      "(u'native', 2)\n",
      "(u'national_park', 2)\n",
      "(u'national_boundary', 2)\n",
      "(u'national_assembly', 2)\n",
      "(u'mythical_place', 2)\n",
      "(u'mythical_being', 2)\n",
      "(u'musical_time', 2)\n",
      "(u'musical_theater', 2)\n",
      "(u'musical_rhythm', 2)\n",
      "(u'music_director', 2)\n",
      "(u'muscular_tissue', 2)\n",
      "(u'mountain_range', 2)\n",
      "(u'mountain_chain', 2)\n",
      "(u'mound', 2)\n",
      "(u'monetary_fund', 2)\n",
      "(u'molecular_biology', 2)\n",
      "(u'missile', 2)\n",
      "(u'minor', 2)\n",
      "(u'minister', 2)\n",
      "(u'miniseries', 2)\n",
      "(u'military_service', 2)\n",
      "(u'military_machine', 2)\n",
      "(u'microhabitat', 2)\n",
      "(u'microenvironment', 2)\n",
      "(u'metric', 2)\n",
      "(u'meter', 2)\n",
      "(u'metallicity', 2)\n",
      "(u'metallic_element', 2)\n",
      "(u'metal', 2)\n",
      "(u'merchant_shipping', 2)\n",
      "(u'merchandising', 2)\n",
      "(u'mentation', 2)\n",
      "(u'memorial', 2)\n",
      "(u'member_of_parliament', 2)\n",
      "(u'medical_specialty', 2)\n",
      "(u'medical_specialist', 2)\n",
      "(u'medical_doctor', 2)\n",
      "(u'medical_care', 2)\n",
      "(u'medical_aid', 2)\n",
      "(u'mathematics_journal', 2)\n",
      "(u'marketplace', 2)\n",
      "(u'marketing', 2)\n",
      "(u'marketer', 2)\n",
      "(u'market_place', 2)\n",
      "(u'market', 2)\n",
      "(u'marine_environment', 2)\n",
      "(u'marine_ecology', 2)\n",
      "(u'marine_creature', 2)\n",
      "(u'marine_animal', 2)\n",
      "(u'manual_labour', 2)\n",
      "(u'manual_labor', 2)\n",
      "(u'maneuver', 2)\n",
      "(u'man_of_letters', 2)\n",
      "(u'making_love', 2)\n",
      "(u'makeup', 2)\n",
      "(u'major_planet', 2)\n",
      "(u'magazine', 2)\n",
      "(u'love', 2)\n",
      "(u'lord', 2)\n",
      "(u'logic', 2)\n",
      "(u'living_accommodations', 2)\n",
      "(u'literate_person', 2)\n",
      "(u'literary_technique', 2)\n",
      "(u'limit', 2)\n",
      "(u'lexical_unit', 2)\n",
      "(u'lexical_class', 2)\n",
      "(u'lexical', 2)\n",
      "(u'lessening', 2)\n",
      "(u'leisure_area', 2)\n",
      "(u'legal_tender', 2)\n",
      "(u'legal_status', 2)\n",
      "(u'legal_proceedings', 2)\n",
      "(u'legal_proceeding', 2)\n",
      "(u'legal_person', 2)\n",
      "(u'learned_person', 2)\n",
      "(u'league', 2)\n",
      "(u'leaf', 2)\n",
      "(u'layer', 2)\n",
      "(u'lawmaker', 2)\n",
      "(u'lawgiver', 2)\n",
      "(u'lawbreaking', 2)\n",
      "(u'law_of_nature', 2)\n",
      "(u'law-breaking', 2)\n",
      "(u'larva', 2)\n",
      "(u'large_integer', 2)\n",
      "(u'landing_field', 2)\n",
      "(u'labourer', 2)\n",
      "(u'labour_market', 2)\n",
      "(u'laborer', 2)\n",
      "(u'labor_market', 2)\n",
      "(u'knowing', 2)\n",
      "(u'kindness', 2)\n",
      "(u'kin', 2)\n",
      "(u'justification', 2)\n",
      "(u'journeying', 2)\n",
      "(u'journey', 2)\n",
      "(u'journalism', 2)\n",
      "(u'job_market', 2)\n",
      "(u'jetport', 2)\n",
      "(u'jeopardy', 2)\n",
      "(u'isle', 2)\n",
      "(u'investment_funds', 2)\n",
      "(u'investment_fund', 2)\n",
      "(u'investment', 2)\n",
      "(u'intoxicant', 2)\n",
      "(u'interval', 2)\n",
      "(u'intersection', 2)\n",
      "(u'intelligence_service', 2)\n",
      "(u'intelligence_assessment', 2)\n",
      "(u'intelligence_agency', 2)\n",
      "(u'integrated_circuit', 2)\n",
      "(u'injustice', 2)\n",
      "(u'infinite', 2)\n",
      "(u'indigenous', 2)\n",
      "(u'indicator', 2)\n",
      "(u'impurity', 2)\n",
      "(u'improvement', 2)\n",
      "(u'ikon', 2)\n",
      "(u'hypothesis', 2)\n",
      "(u'hurt', 2)\n",
      "(u'humor', 2)\n",
      "(u'humanist', 2)\n",
      "(u'human_voice', 2)\n",
      "(u'house_of_prayer', 2)\n",
      "(u'horseman', 2)\n",
      "(u'horseback_rider', 2)\n",
      "(u'hormone', 2)\n",
      "(u'homopterous_insect', 2)\n",
      "(u'homework', 2)\n",
      "(u'home_ground', 2)\n",
      "(u'holy_person', 2)\n",
      "(u'holy_man', 2)\n",
      "(u'hockey_player', 2)\n",
      "(u'history_of_medicine', 2)\n",
      "(u'historiographer', 2)\n",
      "(u'hinderance', 2)\n",
      "(u'health_professional', 2)\n",
      "(u'health_care_provider', 2)\n",
      "(u'head_of_government', 2)\n",
      "(u'head_cook', 2)\n",
      "(u'head_chef', 2)\n",
      "(u'hausdorff_space', 2)\n",
      "(u'hate', 2)\n",
      "(u'harm', 2)\n",
      "(u'hardware', 2)\n",
      "(u'hand_tool', 2)\n",
      "(u'habitat', 2)\n",
      "(u'gymnastic_apparatus', 2)\n",
      "(u'gun', 2)\n",
      "(u'guitarist', 2)\n",
      "(u'guitar_player', 2)\n",
      "(u'guideline', 2)\n",
      "(u'guided_transport', 2)\n",
      "(u'group_behaviour', 2)\n",
      "(u'greyness', 2)\n",
      "(u'green_area', 2)\n",
      "(u'grayness', 2)\n",
      "(u'gray', 2)\n",
      "(u'graphic_art', 2)\n",
      "(u'graphic', 2)\n",
      "(u'grammatical_relation', 2)\n",
      "(u'grain', 2)\n",
      "(u'good_health', 2)\n",
      "(u'glycan', 2)\n",
      "(u'global_market', 2)\n",
      "(u'giving', 2)\n",
      "(u'girlie', 2)\n",
      "(u'gift', 2)\n",
      "(u'geophysical_science', 2)\n",
      "(u'genital', 2)\n",
      "(u'general_assembly', 2)\n",
      "(u'general_anatomy', 2)\n",
      "(u'gem', 2)\n",
      "(u'gazette', 2)\n",
      "(u'gaseous_state', 2)\n",
      "(u'gap', 2)\n",
      "(u'game_equipment', 2)\n",
      "(u'gamble', 2)\n",
      "(u'fundamental_quantity', 2)\n",
      "(u'fundamental_interaction', 2)\n",
      "(u'front', 2)\n",
      "(u'friend', 2)\n",
      "(u'freight_transport', 2)\n",
      "(u'free_software', 2)\n",
      "(u'frat', 2)\n",
      "(u'fragment', 2)\n",
      "(u'formal_language', 2)\n",
      "(u'form_of_address', 2)\n",
      "(u'footgear', 2)\n",
      "(u'football_game', 2)\n",
      "(u'football', 2)\n",
      "(u'food_grain', 2)\n",
      "(u'follower', 2)\n",
      "(u'flying_machine', 2)\n",
      "(u'flux_density', 2)\n",
      "(u'fluid_mechanics', 2)\n",
      "(u'fluid_dynamics', 2)\n",
      "(u'flowing', 2)\n",
      "(u'floral_design', 2)\n",
      "(u'floor', 2)\n",
      "(u'fixing', 2)\n",
      "(u'five_senses', 2)\n",
      "(u'financial_organisation', 2)\n",
      "(u'financial_gain', 2)\n",
      "(u'file_format', 2)\n",
      "(u'fighter', 2)\n",
      "(u'fight', 2)\n",
      "(u'fertilizer', 2)\n",
      "(u'female_animal', 2)\n",
      "(u'feline', 2)\n",
      "(u'fashion_accessory', 2)\n",
      "(u'family_unit', 2)\n",
      "(u'family_relationship', 2)\n",
      "(u'extract', 2)\n",
      "(u'exposure', 2)\n",
      "(u'expiry', 2)\n",
      "(u'experimentation', 2)\n",
      "(u'experiment', 2)\n",
      "(u'expense', 2)\n",
      "(u'expansion', 2)\n",
      "(u'existence', 2)\n",
      "(u'exercise', 2)\n",
      "(u'error', 2)\n",
      "(u'eristic', 2)\n",
      "(u'equus', 2)\n",
      "(u'equine', 2)\n",
      "(u'equid', 2)\n",
      "(u'environmental_condition', 2)\n",
      "(u'environment_variable', 2)\n",
      "(u'enlisted_person', 2)\n",
      "(u'endangerment', 2)\n",
      "(u'emission', 2)\n",
      "(u'electronic_network', 2)\n",
      "(u'electromagnetic_wave', 2)\n",
      "(u'electricity', 2)\n",
      "(u'election_district', 2)\n",
      "(u'edible_fruit', 2)\n",
      "(u'economist', 2)\n",
      "(u'economic_sector', 2)\n",
      "(u'earthquake', 2)\n",
      "(u'due_process', 2)\n",
      "(u'drug_of_abuse', 2)\n",
      "(u'domesticated_animal', 2)\n",
      "(u'domestic_dog', 2)\n",
      "(u'domestic_animal', 2)\n",
      "(u'dogness', 2)\n",
      "(u'dog_type', 2)\n",
      "(u'division', 2)\n",
      "(u'distaste', 2)\n",
      "(u'dislike', 2)\n",
      "(u'disgust', 2)\n",
      "(u'diseased_person', 2)\n",
      "(u'discovery', 2)\n",
      "(u'discharge', 2)\n",
      "(u'dirtying', 2)\n",
      "(u'dirtiness', 2)\n",
      "(u'digital_computer', 2)\n",
      "(u'dicking', 2)\n",
      "(u'devoutness', 2)\n",
      "(u'determination', 2)\n",
      "(u'destruction', 2)\n",
      "(u'descendants', 2)\n",
      "(u'deposit', 2)\n",
      "(u'dependent_territory', 2)\n",
      "(u'dependent_area', 2)\n",
      "(u'denial', 2)\n",
      "(u'delivery', 2)\n",
      "(u'delicacy', 2)\n",
      "(u'decrease', 2)\n",
      "(u'decision-making', 2)\n",
      "(u'decimal_digit', 2)\n",
      "(u'deciding', 2)\n",
      "(u'decease', 2)\n",
      "(u'debt_instrument', 2)\n",
      "(u'dbms', 2)\n",
      "(u'data_type', 2)\n",
      "(u'data_processing', 2)\n",
      "(u'daily', 2)\n",
      "(u'curve', 2)\n",
      "(u'cultivation', 2)\n",
      "(u'crustacean', 2)\n",
      "(u'crustacea', 2)\n",
      "(u'critic', 2)\n",
      "(u'criminal', 2)\n",
      "(u'crew_member', 2)\n",
      "(u'coverall', 2)\n",
      "(u'course_of_study', 2)\n",
      "(u'course_of_instruction', 2)\n",
      "(u'county_seat', 2)\n",
      "(u'counting_number', 2)\n",
      "(u'cosmetics', 2)\n",
      "(u'cosmetic', 2)\n",
      "(u'corporate_title', 2)\n",
      "(u'corporate_identity', 2)\n",
      "(u'copulation', 2)\n",
      "(u'cook', 2)\n",
      "(u'convex_shape', 2)\n",
      "(u'contract', 2)\n",
      "(u'contagion', 2)\n",
      "(u'contact_sport', 2)\n",
      "(u'consumer_durables', 2)\n",
      "(u'constructed_language', 2)\n",
      "(u'conservative', 2)\n",
      "(u'conservancy', 2)\n",
      "(u'connective', 2)\n",
      "(u'confectionary', 2)\n",
      "(u'conductor', 2)\n",
      "(u'comune', 2)\n",
      "(u'computer_hardware', 2)\n",
      "(u'computer_address', 2)\n",
      "(u'committee', 2)\n",
      "(u'commentary', 2)\n",
      "(u'comedy-drama', 2)\n",
      "(u'combination', 2)\n",
      "(u'colourant', 2)\n",
      "(u'colour', 2)\n",
      "(u'colony', 2)\n",
      "(u'collective_investment_scheme', 2)\n",
      "(u'coleopteran', 2)\n",
      "(u'coalition', 2)\n",
      "(u'closed_circulatory_system', 2)\n",
      "(u'classical', 2)\n",
      "(u'civil_engineering_construction', 2)\n",
      "(u'civil_engineering', 2)\n",
      "(u'christian', 2)\n",
      "(u'chip', 2)\n",
      "(u'chemical_reaction', 2)\n",
      "(u'chemical_bond', 2)\n",
      "(u'chef', 2)\n",
      "(u'charge', 2)\n",
      "(u'chain_of_mountains', 2)\n",
      "(u'certificate', 2)\n",
      "(u'ceremony', 2)\n",
      "(u'ceremonial_occasion', 2)\n",
      "(u'cereal_grass', 2)\n",
      "(u'cell', 2)\n",
      "(u'celebration', 2)\n",
      "(u'cause_of_death', 2)\n",
      "(u'cat', 2)\n",
      "(u'case', 2)\n",
      "(u'care', 2)\n",
      "(u'carbohydrate', 2)\n",
      "(u'canis_familiaris', 2)\n",
      "(u'canine', 2)\n",
      "(u'campaign', 2)\n",
      "(u'calamity', 2)\n",
      "(u'business_firm', 2)\n",
      "(u'bus_stop', 2)\n",
      "(u'building_site', 2)\n",
      "(u'building_complex', 2)\n",
      "(u'broadcasting', 2)\n",
      "(u'broadcaster', 2)\n",
      "(u'breed', 2)\n",
      "(u'break', 2)\n",
      "(u'brand_name', 2)\n",
      "(u'brain', 2)\n",
      "(u'boutique', 2)\n",
      "(u'boundary_line', 2)\n",
      "(u'boundary', 2)\n",
      "(u'border', 2)\n",
      "(u'bookselling', 2)\n",
      "(u'bony_fish', 2)\n",
      "(u'bond_servant', 2)\n",
      "(u'bond', 2)\n",
      "(u'body_covering', 2)\n",
      "(u'body_armor', 2)\n",
      "(u'bodily_fluid', 2)\n",
      "(u'blue_blood', 2)\n",
      "(u'blood_supply', 2)\n",
      "(u'black_music', 2)\n",
      "(u'black_magick', 2)\n",
      "(u'black_art', 2)\n",
      "(u'biopolymer', 2)\n",
      "(u'bill', 2)\n",
      "(u'big_cat', 2)\n",
      "(u'bibliology', 2)\n",
      "(u'beverage', 2)\n",
      "(u'beetles', 2)\n",
      "(u'beaux_arts', 2)\n",
      "(u'beat_level', 2)\n",
      "(u'battle', 2)\n",
      "(u'baseball_game', 2)\n",
      "(u'ballplayer', 2)\n",
      "(u'ballgame', 2)\n",
      "(u'automotive_vehicle', 2)\n",
      "(u'authorization', 2)\n",
      "(u'authorisation', 2)\n",
      "(u'attack', 2)\n",
      "(u'atmospheric_precipitation', 2)\n",
      "(u'athletic_game', 2)\n",
      "(u'athletic_conference', 2)\n",
      "(u'assignment', 2)\n",
      "(u'asset', 2)\n",
      "(u'asian_country', 2)\n",
      "(u'artistic_movement', 2)\n",
      "(u'artificial_satellite', 2)\n",
      "(u'articulator', 2)\n",
      "(u'art_music', 2)\n",
      "(u'art_movement', 2)\n",
      "(u'aromatic_plant', 2)\n",
      "(u'army_unit', 2)\n",
      "(u'armour', 2)\n",
      "(u'armed_services', 2)\n",
      "(u'arm', 2)\n",
      "(u'argument', 2)\n",
      "(u'arena', 2)\n",
      "(u'architecture', 2)\n",
      "(u'archeologist', 2)\n",
      "(u'aquatic_bird', 2)\n",
      "(u'applied_scientist', 2)\n",
      "(u'anti-social_behaviour', 2)\n",
      "(u'anthropologist', 2)\n",
      "(u'antecedent', 2)\n",
      "(u'announcement', 2)\n",
      "(u'animal_group', 2)\n",
      "(u'angularity', 2)\n",
      "(u'angular_shape', 2)\n",
      "(u'ancestor', 2)\n",
      "(u'ammunition', 2)\n",
      "(u'american_literature', 2)\n",
      "(u'alphabetic_character', 2)\n",
      "(u'alloy', 2)\n",
      "(u'allotment', 2)\n",
      "(u'allocation', 2)\n",
      "(u'alkaloid', 2)\n",
      "(u'algebra', 2)\n",
      "(u'alcoholic_drink', 2)\n",
      "(u'alcoholic_beverage', 2)\n",
      "(u'airfoil', 2)\n",
      "(u'aircraft_engine', 2)\n",
      "(u'air_travel', 2)\n",
      "(u'age', 2)\n",
      "(u'african-american_music', 2)\n",
      "(u'affair', 2)\n",
      "(u'aerofoil', 2)\n",
      "(u'advocator', 2)\n",
      "(u'advert', 2)\n",
      "(u'adp_system', 2)\n",
      "(u'administrative_centre', 2)\n",
      "(u'adjudicator', 2)\n",
      "(u'adjective', 2)\n",
      "(u'address', 2)\n",
      "(u'activist', 2)\n",
      "(u'action_film', 2)\n",
      "(u'acres', 2)\n",
      "(u'acquirer', 2)\n",
      "(u'acid', 2)\n",
      "(u'abrasive_material', 2)\n",
      "(u'abrasive', 2)\n",
      "(u'zoology', 1)\n",
      "(u'zoological_science', 1)\n",
      "(u'zoological_park', 1)\n",
      "(u'zoological_garden', 1)\n",
      "(u'zoo', 1)\n",
      "(u'youth_subculture', 1)\n",
      "(u'youth_culture', 1)\n",
      "(u'young_woman', 1)\n",
      "(u'yellow', 1)\n",
      "(u'years', 1)\n",
      "(u'wrong', 1)\n",
      "(u'written_text', 1)\n",
      "(u'writing_implement', 1)\n",
      "(u'worshipping', 1)\n",
      "(u'worship', 1)\n",
      "(u'world_organization', 1)\n",
      "(u'world_organisation', 1)\n",
      "(u'workshop', 1)\n",
      "(u'workforce', 1)\n",
      "(u'work_unit', 1)\n",
      "(u'work_force', 1)\n",
      "(u'word_class', 1)\n",
      "(u'wonder', 1)\n",
      "(u'wittiness', 1)\n",
      "(u'witticism', 1)\n",
      "(u'withdrawal', 1)\n",
      "(u'wit', 1)\n",
      "(u'wisdom', 1)\n",
      "(u'winter_sport', 1)\n",
      "(u'wing', 1)\n",
      "(u'wilderness_area', 1)\n",
      "(u'wilderness', 1)\n",
      "(u'wildcat', 1)\n",
      "(u'wild_ox', 1)\n",
      "(u'wild_flower', 1)\n",
      "(u'wild_dog', 1)\n",
      "(u'wild', 1)\n",
      "(u'wig', 1)\n",
      "(u'wifie', 1)\n",
      "(u'wife', 1)\n",
      "(u'white_potato', 1)\n",
      "(u'white', 1)\n",
      "(u'weight_unit', 1)\n",
      "(u'weed_killer', 1)\n",
      "(u'web_services', 1)\n",
      "(u'web_service', 1)\n",
      "(u'web_application', 1)\n",
      "(u'web_app', 1)\n",
      "(u'wealth', 1)\n",
      "(u'weakness', 1)\n",
      "(u'weakening', 1)\n",
      "(u'waterfowl', 1)\n",
      "(u'waterbird', 1)\n",
      "(u'water_ice', 1)\n",
      "(u'waste_product', 1)\n",
      "(u'waste_pipe', 1)\n",
      "(u'waste_matter', 1)\n",
      "(u'waste_material', 1)\n",
      "(u'waste', 1)\n",
      "(u'war_paint', 1)\n",
      "(u'war_machine', 1)\n",
      "(u'wall_painting', 1)\n",
      "(u'wagon', 1)\n",
      "(u'vp', 1)\n",
      "(u'voting_precinct', 1)\n",
      "(u'volume', 1)\n",
      "(u'voice_quality', 1)\n",
      "(u'vocation', 1)\n",
      "(u'vocalizer', 1)\n",
      "(u'vocalization', 1)\n",
      "(u'vocal_music', 1)\n",
      "(u'vitis_vinifera', 1)\n",
      "(u'visual_signal', 1)\n",
      "(u'visual_property', 1)\n",
      "(u'visual_percept', 1)\n",
      "(u'visual_impairment', 1)\n",
      "(u'visual_image', 1)\n",
      "(u'visual_aspect', 1)\n",
      "(u'viscosity', 1)\n",
      "(u'virtue', 1)\n",
      "(u'violet', 1)\n",
      "(u'vinifera_grape', 1)\n",
      "(u'vinifera', 1)\n",
      "(u'vine', 1)\n",
      "(u'view', 1)\n",
      "(u'videocassette', 1)\n",
      "(u'video_game_developer', 1)\n",
      "(u'victim', 1)\n",
      "(u'vice_president', 1)\n",
      "(u'vibration', 1)\n",
      "(u'vertical', 1)\n",
      "(u'venue', 1)\n",
      "(u'venturer', 1)\n",
      "(u'ventilation', 1)\n",
      "(u'venetian_red', 1)\n",
      "(u'venereal_infection', 1)\n",
      "(u'venereal_disease', 1)\n",
      "(u'vendor', 1)\n",
      "(u'vender', 1)\n",
      "(u'variety_meat', 1)\n",
      "(u'valve', 1)\n",
      "(u'valuable', 1)\n",
      "(u'vale', 1)\n",
      "(u'v.p.', 1)\n",
      "(u'utterance', 1)\n",
      "(u'utilisation', 1)\n",
      "(u'user_interface', 1)\n",
      "(u'user', 1)\n",
      "(u'us_navy', 1)\n",
      "(u'upper_house', 1)\n",
      "(u'upper_deck', 1)\n",
      "(u'upper_chamber', 1)\n",
      "(u'untruth', 1)\n",
      "(u'unskillfulness', 1)\n",
      "(u'unrighteousness', 1)\n",
      "(u'unpleasantness', 1)\n",
      "(u'unnaturalness', 1)\n",
      "(u'unnatural_death', 1)\n",
      "(u'unmarried', 1)\n",
      "(u'unjustness', 1)\n",
      "(u'universe', 1)\n",
      "(u'uniting', 1)\n",
      "(u'united_states_president', 1)\n",
      "(u'unit_of_measure', 1)\n",
      "(u'unit_of_length', 1)\n",
      "(u'unit_of_information', 1)\n",
      "(u'uniform_title', 1)\n",
      "(u'uniform', 1)\n",
      "(u'unification', 1)\n",
      "(u'ungulate', 1)\n",
      "(u'unfortunate_person', 1)\n",
      "(u'unfortunate', 1)\n",
      "(u'unfitness', 1)\n",
      "(u'unfaithfulness', 1)\n",
      "(u'undesirable', 1)\n",
      "(u'unconnectedness', 1)\n",
      "(u'unconditioned_reflex', 1)\n",
      "(u'umpire', 1)\n",
      "(u'tyrant_flycatcher', 1)\n",
      "(u'typeface', 1)\n",
      "(u'type_of_architecture', 1)\n",
      "(u'tyke', 1)\n",
      "(u'two-dimensional_figure', 1)\n",
      "(u'twist', 1)\n",
      "(u'turbulent_flow', 1)\n",
      "(u'turbulence', 1)\n",
      "(u'turbomachinery', 1)\n",
      "(u'turbomachine', 1)\n",
      "(u'tunnel', 1)\n",
      "(u'tune', 1)\n",
      "(u'tummy', 1)\n",
      "(u'truncheon', 1)\n",
      "(u'truck', 1)\n",
      "(u'trousers', 1)\n",
      "(u'trouser', 1)\n",
      "(u'troupe', 1)\n",
      "(u'tropical_year', 1)\n",
      "(u'trinity', 1)\n",
      "(u'trilateral', 1)\n",
      "(u'trigon', 1)\n",
      "(u'trickster', 1)\n",
      "(u'tribe', 1)\n",
      "(u'triangle', 1)\n",
      "(u'trial', 1)\n",
      "(u'triad', 1)\n",
      "(u'trespasser', 1)\n",
      "(u'trend', 1)\n",
      "(u'treaty', 1)\n",
      "(u'treasure', 1)\n",
      "(u'trauma', 1)\n",
      "(u'trash', 1)\n",
      "(u'transportation_company', 1)\n",
      "(u'transplantation', 1)\n",
      "(u'transplant', 1)\n",
      "(u'transmitter', 1)\n",
      "(u'transmission_system', 1)\n",
      "(u'transmission_medium', 1)\n",
      "(u'transition_metal', 1)\n",
      "(u'transition_element', 1)\n",
      "(u'transit', 1)\n",
      "(u'transfer_paper', 1)\n",
      "(u'transducer', 1)\n",
      "(u'transcription', 1)\n",
      "(u'train_track', 1)\n",
      "(u'train', 1)\n",
      "(u'tragicomedy', 1)\n",
      "(u'tragic', 1)\n",
      "(u'traffic', 1)\n",
      "(u'tradesman', 1)\n",
      "(u'trader', 1)\n",
      "(u'track_event', 1)\n",
      "(u'toy_dog', 1)\n",
      "(u'townspeople', 1)\n",
      "(u'town_square', 1)\n",
      "(u'town_hall', 1)\n",
      "(u'tower_block', 1)\n",
      "(u'tourney', 1)\n",
      "(u'tournament', 1)\n",
      "(u'touching', 1)\n",
      "(u'torturing', 1)\n",
      "(u'torture', 1)\n",
      "(u'tone', 1)\n",
      "(u'toilet', 1)\n",
      "(u'toil', 1)\n",
      "(u'tissue_layer', 1)\n",
      "(u'timing', 1)\n",
      "(u'times', 1)\n",
      "(u'time_standard', 1)\n",
      "(u'tike', 1)\n",
      "(u'tie-in', 1)\n",
      "(u'throwing', 1)\n",
      "(u'throwaway', 1)\n",
      "(u'throw', 1)\n",
      "(u'threesome', 1)\n",
      "(u'thread_of_execution', 1)\n",
      "(u'thread', 1)\n",
      "(u'thralldom', 1)\n",
      "(u'thrall', 1)\n",
      "(u'thraldom', 1)\n",
      "(u'thoughtfulness', 1)\n",
      "(u'thinker', 1)\n",
      "(u'therapy', 1)\n",
      "(u'therapeutic', 1)\n",
      "(u'theory', 1)\n",
      "(u'theology', 1)\n",
      "(u'theologist', 1)\n",
      "(u'theological_system', 1)\n",
      "(u'theologian', 1)\n",
      "(u'theocracy', 1)\n",
      "(u'theme', 1)\n",
      "(u'theatrical_producer', 1)\n",
      "(u'the_mall', 1)\n",
      "(u'thaumaturgy', 1)\n",
      "(u'textual_matter', 1)\n",
      "(u'textile_machine', 1)\n",
      "(u'text_edition', 1)\n",
      "(u'tertiary_source', 1)\n",
      "(u'terror', 1)\n",
      "(u'ternary_alloy', 1)\n",
      "(u'terminus', 1)\n",
      "(u'term_logic', 1)\n",
      "(u'tenor', 1)\n",
      "(u'tennis_player', 1)\n",
      "(u'tending', 1)\n",
      "(u'temporal_order', 1)\n",
      "(u'temperature_unit', 1)\n",
      "(u'temperature_scale', 1)\n",
      "(u'temperature', 1)\n",
      "(u'temblor', 1)\n",
      "(u'television_station', 1)\n",
      "(u'television_director', 1)\n",
      "(u'telecommunications', 1)\n",
      "(u'teaching_reading', 1)\n",
      "(u'taxation', 1)\n",
      "(u'tax', 1)\n",
      "(u'tater', 1)\n",
      "(u'tape', 1)\n",
      "(u'tactical_manoeuvre', 1)\n",
      "(u'table_service', 1)\n",
      "(u'table_game', 1)\n",
      "(u'system_of_logic', 1)\n",
      "(u'system_camera', 1)\n",
      "(u'syntagma', 1)\n",
      "(u'syndicate', 1)\n",
      "(u'sword', 1)\n",
      "(u'switch', 1)\n",
      "(u'swine', 1)\n",
      "(u'swiftness', 1)\n",
      "(u'sweetshop', 1)\n",
      "(u'swan', 1)\n",
      "(u'susceptibility', 1)\n",
      "(u'surprise', 1)\n",
      "(u'supporting_structure', 1)\n",
      "(u'supporting', 1)\n",
      "(u'supply', 1)\n",
      "(u'supplier', 1)\n",
      "(u'supervisor', 1)\n",
      "(u'supertall', 1)\n",
      "(u'superpartner', 1)\n",
      "(u'superordinate', 1)\n",
      "(u'supernaturalism', 1)\n",
      "(u'superior_planet', 1)\n",
      "(u'sulfate_minerals', 1)\n",
      "(u'suit_of_clothes', 1)\n",
      "(u'suit_of_armour', 1)\n",
      "(u'suit_of_armor', 1)\n",
      "(u'sugars', 1)\n",
      "(u'suffragist', 1)\n",
      "(u'sufferer', 1)\n",
      "(u'subway_system', 1)\n",
      "(u'substitution_cipher', 1)\n",
      "(u'substitute', 1)\n",
      "(u'subspecies', 1)\n",
      "(u'subsidiary_company', 1)\n",
      "(u'subsidiary', 1)\n",
      "(u'subprovince', 1)\n",
      "(u'subjugation', 1)\n",
      "(u'subjection', 1)\n",
      "(u'stylus', 1)\n",
      "(u'style_of_architecture', 1)\n",
      "(u'structural_failure', 1)\n",
      "(u'structural_analysis', 1)\n",
      "(u'string', 1)\n",
      "(u'striated_muscle_tissue', 1)\n",
      "(u'striated_muscle', 1)\n",
      "(u'stretching', 1)\n",
      "(u'stress', 1)\n",
      "(u'street_market', 1)\n",
      "(u'street_dance', 1)\n",
      "(u'stray_dog', 1)\n",
      "(u'stratum', 1)\n",
      "(u'strait', 1)\n",
      "(u'strain', 1)\n",
      "(u'storekeeper', 1)\n",
      "(u'stopper', 1)\n",
      "(u'stomach', 1)\n",
      "(u'stock_market', 1)\n",
      "(u'stock_exchange', 1)\n",
      "(u'stock_character', 1)\n",
      "(u'stock', 1)\n",
      "(u'stitch', 1)\n",
      "(u'stimulant_drug', 1)\n",
      "(u'stew', 1)\n",
      "(u'steroid_hormone', 1)\n",
      "(u'steroid', 1)\n",
      "(u'step-up', 1)\n",
      "(u'step', 1)\n",
      "(u'steering', 1)\n",
      "(u'stay', 1)\n",
      "(u'statute', 1)\n",
      "(u'statistics', 1)\n",
      "(u'statistical_analysis', 1)\n",
      "(u'state_function', 1)\n",
      "(u'state_capital', 1)\n",
      "(u'star_sign', 1)\n",
      "(u'star', 1)\n",
      "(u'stair', 1)\n",
      "(u'spysat', 1)\n",
      "(u'spy_satellite', 1)\n",
      "(u'spreading', 1)\n",
      "(u'spread', 1)\n",
      "(u'spout', 1)\n",
      "(u'spouse', 1)\n",
      "(u'sports_venue', 1)\n",
      "(u'sports_organization', 1)\n",
      "(u'sports_ground', 1)\n",
      "(u'sports_equipment', 1)\n",
      "(u'sponsor', 1)\n",
      "(u'spoken_language', 1)\n",
      "(u'spoken_communication', 1)\n",
      "(u'spiritism', 1)\n",
      "(u'sphere', 1)\n",
      "(u'speeding', 1)\n",
      "(u'speech_sound', 1)\n",
      "(u'specialist_degree', 1)\n",
      "(u'specialist', 1)\n",
      "(u'speaker_system', 1)\n",
      "(u'spatiality', 1)\n",
      "(u'spatial_arrangement', 1)\n",
      "(u'spasm', 1)\n",
      "(u'spar', 1)\n",
      "(u'spacing', 1)\n",
      "(u'spaceflight', 1)\n",
      "(u'spacefaring', 1)\n",
      "(u'space_travel', 1)\n",
      "(u'space_flight', 1)\n",
      "(u'source_text', 1)\n",
      "(u'source_code', 1)\n",
      "(u'soup', 1)\n",
      "(u'soundtrack_album', 1)\n",
      "(u'sound_system', 1)\n",
      "(u'sound_reproduction', 1)\n",
      "(u'sound_recording', 1)\n",
      "(u'sound_change', 1)\n",
      "(u'sorting', 1)\n",
      "(u'sort', 1)\n",
      "(u'sorcery', 1)\n",
      "(u'somatic_chromosome', 1)\n",
      "(u'solution', 1)\n",
      "(u'soln', 1)\n",
      "(u'solid_solution', 1)\n",
      "(u'solar_year', 1)\n",
      "(u'soiling', 1)\n",
      "(u'software_suite', 1)\n",
      "(u'software_standard', 1)\n",
      "(u'software_framework', 1)\n",
      "(u'software_developer', 1)\n",
      "(u'software_company', 1)\n",
      "(u'softback_book', 1)\n",
      "(u'soft_tissue', 1)\n",
      "(u'soft_drug', 1)\n",
      "(u'soft-cover_book', 1)\n",
      "(u'sociologist', 1)\n",
      "(u'socio-economic_class', 1)\n",
      "(u'social_station', 1)\n",
      "(u'social_relation', 1)\n",
      "(u'social_reformer', 1)\n",
      "(u'social_psychology', 1)\n",
      "(u'social_norm', 1)\n",
      "(u'social_gathering', 1)\n",
      "(u'social_class', 1)\n",
      "(u'soccerball', 1)\n",
      "(u'soccer', 1)\n",
      "(u'snatch', 1)\n",
      "(u'snake', 1)\n",
      "(u'smoking', 1)\n",
      "(u'smoke', 1)\n",
      "(u'small_fry', 1)\n",
      "(u'small_boat', 1)\n",
      "(u'small_arms', 1)\n",
      "(u'small-arm', 1)\n",
      "(u'sloven', 1)\n",
      "(u'slope', 1)\n",
      "(u'slob', 1)\n",
      "(u'sleepwear', 1)\n",
      "(u'sleep_disturbance', 1)\n",
      "(u'sleep_disorder', 1)\n",
      "(u'slavery', 1)\n",
      "(u'slave_labour', 1)\n",
      "(u'slammer', 1)\n",
      "(u'skyscraper', 1)\n",
      "(u'skin_disorder', 1)\n",
      "(u'skin_disease', 1)\n",
      "(u'skin_colour', 1)\n",
      "(u'skin_color', 1)\n",
      "(u'skin', 1)\n",
      "(u'skillfulness', 1)\n",
      "(u'skeletal_muscle', 1)\n",
      "(u'skater', 1)\n",
      "(u'skateboarder', 1)\n",
      "(u'sitcom', 1)\n",
      "(u'single', 1)\n",
      "(u'singing_voice', 1)\n",
      "(u'simple_sugar', 1)\n",
      "(u'simple_machine', 1)\n",
      "(u'silverware', 1)\n",
      "(u'silicon_chip', 1)\n",
      "(u'silicate', 1)\n",
      "(u'silent_treatment', 1)\n",
      "(u'silence', 1)\n",
      "(u'signaling_molecule', 1)\n",
      "(u'siding', 1)\n",
      "(u'side', 1)\n",
      "(u'sibling', 1)\n",
      "(u'sib', 1)\n",
      "(u'si_base_unit', 1)\n",
      "(u'showcase', 1)\n",
      "(u'short_title', 1)\n",
      "(u'shopping_precinct', 1)\n",
      "(u'shopping_mall', 1)\n",
      "(u'shopping_centre', 1)\n",
      "(u'shopping_center', 1)\n",
      "(u'shopping_arcade', 1)\n",
      "(u'shopkeeper', 1)\n",
      "(u'shooting_sport', 1)\n",
      "(u'shooting', 1)\n",
      "(u\"shoot_'em_up\", 1)\n",
      "(u'shoot', 1)\n",
      "(u'shit', 1)\n",
      "(u'shire_town', 1)\n",
      "(u'shipper', 1)\n",
      "(u'shift', 1)\n",
      "(u'shellfish', 1)\n",
      "(u'shell_fish', 1)\n",
      "(u'shell', 1)\n",
      "(u'sheath', 1)\n",
      "(u'sharing', 1)\n",
      "(u'sharemarket', 1)\n",
      "(u'share-out', 1)\n",
      "(u'shaper', 1)\n",
      "(u'shagging', 1)\n",
      "(u'shade', 1)\n",
      "(u'sexual_orientation', 1)\n",
      "(u'sex_steroid', 1)\n",
      "(u'sex_hormone', 1)\n",
      "(u'settler', 1)\n",
      "(u'set_phrase', 1)\n",
      "(u'servitude', 1)\n",
      "(u'service_industry', 1)\n",
      "(u'servant', 1)\n",
      "(u'serpent', 1)\n",
      "(u'serious_music', 1)\n",
      "(u'serial_murderer', 1)\n",
      "(u'serial_killer', 1)\n",
      "(u'sepulchre', 1)\n",
      "(u'separate_legal_entity', 1)\n",
      "(u'sentiency', 1)\n",
      "(u'sensory_system', 1)\n",
      "(u'sensor', 1)\n",
      "(u'sensing_element', 1)\n",
      "(u'sensing', 1)\n",
      "(u'sense_modality', 1)\n",
      "(u'sense_impression', 1)\n",
      "(u'sense_experience', 1)\n",
      "(u'sense_datum', 1)\n",
      "(u'sense', 1)\n",
      "(u'sending', 1)\n",
      "(u'senator', 1)\n",
      "(u'senate', 1)\n",
      "(u'semitic_people', 1)\n",
      "(u'semitic', 1)\n",
      "(u'semite', 1)\n",
      "(u'semiconductor_unit', 1)\n",
      "(u'semiconductor_device', 1)\n",
      "(u'semiconducting_material', 1)\n",
      "(u'semblance', 1)\n",
      "(u'selflessness', 1)\n",
      "(u'self-propelled_vehicle', 1)\n",
      "(u'self-denial', 1)\n",
      "(u'self-control', 1)\n",
      "(u'seism', 1)\n",
      "(u'segment', 1)\n",
      "(u'securities_market', 1)\n",
      "(u'secretion', 1)\n",
      "(u'secret_code', 1)\n",
      "(u'secondary_school', 1)\n",
      "(u'sealife', 1)\n",
      "(u'seabird', 1)\n",
      "(u'sea_duck', 1)\n",
      "(u'sea_bird', 1)\n",
      "(u'scrutiny', 1)\n",
      "(u'scriptwriting', 1)\n",
      "(u'scriptwriter', 1)\n",
      "(u'scripture', 1)\n",
      "(u'screenwriting', 1)\n",
      "(u'screenwriter', 1)\n",
      "(u'scrapper', 1)\n",
      "(u'scrape', 1)\n",
      "(u'scrap', 1)\n",
      "(u'science_laboratory', 1)\n",
      "(u'science_fiction', 1)\n",
      "(u'sci-fi', 1)\n",
      "(u'school_text', 1)\n",
      "(u'school_assignment', 1)\n",
      "(u'schedule', 1)\n",
      "(u'scene', 1)\n",
      "(u'scale_of_temperature', 1)\n",
      "(u'scale_of_measurement', 1)\n",
      "(u'scale', 1)\n",
      "(u'saying', 1)\n",
      "(u'savvy', 1)\n",
      "(u'savant', 1)\n",
      "(u'sapwood', 1)\n",
      "(u'sandstone', 1)\n",
      "(u'sandal', 1)\n",
      "(u'sand', 1)\n",
      "(u'saltwater_fish', 1)\n",
      "(u'salt', 1)\n",
      "(u'salpinx', 1)\n",
      "(u'salesperson', 1)\n",
      "(u'salesclerk', 1)\n",
      "(u'sales_representative', 1)\n",
      "(u'sales_outlet', 1)\n",
      "(u'sales_agent', 1)\n",
      "(u'sales', 1)\n",
      "(u'sale', 1)\n",
      "(u'salad', 1)\n",
      "(u'saffron_yellow', 1)\n",
      "(u'saffron', 1)\n",
      "(u'sacred_writing', 1)\n",
      "(u'sacred_text', 1)\n",
      "(u'sacred_scripture', 1)\n",
      "(u'saccharide', 1)\n",
      "(u'rv', 1)\n",
      "(u'russian_literature', 1)\n",
      "(u'runway', 1)\n",
      "(u'runner-up', 1)\n",
      "(u'run', 1)\n",
      "(u'rumour', 1)\n",
      "(u'rumor', 1)\n",
      "(u'rumination', 1)\n",
      "(u'rugby_union', 1)\n",
      "(u'rubber', 1)\n",
      "(u'round', 1)\n",
      "(u'rotary_engine', 1)\n",
      "(u'roof_terrace', 1)\n",
      "(u'roof', 1)\n",
      "(u'romp', 1)\n",
      "(u'roman_province', 1)\n",
      "(u'roman_mythology', 1)\n",
      "(u'roman_deity', 1)\n",
      "(u'roman_alphabet', 1)\n",
      "(u'role', 1)\n",
      "(u'rodentia', 1)\n",
      "(u'rodent', 1)\n",
      "(u'rod', 1)\n",
      "(u'rocky_planet', 1)\n",
      "(u'rocket_scientist', 1)\n",
      "(u'rocket_engineer', 1)\n",
      "(u'rocker', 1)\n",
      "(u'rock_star', 1)\n",
      "(u'rock_and_roll', 1)\n",
      "(u'roadway', 1)\n",
      "(u'road_traffic', 1)\n",
      "(u'rivalry', 1)\n",
      "(u'rights', 1)\n",
      "(u'right_of_way', 1)\n",
      "(u'right', 1)\n",
      "(u'riding_horse', 1)\n",
      "(u'riches', 1)\n",
      "(u'ribbon', 1)\n",
      "(u'rib', 1)\n",
      "(u'rhythmic_pattern', 1)\n",
      "(u'rhythm', 1)\n",
      "(u'rhyming', 1)\n",
      "(u'rhyme', 1)\n",
      "(u'reward', 1)\n",
      "(u'revulsion', 1)\n",
      "(u'revenue_enhancement', 1)\n",
      "(u'retreat', 1)\n",
      "(u'retarding_force', 1)\n",
      "(u'retail_trade', 1)\n",
      "(u'restaurant', 1)\n",
      "(u'rest', 1)\n",
      "(u'respiratory_disease', 1)\n",
      "(u'respiration', 1)\n",
      "(u'resource', 1)\n",
      "(u'resolvent', 1)\n",
      "(u'resistance', 1)\n",
      "(u'resignation', 1)\n",
      "(u'residential_district', 1)\n",
      "(u'residential_area', 1)\n",
      "(u'residential', 1)\n",
      "(u'resident', 1)\n",
      "(u'reservoir', 1)\n",
      "(u'research_laboratory', 1)\n",
      "(u'research_lab', 1)\n",
      "(u'research_institute', 1)\n",
      "(u'research_centre', 1)\n",
      "(u'research_center', 1)\n",
      "(u'reptilian', 1)\n",
      "(u'reptile', 1)\n",
      "(u'reproduction', 1)\n",
      "(u'replication', 1)\n",
      "(u'reparation', 1)\n",
      "(u'repair', 1)\n",
      "(u'renunciation', 1)\n",
      "(u'renewable_resource', 1)\n",
      "(u'remedy', 1)\n",
      "(u'religious_writing', 1)\n",
      "(u'religious_text', 1)\n",
      "(u'religious_music', 1)\n",
      "(u'religious_leader', 1)\n",
      "(u'release', 1)\n",
      "(u'relaxation', 1)\n",
      "(u'relative_majority', 1)\n",
      "(u'reinforcement', 1)\n",
      "(u'regulation', 1)\n",
      "(u'regular_coffee', 1)\n",
      "(u'registry', 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'register', 1)\n",
      "(u'refuse', 1)\n",
      "(u'reformist', 1)\n",
      "(u'reformer', 1)\n",
      "(u'reflex_response', 1)\n",
      "(u'reflection', 1)\n",
      "(u'refined_sugar', 1)\n",
      "(u'reference_work', 1)\n",
      "(u'reference_book', 1)\n",
      "(u'referee', 1)\n",
      "(u'reduction', 1)\n",
      "(u'redox', 1)\n",
      "(u'red', 1)\n",
      "(u'rectification', 1)\n",
      "(u'recreational_vehicle', 1)\n",
      "(u'record_producer', 1)\n",
      "(u'record_company', 1)\n",
      "(u'reconstruction', 1)\n",
      "(u'reckoning', 1)\n",
      "(u'reciprocating_engine', 1)\n",
      "(u'recipe', 1)\n",
      "(u'recess', 1)\n",
      "(u'receiver', 1)\n",
      "(u'realty', 1)\n",
      "(u'raw_material', 1)\n",
      "(u'rational_motive', 1)\n",
      "(u'rapidity', 1)\n",
      "(u'rap_music', 1)\n",
      "(u'railway_track', 1)\n",
      "(u'railway_car', 1)\n",
      "(u'railway', 1)\n",
      "(u'rails', 1)\n",
      "(u'railroad_train', 1)\n",
      "(u'railroad_car', 1)\n",
      "(u'railroad', 1)\n",
      "(u'railcar', 1)\n",
      "(u'rail_transportation', 1)\n",
      "(u'rail_transport', 1)\n",
      "(u'rail_track', 1)\n",
      "(u'rail_line', 1)\n",
      "(u'rail', 1)\n",
      "(u'radioactivity', 1)\n",
      "(u'radio_station', 1)\n",
      "(u'radio_show', 1)\n",
      "(u'radio_programme', 1)\n",
      "(u'radio_program', 1)\n",
      "(u'radio_frequency', 1)\n",
      "(u'radiation', 1)\n",
      "(u'racing_boat', 1)\n",
      "(u'raceway', 1)\n",
      "(u'racetrack', 1)\n",
      "(u'racecourse', 1)\n",
      "(u'race_track', 1)\n",
      "(u'quotation', 1)\n",
      "(u'quitter', 1)\n",
      "(u'quiet', 1)\n",
      "(u'questionnaire', 1)\n",
      "(u'question', 1)\n",
      "(u'quaternary_ammonium_compound', 1)\n",
      "(u'quaternary_ammonium', 1)\n",
      "(u'quark_theory', 1)\n",
      "(u'quark', 1)\n",
      "(u'pustule', 1)\n",
      "(u'pushing', 1)\n",
      "(u'push', 1)\n",
      "(u'purples', 1)\n",
      "(u'purple', 1)\n",
      "(u'pupil', 1)\n",
      "(u'punk_rocker', 1)\n",
      "(u'punk_rock', 1)\n",
      "(u'punk_music', 1)\n",
      "(u'punk', 1)\n",
      "(u'punishment', 1)\n",
      "(u'pundit', 1)\n",
      "(u'punctuation', 1)\n",
      "(u'pull', 1)\n",
      "(u'publicist', 1)\n",
      "(u'public_utility_company', 1)\n",
      "(u'public_utility', 1)\n",
      "(u'public_square', 1)\n",
      "(u'public_speaker', 1)\n",
      "(u'public_service', 1)\n",
      "(u'public_presentation', 1)\n",
      "(u'public-service_corporation', 1)\n",
      "(u'psychostimulant', 1)\n",
      "(u'psychology', 1)\n",
      "(u'psychologist', 1)\n",
      "(u'psychological_science', 1)\n",
      "(u'psychoactive_drug', 1)\n",
      "(u'psychic_energy', 1)\n",
      "(u'provider', 1)\n",
      "(u'protrusion', 1)\n",
      "(u'protozoan_infection', 1)\n",
      "(u'protozoal_infection', 1)\n",
      "(u'protest', 1)\n",
      "(u'protector', 1)\n",
      "(u'protagonist', 1)\n",
      "(u'prostration', 1)\n",
      "(u'propulsion', 1)\n",
      "(u'proponent', 1)\n",
      "(u'proper_subset', 1)\n",
      "(u'proper_class', 1)\n",
      "(u'proof', 1)\n",
      "(u'promulgation', 1)\n",
      "(u'prompt', 1)\n",
      "(u'promotional_material', 1)\n",
      "(u'programming', 1)\n",
      "(u'programmer', 1)\n",
      "(u'programing', 1)\n",
      "(u'progeny', 1)\n",
      "(u'procyonid', 1)\n",
      "(u'proclamation', 1)\n",
      "(u'processor', 1)\n",
      "(u'proceeding', 1)\n",
      "(u'problems', 1)\n",
      "(u'probality', 1)\n",
      "(u'prize', 1)\n",
      "(u'privilege', 1)\n",
      "(u'privately_held_corporation', 1)\n",
      "(u'private_university', 1)\n",
      "(u'private_corporation', 1)\n",
      "(u'prison_cell', 1)\n",
      "(u'prison', 1)\n",
      "(u'printed_circuit', 1)\n",
      "(u'print', 1)\n",
      "(u'primogenitor', 1)\n",
      "(u'primitive_person', 1)\n",
      "(u'primitive_notion', 1)\n",
      "(u'primary_care_provider', 1)\n",
      "(u'prevision', 1)\n",
      "(u'prevaricator', 1)\n",
      "(u'prevarication', 1)\n",
      "(u'pressure_vessel', 1)\n",
      "(u'pressure', 1)\n",
      "(u'pressman', 1)\n",
      "(u'presiding_officer', 1)\n",
      "(u'presentment', 1)\n",
      "(u'presenter', 1)\n",
      "(u'presentation', 1)\n",
      "(u'prescript', 1)\n",
      "(u'prefix', 1)\n",
      "(u'precious_metal', 1)\n",
      "(u'pox', 1)\n",
      "(u'potency', 1)\n",
      "(u'potato', 1)\n",
      "(u'potable', 1)\n",
      "(u'posterity', 1)\n",
      "(u'possibility', 1)\n",
      "(u'portion', 1)\n",
      "(u'port_of_entry', 1)\n",
      "(u'port_city', 1)\n",
      "(u'porch', 1)\n",
      "(u'population_group', 1)\n",
      "(u'population_genetics', 1)\n",
      "(u'pop', 1)\n",
      "(u'poorhouse', 1)\n",
      "(u'poo', 1)\n",
      "(u'polytope', 1)\n",
      "(u'polysaccharide', 1)\n",
      "(u'polyphyletic', 1)\n",
      "(u'polymer', 1)\n",
      "(u'political_doctrine', 1)\n",
      "(u'political_coalition', 1)\n",
      "(u'policy', 1)\n",
      "(u'policier', 1)\n",
      "(u'police_procedural', 1)\n",
      "(u'pole', 1)\n",
      "(u'poker_hand', 1)\n",
      "(u'poker_game', 1)\n",
      "(u'poker', 1)\n",
      "(u'pointer', 1)\n",
      "(u'point_of_intersection', 1)\n",
      "(u'point_in_time', 1)\n",
      "(u'poetic_rhythm', 1)\n",
      "(u'poem', 1)\n",
      "(u'poaceae', 1)\n",
      "(u'plot_of_ground', 1)\n",
      "(u'plaza', 1)\n",
      "(u'plaything', 1)\n",
      "(u'playing_field', 1)\n",
      "(u'playing_card', 1)\n",
      "(u'playfield', 1)\n",
      "(u'plaster', 1)\n",
      "(u'plant_product', 1)\n",
      "(u'plant_pathology', 1)\n",
      "(u'plant_organ', 1)\n",
      "(u'plant_genus', 1)\n",
      "(u'plant_food', 1)\n",
      "(u'plant_disease', 1)\n",
      "(u'plane', 1)\n",
      "(u'plan', 1)\n",
      "(u'plain', 1)\n",
      "(u'pitched_battle', 1)\n",
      "(u'piston_engine', 1)\n",
      "(u'pisces', 1)\n",
      "(u'piping', 1)\n",
      "(u'pipework', 1)\n",
      "(u'pipe', 1)\n",
      "(u'pink', 1)\n",
      "(u'pillow', 1)\n",
      "(u'pile', 1)\n",
      "(u'piece_of_material', 1)\n",
      "(u'piece_of_furniture', 1)\n",
      "(u'picturing', 1)\n",
      "(u'piano_player', 1)\n",
      "(u'pianist', 1)\n",
      "(u'phytopathology', 1)\n",
      "(u'phytopathogen', 1)\n",
      "(u'physiological_reaction', 1)\n",
      "(u'physiological_property', 1)\n",
      "(u'physical_punishment', 1)\n",
      "(u'physical_intimacy', 1)\n",
      "(u'phrase', 1)\n",
      "(u'photographic_printing', 1)\n",
      "(u'photographic_print', 1)\n",
      "(u'photographic_equipment', 1)\n",
      "(u'photographic_camera', 1)\n",
      "(u'phonetician', 1)\n",
      "(u'phone', 1)\n",
      "(u'philosophical_theory', 1)\n",
      "(u'philosophical_system', 1)\n",
      "(u'philosophical_doctrine', 1)\n",
      "(u'pharmaceutical_industry', 1)\n",
      "(u'pharmaceutical_company', 1)\n",
      "(u'pharma', 1)\n",
      "(u'pharaoh_of_egypt', 1)\n",
      "(u'pharaoh', 1)\n",
      "(u'pet', 1)\n",
      "(u'persuasion', 1)\n",
      "(u'persuader', 1)\n",
      "(u'personality', 1)\n",
      "(u'personal_identity', 1)\n",
      "(u'personal_armor', 1)\n",
      "(u'personage', 1)\n",
      "(u'persona_non_grata', 1)\n",
      "(u'person_of_colour', 1)\n",
      "(u'person_of_color', 1)\n",
      "(u'persecutor', 1)\n",
      "(u'perquisite', 1)\n",
      "(u'perissodactyl', 1)\n",
      "(u'peripheral_equipment', 1)\n",
      "(u'peripheral_device', 1)\n",
      "(u'periodization', 1)\n",
      "(u'percussionist', 1)\n",
      "(u'percussion', 1)\n",
      "(u'perceptible', 1)\n",
      "(u'percept', 1)\n",
      "(u'perceiver', 1)\n",
      "(u'penchant', 1)\n",
      "(u'penalization', 1)\n",
      "(u'penalisation', 1)\n",
      "(u'penal_facility', 1)\n",
      "(u'pellet', 1)\n",
      "(u'pedigree', 1)\n",
      "(u'pedal', 1)\n",
      "(u'pear_tree', 1)\n",
      "(u'pear', 1)\n",
      "(u'peak', 1)\n",
      "(u'paying_back', 1)\n",
      "(u'paygrade', 1)\n",
      "(u'paving_material', 1)\n",
      "(u'paving', 1)\n",
      "(u'pause', 1)\n",
      "(u'patriotism', 1)\n",
      "(u'paternity', 1)\n",
      "(u'pastry_dough', 1)\n",
      "(u'pastry', 1)\n",
      "(u'passionflower_vine', 1)\n",
      "(u'passionflower', 1)\n",
      "(u'passion_fruit', 1)\n",
      "(u'passion_flower', 1)\n",
      "(u'passiflora', 1)\n",
      "(u'pass', 1)\n",
      "(u'party_game', 1)\n",
      "(u'particulate_matter', 1)\n",
      "(u'particle_detector', 1)\n",
      "(u'parlour_game', 1)\n",
      "(u'parlor_game', 1)\n",
      "(u'parliamentary_democracy', 1)\n",
      "(u'parents', 1)\n",
      "(u'parent', 1)\n",
      "(u'paramilitary_unit', 1)\n",
      "(u'paramilitary_organization', 1)\n",
      "(u'paramilitary_organisation', 1)\n",
      "(u'paramilitary_force', 1)\n",
      "(u'paramilitary', 1)\n",
      "(u'paperback_book', 1)\n",
      "(u'paper_money', 1)\n",
      "(u'paper_currency', 1)\n",
      "(u'pants', 1)\n",
      "(u'panties', 1)\n",
      "(u'pantie', 1)\n",
      "(u'pant', 1)\n",
      "(u'page', 1)\n",
      "(u'padding', 1)\n",
      "(u'packing', 1)\n",
      "(u'packaging', 1)\n",
      "(u'pace', 1)\n",
      "(u'oxidoreduction', 1)\n",
      "(u'oxidization', 1)\n",
      "(u'oxidisation', 1)\n",
      "(u'oxide_mineral', 1)\n",
      "(u'oxide', 1)\n",
      "(u'oxidation', 1)\n",
      "(u'oxen', 1)\n",
      "(u'ox', 1)\n",
      "(u'oviduct', 1)\n",
      "(u'overshoe', 1)\n",
      "(u'overseas_territory', 1)\n",
      "(u'overlord', 1)\n",
      "(u'overgarment', 1)\n",
      "(u'oven', 1)\n",
      "(u'outpouring', 1)\n",
      "(u'outlaw', 1)\n",
      "(u'outgrowth', 1)\n",
      "(u'outgo', 1)\n",
      "(u'outerwear', 1)\n",
      "(u'outer_garment', 1)\n",
      "(u'outburst', 1)\n",
      "(u'oscillation', 1)\n",
      "(u'os', 1)\n",
      "(u'orthopedist', 1)\n",
      "(u'orthopedics', 1)\n",
      "(u'orthopedic_surgery', 1)\n",
      "(u'orthopaedist', 1)\n",
      "(u'orthopaedics', 1)\n",
      "(u'organs', 1)\n",
      "(u'organizer', 1)\n",
      "(u'organic_solvent', 1)\n",
      "(u'organic_phenomenon', 1)\n",
      "(u'organic_fertilizer', 1)\n",
      "(u'organic', 1)\n",
      "(u'organ_transplantation', 1)\n",
      "(u'organ_transplant', 1)\n",
      "(u'ordnance', 1)\n",
      "(u'ordination', 1)\n",
      "(u'ordering', 1)\n",
      "(u'ordered_series', 1)\n",
      "(u'order_of_succession', 1)\n",
      "(u'order_of_payment', 1)\n",
      "(u'orbital_period', 1)\n",
      "(u'orator', 1)\n",
      "(u'orange_yellow', 1)\n",
      "(u'option', 1)\n",
      "(u'optical_instrument', 1)\n",
      "(u'opposition', 1)\n",
      "(u'oppositeness', 1)\n",
      "(u'onslaught', 1)\n",
      "(u'online_database', 1)\n",
      "(u'on-line_database', 1)\n",
      "(u'olympiad', 1)\n",
      "(u'oilseed', 1)\n",
      "(u'offspring', 1)\n",
      "(u'official_residence', 1)\n",
      "(u'office_suite', 1)\n",
      "(u'office_furniture', 1)\n",
      "(u'office_building', 1)\n",
      "(u'offender', 1)\n",
      "(u'ocean', 1)\n",
      "(u'occupier', 1)\n",
      "(u'occupant', 1)\n",
      "(u'obstruent', 1)\n",
      "(u'obstacle', 1)\n",
      "(u'observer', 1)\n",
      "(u'observation', 1)\n",
      "(u'observance', 1)\n",
      "(u'obliteration', 1)\n",
      "(u'obligation', 1)\n",
      "(u'object-orientation', 1)\n",
      "(u'nymph', 1)\n",
      "(u'nut', 1)\n",
      "(u'numerical_quantity', 1)\n",
      "(u'nucleus', 1)\n",
      "(u'nozzle', 1)\n",
      "(u'novel_adaptation', 1)\n",
      "(u'note_of_hand', 1)\n",
      "(u'not-for-profit', 1)\n",
      "(u'north_american_indian', 1)\n",
      "(u'nonverbal_communication', 1)\n",
      "(u'nonvascular_plant', 1)\n",
      "(u'nonsmoker', 1)\n",
      "(u'nonsense', 1)\n",
      "(u'nonprofit_organization', 1)\n",
      "(u'nonprofit', 1)\n",
      "(u'nonmetal', 1)\n",
      "(u'nongovernmental_organization', 1)\n",
      "(u'nonfiction', 1)\n",
      "(u'non-smoker', 1)\n",
      "(u'non-metropolitan_county', 1)\n",
      "(u'non-metal', 1)\n",
      "(u'non-governmental_organization', 1)\n",
      "(u'non-governmental_organisation', 1)\n",
      "(u'non-fiction', 1)\n",
      "(u'noise', 1)\n",
      "(u'noblewoman', 1)\n",
      "(u'nobleman', 1)\n",
      "(u'noble_metal', 1)\n",
      "(u'nitrogen', 1)\n",
      "(u'nipper', 1)\n",
      "(u'nightwear', 1)\n",
      "(u'nightclothes', 1)\n",
      "(u'ngo', 1)\n",
      "(u'newswriter', 1)\n",
      "(u'newspaperman', 1)\n",
      "(u'newspaper_article', 1)\n",
      "(u'news_media', 1)\n",
      "(u'neural_structure', 1)\n",
      "(u'net_worth', 1)\n",
      "(u'nervousness', 1)\n",
      "(u'nervous_strain', 1)\n",
      "(u'nerves', 1)\n",
      "(u'negroid', 1)\n",
      "(u'negro', 1)\n",
      "(u'negotiator', 1)\n",
      "(u'needleworker', 1)\n",
      "(u'needlework', 1)\n",
      "(u'navigation', 1)\n",
      "(u'naval_ship', 1)\n",
      "(u'natural_virtue', 1)\n",
      "(u'natural_resources', 1)\n",
      "(u'natural_resource', 1)\n",
      "(u'natural_resin', 1)\n",
      "(u'natural_person', 1)\n",
      "(u'natural_history', 1)\n",
      "(u'natural_environment', 1)\n",
      "(u'natural_arch', 1)\n",
      "(u'native_species', 1)\n",
      "(u'native_americans', 1)\n",
      "(u'native_american', 1)\n",
      "(u'nationalism', 1)\n",
      "(u'national_sports_team', 1)\n",
      "(u'national', 1)\n",
      "(u'narcotic', 1)\n",
      "(u'naming', 1)\n",
      "(u'mythology', 1)\n",
      "(u'mystery_story', 1)\n",
      "(u'mystery_novel', 1)\n",
      "(u'mustard_plant', 1)\n",
      "(u'muslin', 1)\n",
      "(u'musical_notation', 1)\n",
      "(u'musical_film', 1)\n",
      "(u'music_notation', 1)\n",
      "(u'museum', 1)\n",
      "(u'musculus', 1)\n",
      "(u'muscular_contraction', 1)\n",
      "(u'muscle_tissue', 1)\n",
      "(u'muscle_spasm', 1)\n",
      "(u'muscle_contraction', 1)\n",
      "(u'murderer', 1)\n",
      "(u'murder_mystery', 1)\n",
      "(u'murder', 1)\n",
      "(u'munition', 1)\n",
      "(u'multiplication', 1)\n",
      "(u'multiple', 1)\n",
      "(u'multimedia', 1)\n",
      "(u'multi-cylinder_engine', 1)\n",
      "(u'moustache', 1)\n",
      "(u'mountain_pass', 1)\n",
      "(u'motortruck', 1)\n",
      "(u'motorist', 1)\n",
      "(u'motorhome', 1)\n",
      "(u'motor_vehicle', 1)\n",
      "(u'motor_coach', 1)\n",
      "(u'motor', 1)\n",
      "(u'motif', 1)\n",
      "(u'moss', 1)\n",
      "(u'mosque', 1)\n",
      "(u'morphology', 1)\n",
      "(u'month', 1)\n",
      "(u'money_circulation', 1)\n",
      "(u'monarchy', 1)\n",
      "(u'momentum', 1)\n",
      "(u'molecular_biologist', 1)\n",
      "(u'moiety', 1)\n",
      "(u'mobile_device', 1)\n",
      "(u'mixed_drink', 1)\n",
      "(u'mistreatment', 1)\n",
      "(u'missy', 1)\n",
      "(u'miss', 1)\n",
      "(u'misdemeanour', 1)\n",
      "(u'misdemeanor', 1)\n",
      "(u'misdeed', 1)\n",
      "(u'misbehaviour', 1)\n",
      "(u'misbehavior', 1)\n",
      "(u'mintage', 1)\n",
      "(u'mill', 1)\n",
      "(u'military_post', 1)\n",
      "(u'military_medicine', 1)\n",
      "(u'military_leader', 1)\n",
      "(u'military_installation', 1)\n",
      "(u'military_hospital', 1)\n",
      "(u'military_expedition', 1)\n",
      "(u'military_campaign', 1)\n",
      "(u'military_base', 1)\n",
      "(u'migrator', 1)\n",
      "(u'middleware', 1)\n",
      "(u'middle_school', 1)\n",
      "(u'microscopy', 1)\n",
      "(u'microscope', 1)\n",
      "(u'microprocessor_chip', 1)\n",
      "(u'microorganism', 1)\n",
      "(u'microcomputer', 1)\n",
      "(u'microchip', 1)\n",
      "(u'micro_chip', 1)\n",
      "(u'metropolitan_county', 1)\n",
      "(u'metric_space', 1)\n",
      "(u'metal_oxide', 1)\n",
      "(u'meshwork', 1)\n",
      "(u'mesh', 1)\n",
      "(u'merchandiser', 1)\n",
      "(u'mercantile_establishment', 1)\n",
      "(u'mental_strain', 1)\n",
      "(u'mental_energy', 1)\n",
      "(u'mensuration', 1)\n",
      "(u'mending', 1)\n",
      "(u'mend', 1)\n",
      "(u'memory_device', 1)\n",
      "(u'membrane', 1)\n",
      "(u'member', 1)\n",
      "(u'melody', 1)\n",
      "(u'medico', 1)\n",
      "(u'medical_practitioner', 1)\n",
      "(u'medical_man', 1)\n",
      "(u'medical_facility', 1)\n",
      "(u'medical_building', 1)\n",
      "(u'medal', 1)\n",
      "(u'mechanisation', 1)\n",
      "(u'mechanics', 1)\n",
      "(u'mechanical_system', 1)\n",
      "(u'mechanical_stress', 1)\n",
      "(u'mechanical_phenomenon', 1)\n",
      "(u'meat', 1)\n",
      "(u'measuring_stick', 1)\n",
      "(u'measuring_rod', 1)\n",
      "(u'meaninglessness', 1)\n",
      "(u'mean', 1)\n",
      "(u'meal', 1)\n",
      "(u'meadow', 1)\n",
      "(u'md', 1)\n",
      "(u'mathematical_structure', 1)\n",
      "(u'mathematical_space', 1)\n",
      "(u'mathematical_product', 1)\n",
      "(u'mathematical_notation', 1)\n",
      "(u'material_resource', 1)\n",
      "(u'mate', 1)\n",
      "(u'masquerade_party', 1)\n",
      "(u'masquerade', 1)\n",
      "(u'masque', 1)\n",
      "(u'mask', 1)\n",
      "(u'mart', 1)\n",
      "(u'married_woman', 1)\n",
      "(u'married_person', 1)\n",
      "(u'marque', 1)\n",
      "(u'markup_language', 1)\n",
      "(u'market_town', 1)\n",
      "(u'marital_status', 1)\n",
      "(u'mariner', 1)\n",
      "(u'marine_biology', 1)\n",
      "(u'manufacturing_plant', 1)\n",
      "(u'manufacturer', 1)\n",
      "(u'manufactory', 1)\n",
      "(u'manual_laborer', 1)\n",
      "(u'mansion_house', 1)\n",
      "(u'mansion', 1)\n",
      "(u'manservant', 1)\n",
      "(u'manse', 1)\n",
      "(u'manpower', 1)\n",
      "(u'manoeuvre', 1)\n",
      "(u'manner_of_speaking', 1)\n",
      "(u'manager', 1)\n",
      "(u'management', 1)\n",
      "(u'man_of_affairs', 1)\n",
      "(u'man-made_lake', 1)\n",
      "(u'maltreatment', 1)\n",
      "(u'mall', 1)\n",
      "(u'male_person', 1)\n",
      "(u'male_monarch', 1)\n",
      "(u'main_group_element', 1)\n",
      "(u'main_group', 1)\n",
      "(u'mail_user_agent', 1)\n",
      "(u'magnifier', 1)\n",
      "(u'magnetic_tape', 1)\n",
      "(u'magistrate', 1)\n",
      "(u'magic', 1)\n",
      "(u'maestro', 1)\n",
      "(u'macrocosm_and_microcosm', 1)\n",
      "(u'luck', 1)\n",
      "(u'loyalty', 1)\n",
      "(u'lover', 1)\n",
      "(u'loss', 1)\n",
      "(u'lorry', 1)\n",
      "(u'loop', 1)\n",
      "(u'looking_at', 1)\n",
      "(u'looking', 1)\n",
      "(u'look', 1)\n",
      "(u'long_time', 1)\n",
      "(u'logical_thinking', 1)\n",
      "(u'logical_system', 1)\n",
      "(u'logical_calculus', 1)\n",
      "(u'logical_argument', 1)\n",
      "(u'lodging', 1)\n",
      "(u'lodge', 1)\n",
      "(u'lockup', 1)\n",
      "(u'local_government_area', 1)\n",
      "(u'local_department', 1)\n",
      "(u'literate', 1)\n",
      "(u'literary_magazine', 1)\n",
      "(u'literary_adaptation', 1)\n",
      "(u'listing', 1)\n",
      "(u'lipoid', 1)\n",
      "(u'lipid', 1)\n",
      "(u'links_course', 1)\n",
      "(u'link', 1)\n",
      "(u'linguistic_unit', 1)\n",
      "(u'lineman', 1)\n",
      "(u'line_segment', 1)\n",
      "(u'line_of_descent', 1)\n",
      "(u'limitation', 1)\n",
      "(u'likeness', 1)\n",
      "(u'lighter-than-air_craft', 1)\n",
      "(u'light_source', 1)\n",
      "(u'light_fixture', 1)\n",
      "(u'light', 1)\n",
      "(u'lift_bridge', 1)\n",
      "(u'life_story', 1)\n",
      "(u'lie', 1)\n",
      "(u'library', 1)\n",
      "(u'libertine', 1)\n",
      "(u'liberation', 1)\n",
      "(u'liar', 1)\n",
      "(u'lexical_item', 1)\n",
      "(u'lexical_category', 1)\n",
      "(u'level', 1)\n",
      "(u'letter', 1)\n",
      "(u'letdown', 1)\n",
      "(u'lesion', 1)\n",
      "(u'lesbianism', 1)\n",
      "(u'lensman', 1)\n",
      "(u'leguminous_plant', 1)\n",
      "(u'legume', 1)\n",
      "(u'legislative_act', 1)\n",
      "(u'legal_opinion', 1)\n",
      "(u'legal_case', 1)\n",
      "(u'legal_action', 1)\n",
      "(u'leaving', 1)\n",
      "(u'leafy_vegetable', 1)\n",
      "(u'leafage', 1)\n",
      "(u'lawyer', 1)\n",
      "(u'lawsuit', 1)\n",
      "(u'last', 1)\n",
      "(u'lake', 1)\n",
      "(u'lady', 1)\n",
      "(u'labour_economics', 1)\n",
      "(u'laboratory_equipment', 1)\n",
      "(u'laboratory', 1)\n",
      "(u'lab', 1)\n",
      "(u'know-how', 1)\n",
      "(u'knight', 1)\n",
      "(u'kitchen', 1)\n",
      "(u'kindred', 1)\n",
      "(u'kin_group', 1)\n",
      "(u'kilogram_calorie', 1)\n",
      "(u'kid', 1)\n",
      "(u'keyboardist', 1)\n",
      "(u'keyboard_instrument', 1)\n",
      "(u'ketone', 1)\n",
      "(u'juvenile', 1)\n",
      "(u'justness', 1)\n",
      "(u'juridical_personality', 1)\n",
      "(u'junk', 1)\n",
      "(u'junior_high_school', 1)\n",
      "(u'junior_high', 1)\n",
      "(u'junction', 1)\n",
      "(u'judiciary', 1)\n",
      "(u'judicial_system', 1)\n",
      "(u'judicial_opinion', 1)\n",
      "(u'judicial_decision', 1)\n",
      "(u'judicial', 1)\n",
      "(u'judicature', 1)\n",
      "(u'judgment', 1)\n",
      "(u'judgement', 1)\n",
      "(u'jointure', 1)\n",
      "(u'jewel', 1)\n",
      "(u'jet_plane', 1)\n",
      "(u'jet_engine', 1)\n",
      "(u'jet_aircraft', 1)\n",
      "(u'jet', 1)\n",
      "(u'jail_cell', 1)\n",
      "(u'jade', 1)\n",
      "(u'jacket', 1)\n",
      "(u'itinerary', 1)\n",
      "(u'isopod', 1)\n",
      "(u'irrelevance', 1)\n",
      "(u'irish_potato', 1)\n",
      "(u'ip', 1)\n",
      "(u'ionizing_radiation', 1)\n",
      "(u'ionising_radiation', 1)\n",
      "(u'ion_channel', 1)\n",
      "(u'ion', 1)\n",
      "(u'investor', 1)\n",
      "(u'invariable', 1)\n",
      "(u'intuition', 1)\n",
      "(u'intruder', 1)\n",
      "(u'introspection', 1)\n",
      "(u'international_organization', 1)\n",
      "(u'international_organisation', 1)\n",
      "(u'international_airport', 1)\n",
      "(u'internal_secretion', 1)\n",
      "(u'internal_organ', 1)\n",
      "(u'internal-combustion_engine', 1)\n",
      "(u'intermediate_school', 1)\n",
      "(u'interloper', 1)\n",
      "(u'interference', 1)\n",
      "(u'interface', 1)\n",
      "(u'intercourse', 1)\n",
      "(u'interactive_media', 1)\n",
      "(u'interaction', 1)\n",
      "(u'intelligence_operation', 1)\n",
      "(u'intelligence_information', 1)\n",
      "(u'intelligence_activity', 1)\n",
      "(u'integral_membrane_protein', 1)\n",
      "(u'integers', 1)\n",
      "(u'insight', 1)\n",
      "(u'inorganic_compound', 1)\n",
      "(u'initiate', 1)\n",
      "(u'information_systems', 1)\n",
      "(u'information_system', 1)\n",
      "(u'information_gathering', 1)\n",
      "(u'influential_person', 1)\n",
      "(u'inferior_planet', 1)\n",
      "(u'infectious_disease', 1)\n",
      "(u'inebriant', 1)\n",
      "(u'industrial_property', 1)\n",
      "(u'industrial_plant', 1)\n",
      "(u'indigenous_species', 1)\n",
      "(u'indigenous_peoples', 1)\n",
      "(u'indicant', 1)\n",
      "(u'indian', 1)\n",
      "(u'independent_city', 1)\n",
      "(u'increment', 1)\n",
      "(u'incomprehensibility', 1)\n",
      "(u'incline', 1)\n",
      "(u'incitation', 1)\n",
      "(u'incarnadine', 1)\n",
      "(u'inactivity', 1)\n",
      "(u'inaction', 1)\n",
      "(u'inability', 1)\n",
      "(u'improperness', 1)\n",
      "(u'important_person', 1)\n",
      "(u'implements_of_war', 1)\n",
      "(u'implement', 1)\n",
      "(u'impinging', 1)\n",
      "(u'imperfectness', 1)\n",
      "(u'imperfection', 1)\n",
      "(u'impairment', 1)\n",
      "(u'impact', 1)\n",
      "(u'immovable', 1)\n",
      "(u'immorality', 1)\n",
      "(u'imaginary_number', 1)\n",
      "(u'imaginary', 1)\n",
      "(u'illustrator', 1)\n",
      "(u'illusionist', 1)\n",
      "(u'ill-usage', 1)\n",
      "(u'identity', 1)\n",
      "(u'ice_mass', 1)\n",
      "(u'ice_hockey_player', 1)\n",
      "(u'ice', 1)\n",
      "(u'husk', 1)\n",
      "(u'husbandry', 1)\n",
      "(u'husband', 1)\n",
      "(u'hurrying', 1)\n",
      "(u'hurler', 1)\n",
      "(u'humour', 1)\n",
      "(u'human_skin_color', 1)\n",
      "(u'human_resources', 1)\n",
      "(u'hull', 1)\n",
      "(u'house_servant', 1)\n",
      "(u'house_of_worship', 1)\n",
      "(u'house_of_representatives', 1)\n",
      "(u'house_of_god', 1)\n",
      "(u'hotel_industry', 1)\n",
      "(u'hotel', 1)\n",
      "(u'hot_cereal', 1)\n",
      "(u'hot_air_balloon', 1)\n",
      "(u'hospital', 1)\n",
      "(u'horseriding', 1)\n",
      "(u'horsemanship', 1)\n",
      "(u'horseback_riding', 1)\n",
      "(u'horse_opera', 1)\n",
      "(u'horizontal_surface', 1)\n",
      "(u'hopper', 1)\n",
      "(u'homosexual', 1)\n",
      "(u'homophile', 1)\n",
      "(u'homonym', 1)\n",
      "(u'holy_order', 1)\n",
      "(u'holding_cell', 1)\n",
      "(u'holdfast', 1)\n",
      "(u'hold', 1)\n",
      "(u'hokum', 1)\n",
      "(u'hog', 1)\n",
      "(u'hoarded_wealth', 1)\n",
      "(u'histories', 1)\n",
      "(u'historical_period', 1)\n",
      "(u'historic_period', 1)\n",
      "(u'historic_house', 1)\n",
      "(u'hired_man', 1)\n",
      "(u'hip_hop', 1)\n",
      "(u'higher_education', 1)\n",
      "(u'high_technology', 1)\n",
      "(u'high_tech', 1)\n",
      "(u'high_status', 1)\n",
      "(u'high_island', 1)\n",
      "(u'high_energy_physics', 1)\n",
      "(u'high-rise', 1)\n",
      "(u'high-energy_physics', 1)\n",
      "(u'hero', 1)\n",
      "(u'heredity', 1)\n",
      "(u'herder', 1)\n",
      "(u'herbicide', 1)\n",
      "(u'heraldry', 1)\n",
      "(u'helping', 1)\n",
      "(u'help', 1)\n",
      "(u'heavy_weapon', 1)\n",
      "(u'heavy_metal_music', 1)\n",
      "(u'heavy_metal', 1)\n",
      "(u'heavier-than-air_craft', 1)\n",
      "(u'heart', 1)\n",
      "(u'hearsay', 1)\n",
      "(u'heap', 1)\n",
      "(u'healthiness', 1)\n",
      "(u'healthcare_facility', 1)\n",
      "(u'health_science', 1)\n",
      "(u'health_facility', 1)\n",
      "(u'healing_knowledge', 1)\n",
      "(u'hat', 1)\n",
      "(u'handwork', 1)\n",
      "(u'handling', 1)\n",
      "(u'handler', 1)\n",
      "(u'handicraft', 1)\n",
      "(u'handicap', 1)\n",
      "(u'handcraft', 1)\n",
      "(u'hand-held_computer', 1)\n",
      "(u'hammer', 1)\n",
      "(u'halon', 1)\n",
      "(u'hall', 1)\n",
      "(u'hairstyle', 1)\n",
      "(u'hairdo', 1)\n",
      "(u'hair_styling', 1)\n",
      "(u'hair_style', 1)\n",
      "(u'habitation', 1)\n",
      "(u'guts', 1)\n",
      "(u'gun_barrel', 1)\n",
      "(u'gum_tree', 1)\n",
      "(u'gum', 1)\n",
      "(u'guild', 1)\n",
      "(u'guidance', 1)\n",
      "(u'guardian', 1)\n",
      "(u'grump', 1)\n",
      "(u'growth_medium', 1)\n",
      "(u'growth', 1)\n",
      "(u'group_discussion', 1)\n",
      "(u'grounds', 1)\n",
      "(u'groundcover', 1)\n",
      "(u'ground_cover', 1)\n",
      "(u'grouch', 1)\n",
      "(u'grocery', 1)\n",
      "(u'grip', 1)\n",
      "(u'grey', 1)\n",
      "(u'green_vegetable', 1)\n",
      "(u'green', 1)\n",
      "(u'greek_deity', 1)\n",
      "(u'greco-roman_mythology', 1)\n",
      "(u'grassland', 1)\n",
      "(u'grasshopper', 1)\n",
      "(u'grass_family', 1)\n",
      "(u'grass', 1)\n",
      "(u'grasp', 1)\n",
      "(u'graphics', 1)\n",
      "(u'graphical_interface', 1)\n",
      "(u'graphic_arts', 1)\n",
      "(u'grapheme', 1)\n",
      "(u'grammatical_construction', 1)\n",
      "(u'gramineae', 1)\n",
      "(u'grab', 1)\n",
      "(u'government_note', 1)\n",
      "(u'government_minister', 1)\n",
      "(u'government_department', 1)\n",
      "(u'government_building', 1)\n",
      "(u'gossip', 1)\n",
      "(u'goosefoot', 1)\n",
      "(u'goodness', 1)\n",
      "(u'good_person', 1)\n",
      "(u'golf_equipment', 1)\n",
      "(u'golf_course', 1)\n",
      "(u'going_away', 1)\n",
      "(u'going', 1)\n",
      "(u'goaltender', 1)\n",
      "(u'glycosyltransferase', 1)\n",
      "(u'glycoprotein', 1)\n",
      "(u'glucosyltransferase', 1)\n",
      "(u'globe', 1)\n",
      "(u'global_organization', 1)\n",
      "(u'glacier', 1)\n",
      "(u'gizmo', 1)\n",
      "(u'girl_group', 1)\n",
      "(u'girl', 1)\n",
      "(u'gi_hormones', 1)\n",
      "(u'getting_even', 1)\n",
      "(u'geophysics', 1)\n",
      "(u'geomorphology', 1)\n",
      "(u'geometry', 1)\n",
      "(u'geology', 1)\n",
      "(u'geological_age', 1)\n",
      "(u'genus_passiflora', 1)\n",
      "(u'genre_of_painting', 1)\n",
      "(u'genitor', 1)\n",
      "(u'genetic_information', 1)\n",
      "(u'generation', 1)\n",
      "(u'general', 1)\n",
      "(u'gender_identity', 1)\n",
      "(u'gemstone', 1)\n",
      "(u'gear_mechanism', 1)\n",
      "(u'gay', 1)\n",
      "(u'gather', 1)\n",
      "(u'gate', 1)\n",
      "(u'gastrointestinal_hormone', 1)\n",
      "(u'gas_turbine', 1)\n",
      "(u'garland', 1)\n",
      "(u'garden_tool', 1)\n",
      "(u'garden', 1)\n",
      "(u'garbage', 1)\n",
      "(u'gaol', 1)\n",
      "(u'gaming_house', 1)\n",
      "(u'gambling_house', 1)\n",
      "(u'gambling_den', 1)\n",
      "(u'gait', 1)\n",
      "(u'gain', 1)\n",
      "(u'gadget', 1)\n",
      "(u'furniture', 1)\n",
      "(u'funnies', 1)\n",
      "(u'fungicide', 1)\n",
      "(u'fundamentals', 1)\n",
      "(u'fundamental_particle', 1)\n",
      "(u'fullness', 1)\n",
      "(u'full_beard', 1)\n",
      "(u'fuel', 1)\n",
      "(u'fry', 1)\n",
      "(u'frolic', 1)\n",
      "(u'freight_train', 1)\n",
      "(u'freewoman', 1)\n",
      "(u'freeman', 1)\n",
      "(u'free_reed', 1)\n",
      "(u'franchising', 1)\n",
      "(u'foundation', 1)\n",
      "(u'fortune', 1)\n",
      "(u'fortuna', 1)\n",
      "(u'fortitude', 1)\n",
      "(u'fortification', 1)\n",
      "(u'fort', 1)\n",
      "(u'forrest', 1)\n",
      "(u'fornicator', 1)\n",
      "(u'formulation', 1)\n",
      "(u'formula', 1)\n",
      "(u'form_of_rule', 1)\n",
      "(u'form_class', 1)\n",
      "(u'forgoing', 1)\n",
      "(u'foretelling', 1)\n",
      "(u'forest', 1)\n",
      "(u'foresight', 1)\n",
      "(u'foreland', 1)\n",
      "(u'foreign_policy', 1)\n",
      "(u'forecasting', 1)\n",
      "(u'forecast', 1)\n",
      "(u'forebear', 1)\n",
      "(u'forbear', 1)\n",
      "(u'footrace', 1)\n",
      "(u'football_team', 1)\n",
      "(u'football_official', 1)\n",
      "(u'football_club', 1)\n",
      "(u'foot_race', 1)\n",
      "(u'foot_pedal', 1)\n",
      "(u'foot_lever', 1)\n",
      "(u'foodservice', 1)\n",
      "(u'food_marketing', 1)\n",
      "(u'font', 1)\n",
      "(u'folk_music', 1)\n",
      "(u'folio', 1)\n",
      "(u'folding_money', 1)\n",
      "(u'fold', 1)\n",
      "(u'flying_field', 1)\n",
      "(u'flyer', 1)\n",
      "(u'flycatcher', 1)\n",
      "(u'flutter', 1)\n",
      "(u'flower_arrangement', 1)\n",
      "(u'floral_arrangement', 1)\n",
      "(u'flooring', 1)\n",
      "(u'floating_point', 1)\n",
      "(u'flavouring', 1)\n",
      "(u'flavoring', 1)\n",
      "(u'flat_solid', 1)\n",
      "(u'flat', 1)\n",
      "(u'fixed_costs', 1)\n",
      "(u'fixed_cost', 1)\n",
      "(u'fixed_charge', 1)\n",
      "(u'fixed-wing_aircraft', 1)\n",
      "(u'fix', 1)\n",
      "(u'fitting', 1)\n",
      "(u'fissure', 1)\n",
      "(u'fish_anatomy', 1)\n",
      "(u'first_derivative', 1)\n",
      "(u'first-person_shooter', 1)\n",
      "(u'first-degree_murder', 1)\n",
      "(u'firearm', 1)\n",
      "(u'fire_arm', 1)\n",
      "(u'finishing_line', 1)\n",
      "(u'finish_line', 1)\n",
      "(u'find', 1)\n",
      "(u'financing', 1)\n",
      "(u'financial_plan', 1)\n",
      "(u'financial_institution', 1)\n",
      "(u'finance', 1)\n",
      "(u'film_writer', 1)\n",
      "(u'fille', 1)\n",
      "(u'filing_cabinet', 1)\n",
      "(u'file_cabinet', 1)\n",
      "(u'figuring', 1)\n",
      "(u'figure_painting', 1)\n",
      "(u'fighting', 1)\n",
      "(u'fig_tree', 1)\n",
      "(u'fielder', 1)\n",
      "(u'fictitious_place', 1)\n",
      "(u'fictional_universe', 1)\n",
      "(u'festivity', 1)\n",
      "(u'festival', 1)\n",
      "(u'fertiliser', 1)\n",
      "(u'fermented_drink', 1)\n",
      "(u'female_sibling', 1)\n",
      "(u'felon', 1)\n",
      "(u'fellow_member', 1)\n",
      "(u'felid', 1)\n",
      "(u'feed', 1)\n",
      "(u'fee', 1)\n",
      "(u'federation', 1)\n",
      "(u'federal_republic', 1)\n",
      "(u'federal_government', 1)\n",
      "(u'federal_district', 1)\n",
      "(u'fearfulness', 1)\n",
      "(u'fear', 1)\n",
      "(u'fastening', 1)\n",
      "(u'fastener', 1)\n",
      "(u'fast_food', 1)\n",
      "(u'farm_machinery', 1)\n",
      "(u'farm_machine', 1)\n",
      "(u'fantasy_film', 1)\n",
      "(u'fantasy', 1)\n",
      "(u'family_tree', 1)\n",
      "(u'family_line', 1)\n",
      "(u'falsity', 1)\n",
      "(u'falsehood', 1)\n",
      "(u'faithfulness', 1)\n",
      "(u'factory', 1)\n",
      "(u'facing', 1)\n",
      "(u'facial_hair', 1)\n",
      "(u'extremity', 1)\n",
      "(u'extracurricular_activity', 1)\n",
      "(u'extracurricular', 1)\n",
      "(u'external_respiration', 1)\n",
      "(u'extent', 1)\n",
      "(u'extensor_muscle', 1)\n",
      "(u'extension', 1)\n",
      "(u'expiration', 1)\n",
      "(u'expeditionary_warfare', 1)\n",
      "(u'expedition', 1)\n",
      "(u'exhale', 1)\n",
      "(u'exhalation', 1)\n",
      "(u'exerciser', 1)\n",
      "(u'excretion', 1)\n",
      "(u'excreta', 1)\n",
      "(u'excrement', 1)\n",
      "(u'excoriation', 1)\n",
      "(u'exclusive_right', 1)\n",
      "(u'excitant', 1)\n",
      "(u'exchange', 1)\n",
      "(u'excavation', 1)\n",
      "(u'exactness', 1)\n",
      "(u'evolution', 1)\n",
      "(u'evildoing', 1)\n",
      "(u'evergreen_plant', 1)\n",
      "(u'evergreen', 1)\n",
      "(u'european_country', 1)\n",
      "(u'eucalyptus_tree', 1)\n",
      "(u'eucalyptus', 1)\n",
      "(u'eucalypt', 1)\n",
      "(u'ethnologist', 1)\n",
      "(u'estrogen', 1)\n",
      "(u'estate', 1)\n",
      "(u'essential_amino_acid', 1)\n",
      "(u'equipment_failure', 1)\n",
      "(u'equilateral_triangle', 1)\n",
      "(u'equestrian_statue', 1)\n",
      "(u'equestrian_portrait', 1)\n",
      "(u'equestrian', 1)\n",
      "(u'epoch', 1)\n",
      "(u'environs', 1)\n",
      "(u'environmental_pollution', 1)\n",
      "(u'entry', 1)\n",
      "(u'entrant', 1)\n",
      "(u'entrance', 1)\n",
      "(u'entourage', 1)\n",
      "(u'enrollee', 1)\n",
      "(u'enlargement', 1)\n",
      "(u'engine_configuration', 1)\n",
      "(u'engine-generator', 1)\n",
      "(u'engine', 1)\n",
      "(u'energy_unit', 1)\n",
      "(u'energy_carrier', 1)\n",
      "(u'endnote', 1)\n",
      "(u'endeavor', 1)\n",
      "(u'enclosure', 1)\n",
      "(u'enchantment', 1)\n",
      "(u'enactment', 1)\n",
      "(u'emu', 1)\n",
      "(u'empowerment', 1)\n",
      "(u'employment_market', 1)\n",
      "(u'employer', 1)\n",
      "(u'employ', 1)\n",
      "(u'emperor_of_rome', 1)\n",
      "(u'emperor', 1)\n",
      "(u'emergence', 1)\n",
      "(u'embroidery', 1)\n",
      "(u'embankment', 1)\n",
      "(u'email_reader', 1)\n",
      "(u'email_client', 1)\n",
      "(u'ellipsoid', 1)\n",
      "(u'elite_group', 1)\n",
      "(u'elementary_function', 1)\n",
      "(u'electronic_mail', 1)\n",
      "(u'electronic_database', 1)\n",
      "(u'electronic_communication', 1)\n",
      "(u'electrical_resistance', 1)\n",
      "(u'electrical_element', 1)\n",
      "(u'electrical_conductance', 1)\n",
      "(u'electrical_circuit', 1)\n",
      "(u'elector', 1)\n",
      "(u'elder', 1)\n",
      "(u'ejection', 1)\n",
      "(u'egyptian', 1)\n",
      "(u'effort', 1)\n",
      "(u'editorialist', 1)\n",
      "(u'edible_nut', 1)\n",
      "(u'economy', 1)\n",
      "(u'economic_system', 1)\n",
      "(u'economic_expert', 1)\n",
      "(u'eating_house', 1)\n",
      "(u'e-mail', 1)\n",
      "(u'dynamics', 1)\n",
      "(u'dye', 1)\n",
      "(u'duty_assignment', 1)\n",
      "(u'duty', 1)\n",
      "(u'dust', 1)\n",
      "(u'dupe', 1)\n",
      "(u'ductus', 1)\n",
      "(u'duck', 1)\n",
      "(u'dryness', 1)\n",
      "(u'drying', 1)\n",
      "(u'drummer', 1)\n",
      "(u'drug_company', 1)\n",
      "(u'drop-off', 1)\n",
      "(u'drop', 1)\n",
      "(u'driver', 1)\n",
      "(u'drinking_water', 1)\n",
      "(u'dravidian_languages', 1)\n",
      "(u'dravidian_language', 1)\n",
      "(u'dravidian', 1)\n",
      "(u'dramedy', 1)\n",
      "(u'drainpipe', 1)\n",
      "(u'drain', 1)\n",
      "(u'draft_horse', 1)\n",
      "(u'downfall', 1)\n",
      "(u'double-stranded_dna', 1)\n",
      "(u'donation', 1)\n",
      "(u'dominicans', 1)\n",
      "(u'dominican_order', 1)\n",
      "(u'dominican', 1)\n",
      "(u'domestic_worker', 1)\n",
      "(u'domestic_servant', 1)\n",
      "(u'domestic_help', 1)\n",
      "(u'dog_shit', 1)\n",
      "(u'dog_breed', 1)\n",
      "(u'documentary', 1)\n",
      "(u'doc', 1)\n",
      "(u'dna', 1)\n",
      "(u'divider', 1)\n",
      "(u'disturbance', 1)\n",
      "(u'distastefulness', 1)\n",
      "(u'distance', 1)\n",
      "(u'dissimilarity', 1)\n",
      "(u'dissatisfaction', 1)\n",
      "(u'disruption', 1)\n",
      "(u'disposal', 1)\n",
      "(u'dispenser', 1)\n",
      "(u'disloyalty', 1)\n",
      "(u'dishware', 1)\n",
      "(u'dishes', 1)\n",
      "(u'discussion', 1)\n",
      "(u'discontentment', 1)\n",
      "(u'discontentedness', 1)\n",
      "(u'disbursement', 1)\n",
      "(u'disbursal', 1)\n",
      "(u'disapproval', 1)\n",
      "(u'disaffirmation', 1)\n",
      "(u'dipole', 1)\n",
      "(u'diplomatist', 1)\n",
      "(u'diplomat', 1)\n",
      "(u'diocese', 1)\n",
      "(u'dimensional_analysis', 1)\n",
      "(u'dimension', 1)\n",
      "(u'digital_image', 1)\n",
      "(u'digit', 1)\n",
      "(u'differential_coefficient', 1)\n",
      "(u'dictator', 1)\n",
      "(u'diagram', 1)\n",
      "(u'devoid', 1)\n",
      "(u'developer', 1)\n",
      "(u'detestation', 1)\n",
      "(u'deterrent', 1)\n",
      "(u'detector', 1)\n",
      "(u'detective_novel', 1)\n",
      "(u'destination', 1)\n",
      "(u'designation', 1)\n",
      "(u'description', 1)\n",
      "(u'descent', 1)\n",
      "(u'derivative', 1)\n",
      "(u'dept', 1)\n",
      "(u'depository', 1)\n",
      "(u'depositary', 1)\n",
      "(u'departure', 1)\n",
      "(u'department', 1)\n",
      "(u'demo', 1)\n",
      "(u'demigod', 1)\n",
      "(u'demarcation', 1)\n",
      "(u'delay', 1)\n",
      "(u'deflexion', 1)\n",
      "(u'deflection', 1)\n",
      "(u'defensive_wall', 1)\n",
      "(u'defensive_structure', 1)\n",
      "(u'defense', 1)\n",
      "(u'defender', 1)\n",
      "(u'defence_in_law', 1)\n",
      "(u'defence', 1)\n",
      "(u'defect', 1)\n",
      "(u'defaecation', 1)\n",
      "(u'decrement', 1)\n",
      "(u'decline_in_quality', 1)\n",
      "(u'decline', 1)\n",
      "(u'decision_making', 1)\n",
      "(u'decision', 1)\n",
      "(u'deceiver', 1)\n",
      "(u'decapod_crustacean', 1)\n",
      "(u'decapod', 1)\n",
      "(u'debauchee', 1)\n",
      "(u'dealer', 1)\n",
      "(u'day', 1)\n",
      "(u'data_system', 1)\n",
      "(u'data_structure', 1)\n",
      "(u'data_point', 1)\n",
      "(u'dance', 1)\n",
      "(u'damage', 1)\n",
      "(u'dainty', 1)\n",
      "(u'daemon', 1)\n",
      "(u'cypher', 1)\n",
      "(u'cycling_team', 1)\n",
      "(u'cutlery', 1)\n",
      "(u'cutaneous_disease', 1)\n",
      "(u'cut_of_meat', 1)\n",
      "(u'cushioning', 1)\n",
      "(u'cushion', 1)\n",
      "(u'curvature', 1)\n",
      "(u'currency_sign', 1)\n",
      "(u'curiosity', 1)\n",
      "(u'cure', 1)\n",
      "(u'curative', 1)\n",
      "(u'cup_of_joe', 1)\n",
      "(u'cumulation', 1)\n",
      "(u'cultural_property', 1)\n",
      "(u'cultivar', 1)\n",
      "(u'culmination', 1)\n",
      "(u'cryptograph', 1)\n",
      "(u'cry', 1)\n",
      "(u'cross_section', 1)\n",
      "(u'cross-reference', 1)\n",
      "(u'cross-index', 1)\n",
      "(u'cross', 1)\n",
      "(u'criticism', 1)\n",
      "(u'crime_thriller', 1)\n",
      "(u'crime_novel', 1)\n",
      "(u'crewmember', 1)\n",
      "(u'crewman', 1)\n",
      "(u'crevice', 1)\n",
      "(u'credit', 1)\n",
      "(u'credentials', 1)\n",
      "(u'credential', 1)\n",
      "(u'crater', 1)\n",
      "(u'crap', 1)\n",
      "(u'crank', 1)\n",
      "(u'craftsman', 1)\n",
      "(u'cows', 1)\n",
      "(u'cow', 1)\n",
      "(u'covering_material', 1)\n",
      "(u'covenant', 1)\n",
      "(u'court_case', 1)\n",
      "(u'courier', 1)\n",
      "(u'courageousness', 1)\n",
      "(u'cour', 1)\n",
      "(u'county-equivalent', 1)\n",
      "(u'country_park', 1)\n",
      "(u'counterpart', 1)\n",
      "(u'cosmos', 1)\n",
      "(u'cortege', 1)\n",
      "(u'correctional_institution', 1)\n",
      "(u'correction', 1)\n",
      "(u'corporate_group', 1)\n",
      "(u'corporal_punishment', 1)\n",
      "(u'cord', 1)\n",
      "(u'copy', 1)\n",
      "(u'conversion_factor', 1)\n",
      "(u'convergence', 1)\n",
      "(u'controversialist', 1)\n",
      "(u'controlled_substance', 1)\n",
      "(u'control_surface', 1)\n",
      "(u'contribution', 1)\n",
      "(u'contractor', 1)\n",
      "(u'contraction', 1)\n",
      "(u'contour', 1)\n",
      "(u'continuous_function', 1)\n",
      "(u'contention', 1)\n",
      "(u'content_word', 1)\n",
      "(u'contemporary_history', 1)\n",
      "(u'contemporary', 1)\n",
      "(u'contemporaries', 1)\n",
      "(u'container', 1)\n",
      "(u'contagious_disease', 1)\n",
      "(u'consumption', 1)\n",
      "(u'consumer', 1)\n",
      "(u'construction_work', 1)\n",
      "(u'construction_aggregate', 1)\n",
      "(u'constraint', 1)\n",
      "(u'constituent_state', 1)\n",
      "(u'constituency', 1)\n",
      "(u'constellation', 1)\n",
      "(u'constant_quantity', 1)\n",
      "(u'constant', 1)\n",
      "(u'consortium', 1)\n",
      "(u'consideration', 1)\n",
      "(u'connector', 1)\n",
      "(u'connective_tissue', 1)\n",
      "(u'connecter', 1)\n",
      "(u'connectedness', 1)\n",
      "(u'conjugation', 1)\n",
      "(u'conformation', 1)\n",
      "(u'confect', 1)\n",
      "(u'conditioner', 1)\n",
      "(u'concord', 1)\n",
      "(u'concoction', 1)\n",
      "(u'concave_shape', 1)\n",
      "(u'comradeship', 1)\n",
      "(u'computer_user', 1)\n",
      "(u'computer_programming', 1)\n",
      "(u'computer_peripheral', 1)\n",
      "(u'computer_database', 1)\n",
      "(u'computer_architecture', 1)\n",
      "(u'component-based_software_engineering', 1)\n",
      "(u'complexness', 1)\n",
      "(u'complexity', 1)\n",
      "(u'complexion', 1)\n",
      "(u'complex_quantity', 1)\n",
      "(u'complex_number', 1)\n",
      "(u'completion', 1)\n",
      "(u'complainant', 1)\n",
      "(u'compartment', 1)\n",
      "(u'communications_protocol', 1)\n",
      "(u'communicable_diseases', 1)\n",
      "(u'communicable_disease', 1)\n",
      "(u'commotion', 1)\n",
      "(u'commoner', 1)\n",
      "(u'common_man', 1)\n",
      "(u'commissioned_military_officer', 1)\n",
      "(u'commercial_building', 1)\n",
      "(u'commercial_agent', 1)\n",
      "(u'commentator', 1)\n",
      "(u'comment', 1)\n",
      "(u'commanding_officer', 1)\n",
      "(u'commander', 1)\n",
      "(u'commandant', 1)\n",
      "(u'command', 1)\n",
      "(u'comedy', 1)\n",
      "(u'comedian', 1)\n",
      "(u'combustion', 1)\n",
      "(u'combat_sport', 1)\n",
      "(u'columnist', 1)\n",
      "(u'coloring_material', 1)\n",
      "(u'color_term', 1)\n",
      "(u'collecting', 1)\n",
      "(u'collapse', 1)\n",
      "(u'coleoptera', 1)\n",
      "(u'coitus', 1)\n",
      "(u'cognomen', 1)\n",
      "(u'coffee_plant', 1)\n",
      "(u'coffee-table_book', 1)\n",
      "(u'coffee', 1)\n",
      "(u'coated_paper', 1)\n",
      "(u'coat', 1)\n",
      "(u'coalition_government', 1)\n",
      "(u'coal_power_plant', 1)\n",
      "(u'clutch', 1)\n",
      "(u'cloud', 1)\n",
      "(u'closing', 1)\n",
      "(u'clink', 1)\n",
      "(u'climbing_plant', 1)\n",
      "(u'climatology', 1)\n",
      "(u'clench', 1)\n",
      "(u'cleft', 1)\n",
      "(u'clearness', 1)\n",
      "(u'clearing', 1)\n",
      "(u'classical_mythology', 1)\n",
      "(u'clarity', 1)\n",
      "(u'clan', 1)\n",
      "(u'claim', 1)\n",
      "(u'civil_parish', 1)\n",
      "(u'civil_engineer', 1)\n",
      "(u'civil_authority', 1)\n",
      "(u'citation', 1)\n",
      "(u'circus', 1)\n",
      "(u'circle', 1)\n",
      "(u'cipher', 1)\n",
      "(u'cigarette', 1)\n",
      "(u'church_fathers', 1)\n",
      "(u'church_father', 1)\n",
      "(u'church', 1)\n",
      "(u'chromosome', 1)\n",
      "(u'choosing', 1)\n",
      "(u'choice', 1)\n",
      "(u'chinese_language', 1)\n",
      "(u'chinese', 1)\n",
      "(u'children', 1)\n",
      "(u'chess_move', 1)\n",
      "(u'cheque', 1)\n",
      "(u'chemist', 1)\n",
      "(u'chemical_reduction', 1)\n",
      "(u'chemical_fertilizer', 1)\n",
      "(u'chemical_fertiliser', 1)\n",
      "(u'check', 1)\n",
      "(u'chastity', 1)\n",
      "(u'chase', 1)\n",
      "(u'charge_per_unit', 1)\n",
      "(u'change_of_location', 1)\n",
      "(u'chance', 1)\n",
      "(u'chambers_of_parliament', 1)\n",
      "(u'chamber', 1)\n",
      "(u'chair', 1)\n",
      "(u'certitude', 1)\n",
      "(u'certification', 1)\n",
      "(u'certainty', 1)\n",
      "(u'cerebration', 1)\n",
      "(u'celestial_navigation', 1)\n",
      "(u'cavity', 1)\n",
      "(u'catholicos', 1)\n",
      "(u'casualty', 1)\n",
      "(u'cassation', 1)\n",
      "(u'cartridge', 1)\n",
      "(u'career', 1)\n",
      "(u'carditis', 1)\n",
      "(u'cardiovascular_system', 1)\n",
      "(u'cardinal_number', 1)\n",
      "(u'cardinal_direction', 1)\n",
      "(u'cardinal', 1)\n",
      "(u'carbonaceous', 1)\n",
      "(u'carbon_group', 1)\n",
      "(u'carbon', 1)\n",
      "(u'captain', 1)\n",
      "(u'capitalist', 1)\n",
      "(u'canvas', 1)\n",
      "(u'canid', 1)\n",
      "(u'candy_store', 1)\n",
      "(u'canal', 1)\n",
      "(u'campus', 1)\n",
      "(u'campervan', 1)\n",
      "(u'camper_van', 1)\n",
      "(u'camera', 1)\n",
      "(u'calmness', 1)\n",
      "(u'call', 1)\n",
      "(u'caldera', 1)\n",
      "(u'calcium_sulphate', 1)\n",
      "(u'calcium_sulfate', 1)\n",
      "(u'cager', 1)\n",
      "(u'cadence', 1)\n",
      "(u'c-suite', 1)\n",
      "(u'byte', 1)\n",
      "(u'buzzword', 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'buzz_word', 1)\n",
      "(u'businesswoman', 1)\n",
      "(u'business_model', 1)\n",
      "(u'business_activity', 1)\n",
      "(u'bus_service', 1)\n",
      "(u'bus_company', 1)\n",
      "(u'burning', 1)\n",
      "(u'bunk', 1)\n",
      "(u'bulge', 1)\n",
      "(u'building_block', 1)\n",
      "(u'brown', 1)\n",
      "(u'broadcasting_station', 1)\n",
      "(u'broadcast_station', 1)\n",
      "(u'broadcast_programming', 1)\n",
      "(u'bridge', 1)\n",
      "(u'breeding', 1)\n",
      "(u'breathing_out', 1)\n",
      "(u'breathing', 1)\n",
      "(u'breathe_out', 1)\n",
      "(u'breath', 1)\n",
      "(u'breaking', 1)\n",
      "(u'breakfast_food', 1)\n",
      "(u'breakfast_cereal', 1)\n",
      "(u'breakdown', 1)\n",
      "(u'breakdancing', 1)\n",
      "(u'breakdance', 1)\n",
      "(u'break_dance', 1)\n",
      "(u'breadth', 1)\n",
      "(u'breadbasket', 1)\n",
      "(u'bravery', 1)\n",
      "(u'braveness', 1)\n",
      "(u'bramble_bush', 1)\n",
      "(u'brace', 1)\n",
      "(u'boy_band', 1)\n",
      "(u'boxer', 1)\n",
      "(u'bovid', 1)\n",
      "(u'bounds', 1)\n",
      "(u'bound', 1)\n",
      "(u'bottom', 1)\n",
      "(u'borrowing', 1)\n",
      "(u'borderline', 1)\n",
      "(u'border_checkpoint', 1)\n",
      "(u'booster', 1)\n",
      "(u'bookstore', 1)\n",
      "(u'book_series', 1)\n",
      "(u'book_of_account', 1)\n",
      "(u'book_design', 1)\n",
      "(u'bondage', 1)\n",
      "(u'bollywood', 1)\n",
      "(u'body_waste', 1)\n",
      "(u'body_substance', 1)\n",
      "(u'body_fluid', 1)\n",
      "(u'body_armour', 1)\n",
      "(u'board_game', 1)\n",
      "(u'bludgeon', 1)\n",
      "(u'blowup', 1)\n",
      "(u'bloodstream', 1)\n",
      "(u'bloodline', 1)\n",
      "(u'blood_line', 1)\n",
      "(u'blood_circulation', 1)\n",
      "(u'blood', 1)\n",
      "(u'blister', 1)\n",
      "(u'bleb', 1)\n",
      "(u'blank_space', 1)\n",
      "(u'bladder', 1)\n",
      "(u'black_person', 1)\n",
      "(u'black_people', 1)\n",
      "(u'black_magic', 1)\n",
      "(u'black_belt', 1)\n",
      "(u'black', 1)\n",
      "(u'biosystem', 1)\n",
      "(u'biomedical_scientist', 1)\n",
      "(u'biomaterial', 1)\n",
      "(u'biologist', 1)\n",
      "(u'biological_sequence', 1)\n",
      "(u'biological_membrane', 1)\n",
      "(u'biography', 1)\n",
      "(u'biographical_film', 1)\n",
      "(u'biofluid', 1)\n",
      "(u'biocide', 1)\n",
      "(u'binomen', 1)\n",
      "(u'binary_operation', 1)\n",
      "(u'binary_arithmetic_operation', 1)\n",
      "(u'billards', 1)\n",
      "(u'bill_of_exchange', 1)\n",
      "(u'bibliography', 1)\n",
      "(u'bibliographic_index', 1)\n",
      "(u'bibliographic_database', 1)\n",
      "(u'bevel_gear', 1)\n",
      "(u'benevolence', 1)\n",
      "(u'benefactor', 1)\n",
      "(u'bend', 1)\n",
      "(u'belonging', 1)\n",
      "(u'bell_shape', 1)\n",
      "(u'bell', 1)\n",
      "(u'beholder', 1)\n",
      "(u'beguiler', 1)\n",
      "(u'beetle', 1)\n",
      "(u'beauty_treatment', 1)\n",
      "(u'beautification', 1)\n",
      "(u'beat', 1)\n",
      "(u'beard', 1)\n",
      "(u'beam', 1)\n",
      "(u'battler', 1)\n",
      "(u'basketball_player', 1)\n",
      "(u'baseball_team', 1)\n",
      "(u'base_metal', 1)\n",
      "(u'barrel', 1)\n",
      "(u'banner', 1)\n",
      "(u'banknote', 1)\n",
      "(u'bank_note', 1)\n",
      "(u'bank_check', 1)\n",
      "(u'bank_bill', 1)\n",
      "(u'banishment', 1)\n",
      "(u'bandleader', 1)\n",
      "(u'baltic_state', 1)\n",
      "(u'baltic_country', 1)\n",
      "(u'balloon', 1)\n",
      "(u'badge_of_honour', 1)\n",
      "(u'badge_of_honor', 1)\n",
      "(u'aversion', 1)\n",
      "(u'averment', 1)\n",
      "(u'average', 1)\n",
      "(u'autonomous_city', 1)\n",
      "(u'automobile', 1)\n",
      "(u'automation', 1)\n",
      "(u'autoimmune_disease', 1)\n",
      "(u'auto', 1)\n",
      "(u'auditory_sensation', 1)\n",
      "(u'auditory_communication', 1)\n",
      "(u'audio_system', 1)\n",
      "(u'audio', 1)\n",
      "(u'attraction', 1)\n",
      "(u'attorney', 1)\n",
      "(u'attire', 1)\n",
      "(u'attentiveness', 1)\n",
      "(u'attention', 1)\n",
      "(u'attempt', 1)\n",
      "(u'attacker', 1)\n",
      "(u'attachment', 1)\n",
      "(u'atmospheric_conditions', 1)\n",
      "(u'athletic_field', 1)\n",
      "(u'athletic_competition', 1)\n",
      "(u'astrophysics', 1)\n",
      "(u'astrophysicist', 1)\n",
      "(u'astronomical_year', 1)\n",
      "(u'astonishment', 1)\n",
      "(u'association_football', 1)\n",
      "(u'assistance', 1)\n",
      "(u'assist', 1)\n",
      "(u'assignation', 1)\n",
      "(u'asseveration', 1)\n",
      "(u'assessment', 1)\n",
      "(u'assertion', 1)\n",
      "(u'assembling', 1)\n",
      "(u'assault', 1)\n",
      "(u'aspen', 1)\n",
      "(u'aspect', 1)\n",
      "(u'asian_nation', 1)\n",
      "(u'ascii_text_file', 1)\n",
      "(u'ascendent', 1)\n",
      "(u'ascendance', 1)\n",
      "(u'artistic_theme', 1)\n",
      "(u'artisan', 1)\n",
      "(u'artificiality', 1)\n",
      "(u'artificial_personality', 1)\n",
      "(u'artificial_language', 1)\n",
      "(u'artificial_lake', 1)\n",
      "(u'artificer', 1)\n",
      "(u'article_of_furniture', 1)\n",
      "(u'arthritis', 1)\n",
      "(u'artery', 1)\n",
      "(u'art_of_sculpture', 1)\n",
      "(u'art_historian', 1)\n",
      "(u'arranger', 1)\n",
      "(u'army_base', 1)\n",
      "(u'arms', 1)\n",
      "(u'armor', 1)\n",
      "(u'armed_service', 1)\n",
      "(u'aristotelian_logic', 1)\n",
      "(u'argumentation', 1)\n",
      "(u'arguing', 1)\n",
      "(u'architectural_style', 1)\n",
      "(u'archaeologist', 1)\n",
      "(u'archaeological_site', 1)\n",
      "(u'approval', 1)\n",
      "(u'appraisal', 1)\n",
      "(u'apportionment_of_seats', 1)\n",
      "(u'apportionment', 1)\n",
      "(u'apportioning', 1)\n",
      "(u'appointment', 1)\n",
      "(u'appendage', 1)\n",
      "(u'apartment_block', 1)\n",
      "(u'antique_white', 1)\n",
      "(u'anticipation', 1)\n",
      "(u'antibody', 1)\n",
      "(u'answer', 1)\n",
      "(u'annunciation', 1)\n",
      "(u'annum', 1)\n",
      "(u'annulment', 1)\n",
      "(u'annual_plant', 1)\n",
      "(u'animation', 1)\n",
      "(u'animal_testing', 1)\n",
      "(u'animal_husbandry', 1)\n",
      "(u'animal_experiment', 1)\n",
      "(u'animal_disease', 1)\n",
      "(u'amphibian', 1)\n",
      "(u'ammo', 1)\n",
      "(u'amino_acid', 1)\n",
      "(u'american_indian', 1)\n",
      "(u'amaranth', 1)\n",
      "(u'alphabet', 1)\n",
      "(u'alluvium', 1)\n",
      "(u'alluvion', 1)\n",
      "(u'alluvial_sediment', 1)\n",
      "(u'alluvial_deposit', 1)\n",
      "(u'allegory', 1)\n",
      "(u'algorithm', 1)\n",
      "(u'airstream', 1)\n",
      "(u'airport', 1)\n",
      "(u'aircraft_part', 1)\n",
      "(u'air_unit', 1)\n",
      "(u'air_transportation', 1)\n",
      "(u'air_temperature', 1)\n",
      "(u'air_quality', 1)\n",
      "(u'air_pollution', 1)\n",
      "(u'air_pollutant', 1)\n",
      "(u'agriculture', 1)\n",
      "(u'agricultural_machinery', 1)\n",
      "(u'agricultural_equipment', 1)\n",
      "(u'aggressor', 1)\n",
      "(u'aggregate', 1)\n",
      "(u'afro-asiatic_languages', 1)\n",
      "(u'afro-asiatic', 1)\n",
      "(u'african', 1)\n",
      "(u'affixation', 1)\n",
      "(u'affect', 1)\n",
      "(u'aerospace_engineer', 1)\n",
      "(u'aeronautical_engineering', 1)\n",
      "(u'aeronautical_engineer', 1)\n",
      "(u'aero_engine', 1)\n",
      "(u'advice', 1)\n",
      "(u'advertising', 1)\n",
      "(u'advertisement', 1)\n",
      "(u'adventure_story', 1)\n",
      "(u'adventure_game', 1)\n",
      "(u'address_space', 1)\n",
      "(u'adaptation', 1)\n",
      "(u'ad', 1)\n",
      "(u'actus_reus', 1)\n",
      "(u'actuation', 1)\n",
      "(u'actress', 1)\n",
      "(u'activator', 1)\n",
      "(u'action_at_law', 1)\n",
      "(u'acquiring', 1)\n",
      "(u'acne_vulgaris', 1)\n",
      "(u'acknowledgment', 1)\n",
      "(u'achievement', 1)\n",
      "(u'accuracy_and_precision', 1)\n",
      "(u'accuracy', 1)\n",
      "(u'accounting', 1)\n",
      "(u'accountancy', 1)\n",
      "(u'accordion', 1)\n",
      "(u'accidental_injury', 1)\n",
      "(u'accessory', 1)\n",
      "(u'acceptance', 1)\n",
      "(u'acceleration', 1)\n",
      "(u'academic_journal', 1)\n",
      "(u'abuse', 1)\n",
      "(u'abstinence', 1)\n",
      "(u'abrogation', 1)\n",
      "(u'abortion', 1)\n",
      "(u'aboriginal', 1)\n",
      "4233\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# person is the most popular hypernym in training set (310 times), followed by city (63)\n",
    "hypernym_distrib = Counter(train_hyper)\n",
    "for v, k in  sorted(((value, key) for (key,value) in hypernym_distrib.items()), reverse = True):\n",
    "    print (k,v)\n",
    "\n",
    "# There are 4,233 unique hypernyms in all\n",
    "print len(hypernym_distrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct synonyms for training, testing and validation terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_synyonyms(hyponyms, hypernyms, n=15):\n",
    "    synonyms = {}\n",
    "    for term in set(hyponyms):\n",
    "        term_hypernyms = [h for q, h in zip(hyponyms, hypernyms) if q == term]    \n",
    "        synonyms[term] = list(filter(lambda x: x not in term_hypernyms, zip(*model.most_similar(term, topn=20))[0]))[:n]\n",
    "        \n",
    "    return synonyms\n",
    "    \n",
    "#get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)    \n",
    "#get_synyonyms(valid_query, valid_hyper)    \n",
    "\n",
    "def get_random(hyponyms, hypernyms, vocab, n = 15):\n",
    "    random_words = {}\n",
    "    for term in set(hyponyms):\n",
    "        term_hypernyms = [h for q, h in zip(hyponyms, hypernyms) if q == term]    \n",
    "        \n",
    "        some_words = np.random.choice(vocab, 20, replace=False)        \n",
    "        random_words[term] = list(filter(lambda x: x not in term_hypernyms, some_words))[:n]\n",
    "    \n",
    "    return random_words\n",
    "\n",
    "#get_random(valid_query, valid_hyper, vocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, \n",
    "                 train_query, train_hyper, \n",
    "                 test_query, test_hyper, \n",
    "                 valid_query, valid_hyper, \n",
    "                 vocab, embeddings):\n",
    "        \n",
    "                \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.valid_query = valid_query\n",
    "        self.valid_hyper = valid_hyper\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        #self.synonyms = synonyms\n",
    "                \n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['animal'].shape[0]\n",
    "        \n",
    "        print (\"Tokenising words...\")\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = tokenizer = Tokenizer(num_words = 300000, filters='')\n",
    "        self.tokenizer.fit_on_texts(train_query + test_query + valid_query + vocab)\n",
    "        \n",
    "        print (\"Creating embedding matrix...\")\n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings[word]\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector (already normalised)\n",
    "                    #embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)\n",
    "        \n",
    "        print (\"Creating random words/synonyms...\")\n",
    "        self.random_words = get_random(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper, vocab)  \n",
    "        self.synonyms = get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = Data(train_query, train_hyper, test_query, test_hyper, valid_query, valid_hyper, vocab, model)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('.', 'pickle')\n",
    "#pickle.dump(data, open(os.path.join(dest, 'semeval_data.pkl'), 'wb'), protocol=2)\n",
    "data = pickle.load(open(os.path.join(dest, 'semeval_data.pkl'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'starkers', u'zefir', u'unmirrored', u'absorb', u'acushla', u'high-level_design', u'hyposensitivity', u'rumours', u'3801', u'destroyer_escort', u'demultiplex', u'dogma', u'ozonosphere', u'euthanize', u'polyhedral']\n",
      "[u'grand_slam', u'davis_cup', u'semifinal', u'unbeaten', u'world_championship', u'olympic_record', u'championship', u'post-match', u'cup_final', u'quarterfinal', u'bhupathi', u'clinched', u'junior_welterweight', u'unseeded', u'olympic_champion']\n"
     ]
    }
   ],
   "source": [
    "print (data.random_words['starcraft'])\n",
    "print (data.synonyms['rod_laver'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that returns negative samples alongside set of positive samples\n",
    "# we need to pass:\n",
    "# the batch hyponym terms, batch of hypernym terms, negative_tuples, tokenizer \n",
    "# to create sequences\n",
    "def extend_batch_with_negatives(batch_X_term, batch_X_hyper, negative_tuples,                              \n",
    "                                tokenizer, m):\n",
    "    # initialise negative tuples container\n",
    "    positive_words = [tokenizer.index_word[term_id] for term_id in batch_X_term.flatten()]\n",
    "    \n",
    "    # tokenize -ve samples\n",
    "    neg_terms, neg_hyper = [], []\n",
    "    for n in positive_words:\n",
    "        for n2 in negative_tuples[n][:m]:\n",
    "            neg_terms.append(n)\n",
    "            neg_hyper.append(n2)\n",
    "    \n",
    "    neg_terms_seq = tokenizer.texts_to_sequences(neg_terms)\n",
    "    neg_hyper_seq = tokenizer.texts_to_sequences(neg_hyper)\n",
    "\n",
    "    # before increasing size of our batch, let's set the actual y values\n",
    "    # the first n terms are true (1s), and the rest are the -ve samples (0)\n",
    "    batch_y_label = np.concatenate((\n",
    "            np.ones(batch_X_term.shape[0]),\n",
    "            np.zeros(len(neg_terms_seq))\n",
    "    ))\n",
    "    # finally, stack -ve sequences at the bottom of +ves to \n",
    "    # create our final training batch\n",
    "    # at most, batch size will be 192 samples            \n",
    "\n",
    "    batch_X_term = np.vstack((batch_X_term, np.array(neg_terms_seq)))\n",
    "    batch_X_hyper = np.vstack((batch_X_hyper, np.array(neg_hyper_seq)))\n",
    "    \n",
    "    return batch_X_term, batch_X_hyper, batch_y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_hypernyms_to_one_line(dataset):\n",
    "    ordered_queries = sorted(list(set(dataset[0])))\n",
    "    one_line = {}\n",
    "    for w in ordered_queries:\n",
    "        word_hypernyms = [h for q, h in zip(*dataset) if q == w]\n",
    "        one_line[w] = word_hypernyms\n",
    "    return one_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from task_scorer.py provided with shared task resources\n",
    "def mean_reciprocal_rank(r):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    r = np.asarray(r).nonzero()[0]\n",
    "    return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "def precision_at_k(r, k, n):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return (np.mean(r)*k)/min(k,n)\n",
    "    # Modified from the first version. Now the gold elements are taken into account\n",
    "\n",
    "def average_precision(r,n):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1, n) for k in range(r.size)]\n",
    "    #Modified from the first version (removed \"if r[k]\"). All elements (zero and nonzero) are taken into account\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "def mean_average_precision(r,n):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return average_precision(r,n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "def get_evaluation_scores(dataset, predictions):\n",
    "    all_scores = []    \n",
    "    scores_names = ['MRR', 'MAP', 'P@1', 'P@5', 'P@10']\n",
    "    for query, gold_hyps in convert_hypernyms_to_one_line(dataset).items():\n",
    "\n",
    "        avg_pat1 = []\n",
    "        avg_pat2 = []\n",
    "        avg_pat3 = []\n",
    "\n",
    "        pred_hyps = predictions[query]\n",
    "        gold_hyps_n = len(gold_hyps)    \n",
    "        r = [0 for i in range(15)]\n",
    "\n",
    "        for j in range(len(pred_hyps)):\n",
    "            if j < gold_hyps_n:\n",
    "                pred_hyp = pred_hyps[j]\n",
    "                if pred_hyp in gold_hyps:\n",
    "                    r[j] = 1\n",
    "\n",
    "        avg_pat1.append(precision_at_k(r,1,gold_hyps_n))\n",
    "        avg_pat2.append(precision_at_k(r,5,gold_hyps_n))\n",
    "        avg_pat3.append(precision_at_k(r,10,gold_hyps_n))    \n",
    "\n",
    "        mrr_score_numb = mean_reciprocal_rank(r)\n",
    "        map_score_numb = mean_average_precision(r,gold_hyps_n)\n",
    "        avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "        avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "        avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "        score_results = [mrr_score_numb, map_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "        all_scores.append(score_results)\n",
    "    return scores_names, all_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to run evaluation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# alternative hypernym generator by applying Phi weights to hyponym and see which \n",
    "# words are closest to this vector\n",
    "def crim_get_hypernym(word, tokenizer, phi, cluster_weight, bias, embeddings, top):\n",
    "    \n",
    "    q_idx = tokenizer.word_index[word]\n",
    "    q = embeddings[q_idx] \n",
    "\n",
    "    projections = np.dot(q, phi)\n",
    "    #projections /= np.linalg.norm(projections, axis=1).reshape(-1,1)\n",
    "    \n",
    "    sim_matrix = np.dot(cluster_weight.T, np.dot(embeddings[1:], projections.T).T) + bias\n",
    "    top_ranked_sequence = np.argsort(sim_matrix[0])[::-1][:top]\n",
    "    \n",
    "    return zip(tokenizer.sequences_to_texts(top_ranked_sequence.reshape(-1,1) + 1), \n",
    "               sim_matrix.flatten()[top_ranked_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_get_hypernym(word, tokenizer, phi, cluster_weight, bias, embeddings, top):\n",
    "    q_idx = tokenizer.word_index[word]\n",
    "    q = embeddings[q_idx] \n",
    "    \n",
    "    projections = np.dot(q, phi)\n",
    "    s = np.dot(embeddings[1:], projections.T)\n",
    "    linear_combination = (s.T * cluster_weight) + bias\n",
    "\n",
    "    best_projection = np.max(linear_combination, axis=0)\n",
    "    top_words = np.argsort(best_projection)[::-1][:top]\n",
    "    \n",
    "    return zip(tokenizer.sequences_to_texts(top_words.reshape(-1,1) + 1), \n",
    "               best_projection[top_words])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function which generates top 15 predictions for each hyponym query term\n",
    "# and returns results as dictionary\n",
    "def predict_crim_hypernyms(queries, tokenizer, model, algol):\n",
    "    #hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    #hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in set(data.valid_hyper)]\n",
    "    \n",
    "    ordered_queries = sorted(list(set(queries)))\n",
    "    results = {}\n",
    "        \n",
    "    # extract the Phi matrices out of trained model\n",
    "    dense = [l.get_weights()[0] for l in model.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "    dense = np.asarray(dense)\n",
    "    \n",
    "    # extract affine transform layer weights\n",
    "    cluster_weight = model.get_layer(name='Prediction').get_weights()[0]\n",
    "    bias = model.get_layer(name='Prediction').get_weights()[1]\n",
    "    embeddings = model.get_layer(name=\"TermEmbedding\").get_weights()[0]\n",
    "\n",
    "    \n",
    "    for idx, word in enumerate(ordered_queries):        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "        \n",
    "        #predicted_hypers = alt_get_hypernym(word, model, data, dense, 15)\n",
    "        predicted_hypers = algol(word, tokenizer, dense, cluster_weight, bias, embeddings, 15)\n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_cluster_hypernyms(queries, tokenizer, cluster_list, knn_model=None):\n",
    "    #hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    #hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in set(data.valid_hyper)]\n",
    "    \n",
    "    ordered_queries = sorted(list(set(queries)))\n",
    "    \n",
    "    results = {}\n",
    "        \n",
    "    # embeddings are present in a \"shared\" model that is used as the first layer of each cluster\n",
    "    embeddings = [l for l in cluster_list[0].model.layers if type(l) == Model][0].get_layer(name='WE').get_weights()[0]\n",
    "    \n",
    "    # extract the Phi matrices out of trained model\n",
    "    dense = np.zeros((len(cluster_list), embeddings.shape[1], embeddings.shape[1]))\n",
    "    lr_weights = np.zeros((len(cluster_list), 1))\n",
    "    lr_bias = np.zeros((len(cluster_list), 1))\n",
    "\n",
    "    for idx, cluster in enumerate(cluster_list):\n",
    "        dense[idx] = cluster.model.get_layer(name='Phi0').get_weights()[0]\n",
    "        lr_weights[idx] = cluster.model.get_layer(name='Prediction').get_weights()[0]\n",
    "        lr_bias[idx] = cluster.model.get_layer(name='Prediction').get_weights()[1]    \n",
    "    \n",
    "    for idx, word in enumerate(ordered_queries):        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "            \n",
    "            \n",
    "        if knn_model:            \n",
    "            cluster_probs = knn_model.predict_proba(embeddings[tokenizer.word_index[word]].reshape(1,-1))\n",
    "            cluster_idx = np.where(cluster_probs > 0.)[1]                        \n",
    "            predicted_hypers = cluster_get_hypernym(word, tokenizer, \n",
    "                                                    dense[cluster_idx], \n",
    "                                                    lr_weights[cluster_idx], \n",
    "                                                    lr_bias[cluster_idx], embeddings, 15)\n",
    "        else:        \n",
    "            #predicted_hypers = alt_get_hypernym(word, model, data, dense, 15)\n",
    "            predicted_hypers = cluster_get_hypernym(word, tokenizer, \n",
    "                                                    dense, lr_weights, lr_bias, \n",
    "                                                    embeddings, 15)\n",
    "        \n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bierstadt': [u'cultural_landscape',\n",
       "  u'television_production',\n",
       "  u'business_establishment',\n",
       "  u'convey',\n",
       "  u'musical_work',\n",
       "  u'business_organization',\n",
       "  u'cultural_tourism',\n",
       "  u'visual_art',\n",
       "  u'performing_arts',\n",
       "  u'theatrical_production',\n",
       "  u'commercial_agency',\n",
       "  u'musical_performance',\n",
       "  u'news_event',\n",
       "  u'subject',\n",
       "  u'artistic'],\n",
       " u'bloodguilt': [u'person',\n",
       "  u'television_production',\n",
       "  u'electronic_media',\n",
       "  u'subject',\n",
       "  u'context',\n",
       "  u'musical_work',\n",
       "  u'scope',\n",
       "  u'convey',\n",
       "  u'responsibility',\n",
       "  u'relate',\n",
       "  u'news_event',\n",
       "  u'terms',\n",
       "  u'scriptwriting',\n",
       "  u'creative_work',\n",
       "  u'relevant'],\n",
       " u'boatlift': [u'television_production',\n",
       "  u'business_establishment',\n",
       "  u'business_organization',\n",
       "  u'cultural_landscape',\n",
       "  u'business',\n",
       "  u'convey',\n",
       "  u'electronic_media',\n",
       "  u'entertainment',\n",
       "  u'musical_work',\n",
       "  u'responsibility',\n",
       "  u'performing_arts',\n",
       "  u'audience',\n",
       "  u'sponsorship',\n",
       "  u'local_community',\n",
       "  u'professional_sports'],\n",
       " u'burger_king': [u'television_production',\n",
       "  u'business_organization',\n",
       "  u'business',\n",
       "  u'personal_computer_hardware',\n",
       "  u'end-user',\n",
       "  u'promotional',\n",
       "  u'end_user',\n",
       "  u'service_provider',\n",
       "  u'software_application',\n",
       "  u'commercial_agency',\n",
       "  u'user',\n",
       "  u'internet_site',\n",
       "  u'multimedia',\n",
       "  u'computer_networking',\n",
       "  u'computer_network'],\n",
       " u'burgh': [u'business_establishment',\n",
       "  u'cultural_landscape',\n",
       "  u'business_organization',\n",
       "  u'business',\n",
       "  u'responsibility',\n",
       "  u'local_government',\n",
       "  u'legal_entity',\n",
       "  u'enabling_legislation',\n",
       "  u'business_relationship',\n",
       "  u'television_production',\n",
       "  u'municipal_government',\n",
       "  u'convey',\n",
       "  u'economic_sector',\n",
       "  u'owner',\n",
       "  u'commercial_activity'],\n",
       " u'calmodulin': [u'end-user',\n",
       "  u'software_application',\n",
       "  u'computer_software',\n",
       "  u'software_component',\n",
       "  u'end_user',\n",
       "  u'information_processing_system',\n",
       "  u'application_software',\n",
       "  u'computer_hardware',\n",
       "  u'technological',\n",
       "  u'computing_system',\n",
       "  u'computer',\n",
       "  u'user',\n",
       "  u'computer_networking',\n",
       "  u'designing',\n",
       "  u'computer_technology'],\n",
       " u'catechumen': [u'person',\n",
       "  u'convey',\n",
       "  u'value_system',\n",
       "  u'embodying',\n",
       "  u'embody',\n",
       "  u'context',\n",
       "  u'subject',\n",
       "  u'responsibility',\n",
       "  u'society',\n",
       "  u'embodied',\n",
       "  u'recognize',\n",
       "  u'public_servant',\n",
       "  u'personally',\n",
       "  u'creative_work',\n",
       "  u'life_story'],\n",
       " u'cingulate_gyrus': [u'information_processing_system',\n",
       "  u'end-user',\n",
       "  u'basic',\n",
       "  u'scope',\n",
       "  u'physical',\n",
       "  u'defining',\n",
       "  u'concepts',\n",
       "  u'software_application',\n",
       "  u'relate',\n",
       "  u'technological',\n",
       "  u'particular',\n",
       "  u'software_component',\n",
       "  u'participant',\n",
       "  u'conceptual',\n",
       "  u'context'],\n",
       " u'cloister': [u'convey',\n",
       "  u'cultural_landscape',\n",
       "  u'person',\n",
       "  u'business_establishment',\n",
       "  u'value_system',\n",
       "  u'responsibility',\n",
       "  u'business_organization',\n",
       "  u'television_production',\n",
       "  u'tangible',\n",
       "  u'society',\n",
       "  u'subject',\n",
       "  u'business',\n",
       "  u'musical_work',\n",
       "  u'context',\n",
       "  u'legal_person'],\n",
       " u'cromford_canal': [u'cultural_landscape',\n",
       "  u'destination',\n",
       "  u'business_organization',\n",
       "  u'transportation',\n",
       "  u'cultural_tourism',\n",
       "  u'business_establishment',\n",
       "  u'intelligent_transportation_system',\n",
       "  u'local_community',\n",
       "  u'commercial_agency',\n",
       "  u'built_environment',\n",
       "  u'business',\n",
       "  u'economic_sector',\n",
       "  u'tourism',\n",
       "  u'line_organization',\n",
       "  u'enabling_legislation'],\n",
       " u'dirham': [u'business',\n",
       "  u'business_organization',\n",
       "  u'person',\n",
       "  u'financial_transaction',\n",
       "  u'responsibility',\n",
       "  u'legal',\n",
       "  u'asset',\n",
       "  u'economic_sector',\n",
       "  u'goods_and_services',\n",
       "  u'stipulate',\n",
       "  u'business_relationship',\n",
       "  u'rules',\n",
       "  u'behalf',\n",
       "  u'legal_person',\n",
       "  u'tangible'],\n",
       " u'don_mckellar': [u'television_production',\n",
       "  u'scriptwriting',\n",
       "  u'musical_work',\n",
       "  u'theatrical_production',\n",
       "  u'film_making',\n",
       "  u'film_production',\n",
       "  u'scriptwriter',\n",
       "  u'entertainment_industry',\n",
       "  u'music_video',\n",
       "  u'musical_performance',\n",
       "  u'edutainment',\n",
       "  u'feature_film',\n",
       "  u'media_professional',\n",
       "  u'actor',\n",
       "  u'audience'],\n",
       " u'dragonfly': [u'computer_graphic',\n",
       "  u'graphic',\n",
       "  u'television_production',\n",
       "  u'edutainment',\n",
       "  u'multimedia',\n",
       "  u'graphics',\n",
       "  u'software_application',\n",
       "  u'end-user',\n",
       "  u'musical_performance',\n",
       "  u'storyboard',\n",
       "  u'interactive',\n",
       "  u'musical_work',\n",
       "  u'user',\n",
       "  u'multimedia_system',\n",
       "  u'theatrical_production'],\n",
       " u'drug_of_abuse': [u'scope',\n",
       "  u'end-user',\n",
       "  u'business_organization',\n",
       "  u'terms',\n",
       "  u'economic_sector',\n",
       "  u'computer_networking',\n",
       "  u'particular',\n",
       "  u'technological',\n",
       "  u'purpose',\n",
       "  u'telecommunication',\n",
       "  u'computer_software',\n",
       "  u'applicable',\n",
       "  u'defining',\n",
       "  u'take_into_account',\n",
       "  u'technical'],\n",
       " u'emmeline_pankhurst': [u'person',\n",
       "  u'business',\n",
       "  u'business_establishment',\n",
       "  u'responsibility',\n",
       "  u'television_production',\n",
       "  u'elected_official',\n",
       "  u'business_organization',\n",
       "  u'public_servant',\n",
       "  u'behalf',\n",
       "  u'role_model',\n",
       "  u'participant',\n",
       "  u'convey',\n",
       "  u'promotional',\n",
       "  u'young_person',\n",
       "  u'personally'],\n",
       " u'entente': [u'responsibility',\n",
       "  u'respect',\n",
       "  u'governing',\n",
       "  u'business',\n",
       "  u'business_organization',\n",
       "  u'behalf',\n",
       "  u'rules',\n",
       "  u'law',\n",
       "  u'legal',\n",
       "  u'governmental',\n",
       "  u'obligation',\n",
       "  u'rights',\n",
       "  u'policy',\n",
       "  u'scope',\n",
       "  u'political'],\n",
       " u'fertility_rate': [u'scope',\n",
       "  u'business_organization',\n",
       "  u'economic_sector',\n",
       "  u'actual',\n",
       "  u'goods_and_services',\n",
       "  u'purpose',\n",
       "  u'take_into_account',\n",
       "  u'advance',\n",
       "  u'terms',\n",
       "  u'basic',\n",
       "  u'societal',\n",
       "  u'defining',\n",
       "  u'technological',\n",
       "  u'computer_software',\n",
       "  u'tangible'],\n",
       " u'funen': [u'cultural_landscape',\n",
       "  u'destination',\n",
       "  u'business_establishment',\n",
       "  u'commercial_agency',\n",
       "  u'television_production',\n",
       "  u'cultural_tourism',\n",
       "  u'city',\n",
       "  u'convey',\n",
       "  u'person',\n",
       "  u'business_organization',\n",
       "  u'tourist',\n",
       "  u'performing_arts',\n",
       "  u'business',\n",
       "  u'tourism',\n",
       "  u'transportation'],\n",
       " u'gamesmanship': [u'responsibility',\n",
       "  u'business',\n",
       "  u'business_organization',\n",
       "  u'behalf',\n",
       "  u'person',\n",
       "  u'television_production',\n",
       "  u'scope',\n",
       "  u'public_service',\n",
       "  u'convey',\n",
       "  u'participant',\n",
       "  u'electronic_media',\n",
       "  u'elected_official',\n",
       "  u'context',\n",
       "  u'purpose',\n",
       "  u'managerial'],\n",
       " u'griselda': [u'person',\n",
       "  u'theatrical_production',\n",
       "  u'cultural_landscape',\n",
       "  u'musical_performance',\n",
       "  u'television_production',\n",
       "  u'musical_work',\n",
       "  u'convey',\n",
       "  u'business_establishment',\n",
       "  u'musical_composition',\n",
       "  u'artistic',\n",
       "  u'life_story',\n",
       "  u'famous_person',\n",
       "  u'performing_arts',\n",
       "  u'visual_art',\n",
       "  u'sound_recording'],\n",
       " u'heming': [u'television_production',\n",
       "  u'participant',\n",
       "  u'business_organization',\n",
       "  u'promotional',\n",
       "  u'business',\n",
       "  u'musical_work',\n",
       "  u'media_professional',\n",
       "  u'theatrical_production',\n",
       "  u'creative',\n",
       "  u'person',\n",
       "  u'news_event',\n",
       "  u'audience',\n",
       "  u'performing_arts',\n",
       "  u'subject',\n",
       "  u'cultural_landscape'],\n",
       " u'honorable_discharge': [u'responsibility',\n",
       "  u'scope',\n",
       "  u'business',\n",
       "  u'business_organization',\n",
       "  u'policy',\n",
       "  u'electronic_media',\n",
       "  u'sponsorship',\n",
       "  u'subject',\n",
       "  u'enabling_legislation',\n",
       "  u'purpose',\n",
       "  u'governmental',\n",
       "  u'television_production',\n",
       "  u'behalf',\n",
       "  u'local_government',\n",
       "  u'educational_institution'],\n",
       " u'hurricane_iniki': [u'cultural_landscape',\n",
       "  u'local_community',\n",
       "  u'economic_sector',\n",
       "  u'built_environment',\n",
       "  u'telecommunication_system',\n",
       "  u'transportation',\n",
       "  u'tourism',\n",
       "  u'meteorology',\n",
       "  u'weather_forecasting',\n",
       "  u'destination',\n",
       "  u'operate',\n",
       "  u'teleshopping',\n",
       "  u'tangible',\n",
       "  u'business_establishment',\n",
       "  u'societal'],\n",
       " u'in-joke': [u'television_production',\n",
       "  u'scriptwriting',\n",
       "  u'musical_work',\n",
       "  u'edutainment',\n",
       "  u'musical_performance',\n",
       "  u'multimedia',\n",
       "  u'graphic',\n",
       "  u'storyboard',\n",
       "  u'interactive_media',\n",
       "  u'theatrical_production',\n",
       "  u'musical_composition',\n",
       "  u'film_production',\n",
       "  u'film_making',\n",
       "  u'personal_computer_hardware',\n",
       "  u'electronic_media'],\n",
       " u'jurmo': [u'cultural_landscape',\n",
       "  u'convey',\n",
       "  u'business_organization',\n",
       "  u'tangible',\n",
       "  u'business',\n",
       "  u'scope',\n",
       "  u'participant',\n",
       "  u'responsibility',\n",
       "  u'local_community',\n",
       "  u'economic_sector',\n",
       "  u'context',\n",
       "  u'business_establishment',\n",
       "  u'defining',\n",
       "  u'cultural',\n",
       "  u'cultural_tourism'],\n",
       " u'konstantin_tsiolkovsky': [u'television_production',\n",
       "  u'participant',\n",
       "  u'person',\n",
       "  u'business_organization',\n",
       "  u'edutainment',\n",
       "  u'business',\n",
       "  u'subject',\n",
       "  u'scriptwriting',\n",
       "  u'theatrical_production',\n",
       "  u'creative',\n",
       "  u'convey',\n",
       "  u'industrial_design',\n",
       "  u'end-user',\n",
       "  u'performing_arts',\n",
       "  u'musical_work'],\n",
       " u'kralendijk': [u'cultural_landscape',\n",
       "  u'destination',\n",
       "  u'built_environment',\n",
       "  u'business_organization',\n",
       "  u'cultural_tourism',\n",
       "  u'business_establishment',\n",
       "  u'economic_sector',\n",
       "  u'tourism',\n",
       "  u'television_production',\n",
       "  u'intelligent_transportation_system',\n",
       "  u'transportation',\n",
       "  u'commercial_agency',\n",
       "  u'end-user',\n",
       "  u'amusement_ride',\n",
       "  u'tourist_attraction'],\n",
       " u'lemongrass': [u'end-user',\n",
       "  u'particular',\n",
       "  u'television_production',\n",
       "  u'person',\n",
       "  u'musical_work',\n",
       "  u'relate',\n",
       "  u'example',\n",
       "  u'cultural_landscape',\n",
       "  u'business_organization',\n",
       "  u'edutainment',\n",
       "  u'product_design',\n",
       "  u'definition',\n",
       "  u'context',\n",
       "  u'physical',\n",
       "  u'computer_graphic'],\n",
       " u'marquee': [u'television_production',\n",
       "  u'business_organization',\n",
       "  u'business_establishment',\n",
       "  u'personal_computer_hardware',\n",
       "  u'end-user',\n",
       "  u'teleshopping',\n",
       "  u'business',\n",
       "  u'computer_graphic',\n",
       "  u'promotional',\n",
       "  u'multimedia',\n",
       "  u'edutainment',\n",
       "  u'professional_sports',\n",
       "  u'musical_work',\n",
       "  u'graphic',\n",
       "  u'entertainment'],\n",
       " u'matthew_murphy': [u'television_production',\n",
       "  u'participant',\n",
       "  u'business',\n",
       "  u'person',\n",
       "  u'multimedia',\n",
       "  u'edutainment',\n",
       "  u'end-user',\n",
       "  u'software_application',\n",
       "  u'scope',\n",
       "  u'responsibility',\n",
       "  u'business_organization',\n",
       "  u'service_provider',\n",
       "  u'user',\n",
       "  u'computer_networking',\n",
       "  u'context'],\n",
       " u'minnamurra': [u'cultural_landscape',\n",
       "  u'destination',\n",
       "  u'cultural_tourism',\n",
       "  u'park',\n",
       "  u'tourist_attraction',\n",
       "  u'built_environment',\n",
       "  u'local_community',\n",
       "  u'tourism',\n",
       "  u'convey',\n",
       "  u'transportation',\n",
       "  u'geographical',\n",
       "  u'business_establishment',\n",
       "  u'natural_landscape',\n",
       "  u'tourist',\n",
       "  u'heritage'],\n",
       " u'neon_genesis_evangelion': [u'television_production',\n",
       "  u'scriptwriting',\n",
       "  u'edutainment',\n",
       "  u'musical_work',\n",
       "  u'interactive_media',\n",
       "  u'context',\n",
       "  u'multimedia',\n",
       "  u'interactive_entertainment',\n",
       "  u'storyboard',\n",
       "  u'musical_performance',\n",
       "  u'electronic_media',\n",
       "  u'creative',\n",
       "  u'film_making',\n",
       "  u'computer_game',\n",
       "  u'creative_work'],\n",
       " u'nsanje': [u'cultural_landscape',\n",
       "  u'local_community',\n",
       "  u'economic_sector',\n",
       "  u'enabling_legislation',\n",
       "  u'local_government',\n",
       "  u'societal',\n",
       "  u'scope',\n",
       "  u'responsibility',\n",
       "  u'transportation',\n",
       "  u'national',\n",
       "  u'governmental',\n",
       "  u'tangible',\n",
       "  u'business_establishment',\n",
       "  u'business',\n",
       "  u'citizens'],\n",
       " u'palisade': [u'cultural_landscape',\n",
       "  u'business_establishment',\n",
       "  u'business_organization',\n",
       "  u'economic_sector',\n",
       "  u'end-user',\n",
       "  u'convey',\n",
       "  u'commercial_agency',\n",
       "  u'intelligent_transportation_system',\n",
       "  u'destination',\n",
       "  u'cultural_tourism',\n",
       "  u'operate',\n",
       "  u'built_environment',\n",
       "  u'scope',\n",
       "  u'telecommunication_system',\n",
       "  u'relate'],\n",
       " u'pinny': [u'television_production',\n",
       "  u'person',\n",
       "  u'musical_work',\n",
       "  u'business_organization',\n",
       "  u'convey',\n",
       "  u'musical_composition',\n",
       "  u'creative_work',\n",
       "  u'creative',\n",
       "  u'relate',\n",
       "  u'promotional',\n",
       "  u'participant',\n",
       "  u'musical_performance',\n",
       "  u'theatrical_production',\n",
       "  u'subject',\n",
       "  u'sound_recording'],\n",
       " u'pinot_blanc': [u'television_production',\n",
       "  u'musical_work',\n",
       "  u'cultural_landscape',\n",
       "  u'person',\n",
       "  u'business_organization',\n",
       "  u'end-user',\n",
       "  u'musical_composition',\n",
       "  u'sound_recording',\n",
       "  u'convey',\n",
       "  u'business_establishment',\n",
       "  u'musical_performance',\n",
       "  u'business',\n",
       "  u'relate',\n",
       "  u'end_user',\n",
       "  u'theatrical_production'],\n",
       " u'polysemy': [u'context',\n",
       "  u'defining',\n",
       "  u'scope',\n",
       "  u'definition',\n",
       "  u'electronic_media',\n",
       "  u'convey',\n",
       "  u'purpose',\n",
       "  u'particular',\n",
       "  u'subject',\n",
       "  u'relevant',\n",
       "  u'terms',\n",
       "  u'define',\n",
       "  u'actual',\n",
       "  u'conceptual',\n",
       "  u'meaningful'],\n",
       " u'quebec': [u'business_organization',\n",
       "  u'business',\n",
       "  u'responsibility',\n",
       "  u'local_government',\n",
       "  u'business_establishment',\n",
       "  u'enabling_legislation',\n",
       "  u'commercial_activity',\n",
       "  u'economic_sector',\n",
       "  u'cultural_landscape',\n",
       "  u'governmental',\n",
       "  u'legal_entity',\n",
       "  u'local_community',\n",
       "  u'convey',\n",
       "  u'ownership',\n",
       "  u'behalf'],\n",
       " u'relcom': [u'end-user',\n",
       "  u'computer_network',\n",
       "  u'computer_system',\n",
       "  u'end_user',\n",
       "  u'application_software',\n",
       "  u'software_application',\n",
       "  u'computer_software',\n",
       "  u'user',\n",
       "  u'computer',\n",
       "  u'computer_hardware',\n",
       "  u'electronic_media',\n",
       "  u'service_provider',\n",
       "  u'software_product',\n",
       "  u'applications_software',\n",
       "  u'telecommunication'],\n",
       " u'roundhouse': [u'cultural_landscape',\n",
       "  u'business_establishment',\n",
       "  u'television_production',\n",
       "  u'cultural_tourism',\n",
       "  u'commercial_agency',\n",
       "  u'destination',\n",
       "  u'performing_arts',\n",
       "  u'business_organization',\n",
       "  u'amusement_ride',\n",
       "  u'professional_sports',\n",
       "  u'built_environment',\n",
       "  u'adaptive_reuse',\n",
       "  u'musical_performance',\n",
       "  u'intelligent_transportation_system',\n",
       "  u'theatrical_production'],\n",
       " u'sedative': [u'particular',\n",
       "  u'scope',\n",
       "  u'specific',\n",
       "  u'person',\n",
       "  u'terms',\n",
       "  u'relevant',\n",
       "  u'definition',\n",
       "  u'physical',\n",
       "  u'example',\n",
       "  u'medical_diagnosis',\n",
       "  u'specifically',\n",
       "  u'scientific_discipline',\n",
       "  u'defining',\n",
       "  u'purpose',\n",
       "  u'subject'],\n",
       " u'shambles': [u'responsibility',\n",
       "  u'business',\n",
       "  u'business_organization',\n",
       "  u'convey',\n",
       "  u'business_establishment',\n",
       "  u'person',\n",
       "  u'behalf',\n",
       "  u'television_production',\n",
       "  u'value_system',\n",
       "  u'respect',\n",
       "  u'economic_sector',\n",
       "  u'governing',\n",
       "  u'electronic_media',\n",
       "  u'sponsorship',\n",
       "  u'business_relationship'],\n",
       " u'shellac': [u'end-user',\n",
       "  u'end_user',\n",
       "  u'personal_computer_hardware',\n",
       "  u'software_component',\n",
       "  u'particular',\n",
       "  u'computer_software',\n",
       "  u'musical_work',\n",
       "  u'business_organization',\n",
       "  u'technical_specification',\n",
       "  u'terms',\n",
       "  u'software_publisher',\n",
       "  u'sound_recording',\n",
       "  u'specific',\n",
       "  u'domain_engineering',\n",
       "  u'software_product'],\n",
       " u'showroom': [u'business_organization',\n",
       "  u'end-user',\n",
       "  u'business',\n",
       "  u'television_production',\n",
       "  u'business_establishment',\n",
       "  u'telecommunication_equipment',\n",
       "  u'personal_computer_hardware',\n",
       "  u'end_user',\n",
       "  u'commercial_agency',\n",
       "  u'product_design',\n",
       "  u'service_provider',\n",
       "  u'teleshopping',\n",
       "  u'telecommunication',\n",
       "  u'computer_network',\n",
       "  u'computer_networking'],\n",
       " u'sociology_department': [u'business_organization',\n",
       "  u'business',\n",
       "  u'responsibility',\n",
       "  u'scope',\n",
       "  u'public_service',\n",
       "  u'purpose',\n",
       "  u'participant',\n",
       "  u'electronic_media',\n",
       "  u'television_production',\n",
       "  u'educational_institution',\n",
       "  u'society',\n",
       "  u'tangible',\n",
       "  u'convener',\n",
       "  u'managerial',\n",
       "  u'defining'],\n",
       " u'starcraft': [u'software_application',\n",
       "  u'computer',\n",
       "  u'multimedia',\n",
       "  u'video_game_development',\n",
       "  u'edutainment',\n",
       "  u'television_production',\n",
       "  u'personal_computer_hardware',\n",
       "  u'user',\n",
       "  u'interactive_entertainment',\n",
       "  u'end-user',\n",
       "  u'interactive_media',\n",
       "  u'computer_game',\n",
       "  u'graphics',\n",
       "  u'software_product',\n",
       "  u'application_software'],\n",
       " u'tahini': [u'end-user',\n",
       "  u'economic_sector',\n",
       "  u'scope',\n",
       "  u'end_user',\n",
       "  u'business_organization',\n",
       "  u'telecommunication_system',\n",
       "  u'particular',\n",
       "  u'definition',\n",
       "  u'example',\n",
       "  u'technical_specification',\n",
       "  u'computer_software',\n",
       "  u'television_production',\n",
       "  u'computer_networking',\n",
       "  u'software_application',\n",
       "  u'relevant'],\n",
       " u'tim_tebow': [u'television_production',\n",
       "  u'professional_sports',\n",
       "  u'business_organization',\n",
       "  u'edutainment',\n",
       "  u'interactive_entertainment',\n",
       "  u'participant',\n",
       "  u'relate',\n",
       "  u'product_design',\n",
       "  u'person',\n",
       "  u'end-user',\n",
       "  u'brand_image',\n",
       "  u'brand_equity',\n",
       "  u'co-branding',\n",
       "  u'news_event',\n",
       "  u'musical_performance'],\n",
       " u'timepiece': [u'television_production',\n",
       "  u'end-user',\n",
       "  u'personal_computer_hardware',\n",
       "  u'end_user',\n",
       "  u'product_design',\n",
       "  u'musical_work',\n",
       "  u'computer',\n",
       "  u'electronic_media',\n",
       "  u'software_application',\n",
       "  u'edutainment',\n",
       "  u'computer_graphic',\n",
       "  u'software_product',\n",
       "  u'multimedia',\n",
       "  u'interactive_media',\n",
       "  u'computer_software'],\n",
       " u'tussle': [u'person',\n",
       "  u'television_production',\n",
       "  u'business',\n",
       "  u'business_organization',\n",
       "  u'responsibility',\n",
       "  u'participant',\n",
       "  u'behalf',\n",
       "  u'entertainment',\n",
       "  u'entertainment_industry',\n",
       "  u'professional_sports',\n",
       "  u'promotional',\n",
       "  u'audience',\n",
       "  u'sponsorship',\n",
       "  u'business_establishment',\n",
       "  u'business_relationship']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cluster_hypernyms(data.valid_query, data.tokenizer, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This method retrieves the words which our model considers the most probably hypernyms.  \n",
    "# Problem with this method is that it's excruciatingly slow so I developed a numpy-based\n",
    "# algorithm which is significantly faster.  Refer to alt_get_hypernym\n",
    "\n",
    "def crim_get_top_hypernyms(query, hyper_candidates, model, data, top):    \n",
    "    candidates = data.tokenizer.texts_to_sequences(data.vocab)\n",
    "    candidates = np.asarray(candidates).flatten()\n",
    "    \n",
    "    query_id = data.tokenizer.word_index[query]\n",
    "    predictions = model.predict([np.asarray([query_id] * len(data.vocab)), candidates])\n",
    "\n",
    "    best_predictions = np.argsort(predictions.flatten())[::-1][:top]\n",
    "    return list(map(lambda x: (data.vocab[x], predictions[x][0]), best_predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = data.tokenizer.texts_to_sequences(data.vocab)\n",
    "candidates = np.asarray(candidates).flatten()\n",
    "\n",
    "predictions = np.zeros((5, len(data.vocab)))\n",
    "query_id = data.tokenizer.word_index['starcraft']\n",
    "for idx, c in enumerate(cluster_list):\n",
    "    predictions[idx] = c.model.predict([np.asarray([query_id] * len(data.vocab)), candidates]).flatten()\n",
    "    \n",
    "best_projection = np.max(predictions, axis=0)\n",
    "top_words = np.argsort(best_projection)[::-1][:15]\n",
    "\n",
    "zip( map ( lambda x: data.tokenizer.index_word[candidates[x]], top_words), \n",
    "               best_projection[top_words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Projection Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Phi layer initialiser\n",
    "def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "    rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)\n",
    "    #identity = K.eye(shape[-1], dtype='float32')        \n",
    "    rident = tf.eye(shape[-1]) * rnorm\n",
    "    return rident\n",
    "\n",
    "def random_plus_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "    rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)    \n",
    "    rident = tf.eye(shape[-1]) + rnorm\n",
    "    return rident\n",
    "\n",
    "def random_normal(shape, dtype=\"float32\", partition_info=None): \n",
    "    return K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.05) \n",
    "\n",
    "def get_CRIM_model(phi_k=1, train_embeddings=False,\\\n",
    "                   embeddings_dim=300, vocab_size=1000,\\\n",
    "                   embeddings_matrix=None,\n",
    "                   phi_init = None,\n",
    "                   phi_activity_regularisation = None,\n",
    "                   sigmoid_kernel_regularisation = None,\n",
    "                   sigmoid_bias_regularisation = None,\n",
    "                   sigmoid_kernel_constraint = None,\n",
    "                   dropout_rate = 0.,\n",
    "                   learning_rate = 0.001\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size + 1, embeddings_dim, embeddings_constraint = UnitNorm(axis=1), \n",
    "                                input_length=1, name='TermEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = embedding_layer(hypo_input)    \n",
    "    hyper_embedding = embedding_layer(hyper_input)\n",
    "    \n",
    "    # Add Dropout to avoid overfit    \n",
    "    hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "    hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init,                               \n",
    "                               name='Phi%d' % (i)) (hypo_embedding))\n",
    "        \n",
    "\n",
    "    #phi1 = Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                #kernel_initializer=random_identity, name='Phi1')(hypo_embedding)\n",
    "\n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "    \n",
    "    phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "    \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=Zeros,\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer=Zeros,                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       ) (phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "        \n",
    "    # inject pre-trained embedding weights into Embedding layer\n",
    "    model.get_layer(name='TermEmbedding').set_weights([embeddings_matrix])\n",
    "    model.get_layer(name='TermEmbedding').trainable = train_embeddings    \n",
    "\n",
    "    adam = Adam(lr = learning_rate, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idea is to train an initial model to generate reasonabl projection matrices that\n",
    "# when applied to a hyponym, gets it close to the hypernym in question.\n",
    "\n",
    "# after initial training, we will inject the phi and sigmoid layer trained weights into the model;\n",
    "# the projections will be frozen but instead we will adjust the embeddings.  \n",
    "# prediction layer weights may also be modified \n",
    "def get_CRIM_model_freeze_phi(phi_k=1, train_phi=False,\n",
    "                              phi_weights = None,\n",
    "                              lr_weights = None,\n",
    "                              embeddings_dim=300, vocab_size=1000,\n",
    "                              embeddings_matrix=None,\n",
    "                              phi_init = None,\n",
    "                              phi_activity_regularisation = None,\n",
    "                              sigmoid_kernel_regularisation = None,\n",
    "                              sigmoid_bias_regularisation = None,\n",
    "                              sigmoid_kernel_constraint = None,\n",
    "                              dropout_rate = 0.,\n",
    "                              learning_rate = 0.00025\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size + 1, embeddings_dim, embeddings_constraint = UnitNorm(axis=1), \n",
    "                                input_length=1, name='TermEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = embedding_layer(hypo_input)    \n",
    "    hyper_embedding = embedding_layer(hyper_input)\n",
    "    \n",
    "    # Add Dropout to avoid overfit    \n",
    "    hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "    hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init,                               \n",
    "                               name='Phi%d' % (i)) (hypo_embedding))\n",
    "            \n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "            \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=Zeros,\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer=Zeros,                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       ) (phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "        \n",
    "    # inject pre-trained embedding weights into Embedding layer\n",
    "    model.get_layer(name='TermEmbedding').set_weights([embeddings_matrix])\n",
    "    #model.get_layer(name='TermEmbedding').trainable = train_embeddings    \n",
    "    \n",
    "    phi_projections = [l for l in model.layers if l.name.startswith('Phi')]    \n",
    "    for idx, phi_projection in enumerate(phi_projections):\n",
    "        phi_projection.set_weights([phi_weights[idx]])\n",
    "        phi_projection.trainable = train_phi\n",
    "        \n",
    "    model.get_layer(name='Prediction').set_weights(lr_weights)\n",
    "    \n",
    "    adam = Adam(lr=learning_rate, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training algorithm incorporates mini-batch stochastic descent and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,       # the model which parameters will be learnt\n",
    "          epochs,      # number of epochs to run          \n",
    "          batch_size,  # size of mini-batch\n",
    "          m,           # number of negative samples\n",
    "          data,        # data required for training                              \n",
    "          neg_strategy\n",
    "         ):\n",
    "\n",
    "            \n",
    "    # create sequences\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.valid_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.valid_hyper)\n",
    "                \n",
    "    samples = np.arange(len(term_train_seq))\n",
    "    validation_samples = np.arange(len(term_test_seq))\n",
    "    \n",
    "    # train algorithm\n",
    "    for epoch in range(epochs):\n",
    "        # reset loss\n",
    "        loss = 0.\n",
    "        test_loss = 0.\n",
    "                        \n",
    "        np.random.shuffle(samples)        \n",
    "\n",
    "        shuffled_X_term, shuffled_X_hyper =\\\n",
    "            np.array(term_train_seq, dtype='int32')[samples],\\\n",
    "            np.array(hyper_train_seq, dtype='int32')[samples]\n",
    "\n",
    "        for b in range(0, len(samples), batch_size):\n",
    "            # product mini-batch, consisting of 32 +ve samples\n",
    "            batch_X_term = shuffled_X_term[b:b + batch_size] \n",
    "            batch_X_hyper = shuffled_X_hyper[b:b + batch_size]\n",
    "\n",
    "            # complement +ve samples with negatives\n",
    "            batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "            extend_batch_with_negatives(batch_X_term, batch_X_hyper,\n",
    "                                        neg_strategy,\n",
    "                                        data.tokenizer, m\n",
    "                                       )            \n",
    "            \n",
    "            # shuffle validation set indices\n",
    "            #np.random.shuffle(validation_samples)\n",
    "            \n",
    "            # pick batch of shuffled test instances with size equal to training batch\n",
    "            batch_X_test_term, batch_X_test_hyper =\\\n",
    "                np.array(term_test_seq, dtype='int32')[validation_samples],\\\n",
    "                np.array(hyper_test_seq, dtype='int32')[validation_samples]\n",
    "            \n",
    "            #batch_y_test_label = np.ones(batch_X_test_term.shape[0]),\n",
    "            \n",
    "            # distort test batch with some negatives to check how algorithm fares with\n",
    "            # negatives\n",
    "            \n",
    "            batch_X_test_term, batch_X_test_hyper, batch_y_test_label =\\\n",
    "            extend_batch_with_negatives(batch_X_test_term, batch_X_test_hyper,\n",
    "                                        neg_strategy,\n",
    "                                        data.tokenizer, m\n",
    "                                       )            \n",
    "\n",
    "            # train on batch\n",
    "            loss += model.train_on_batch([batch_X_term, batch_X_hyper], \n",
    "                                          batch_y_label)[0]\n",
    "            \n",
    "            test_loss += model.test_on_batch([batch_X_test_term, batch_X_test_hyper], \n",
    "                                              batch_y_test_label)[0]                \n",
    "                \n",
    "            \n",
    "        print('Epoch:', epoch+1, 'Loss:', loss, 'Test Loss:', test_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Hyponym (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TermEmbedding (Embedding)       (None, 1, 200)       43904800    Hyponym[0][0]                    \n",
      "                                                                 Hypernym[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Hypo (Dropout)          (None, 1, 200)       0           TermEmbedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Hypernym (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Phi0 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           Phi0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Hyper (Dropout)         (None, 1, 200)       0           TermEmbedding[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Phi (Dropout)           (None, 200)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           Dropout_Hyper[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "DotProduct (Dot)                (None, 1)            0           Dropout_Phi[0][0]                \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Prediction (Dense)              (None, 1)            2           DotProduct[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 43,944,802\n",
      "Trainable params: 40,002\n",
      "Non-trainable params: 43,904,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "#rand_norm_m0_sd001 = RandomNormal(mean = 0.0, stddev=0.01, seed=42)\n",
    "#rand_norm = RandomNormal(mean = 0.0, stddev=1., seed=42)\n",
    "\n",
    "# negative sampling options\n",
    "neg_sampling_options = {'synonym':data.synonyms,                                                 \n",
    "                        'random':data.random_words\n",
    "                       }\n",
    "\n",
    "# phi random init options\n",
    "phi_init_options = {'random_plus_identity': random_plus_identity,\n",
    "                    'random_identity': random_identity, \n",
    "                    'random_normal': random_normal}\n",
    "\n",
    "kernel_constraints = {'None': None, 'ForceToOne': ForceToOne()}\n",
    "\n",
    "# positive batch size\n",
    "batch_size = 32\n",
    "\n",
    "# implement mini-batch stochastic training\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# number of negative samples\n",
    "m = 10\n",
    "# number of projections\n",
    "phi_k = 1\n",
    "# train (True) or freeze\n",
    "train_embeddings = False\n",
    "# negative sample strategy\n",
    "negative_option = 'random'\n",
    "# initialise phi strategy\n",
    "phi_init_option = 'random_identity'\n",
    "# constrain LR parameter\n",
    "kernel_constraint_option = 'None'\n",
    "# dropout rate\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# create model\n",
    "crim_model = get_CRIM_model(phi_k = phi_k, train_embeddings = train_embeddings,\n",
    "                            embeddings_dim = data.embeddings_dim, vocab_size = len(data.tokenizer.word_counts),\n",
    "                            embeddings_matrix = data.embedding_matrix,\n",
    "                            phi_init = phi_init_options[phi_init_option],                            \n",
    "                            sigmoid_kernel_regularisation = None, #l2(0.001),\n",
    "                            sigmoid_bias_regularisation = None, #l2(0.001),\n",
    "                            sigmoid_kernel_constraint = kernel_constraints[kernel_constraint_option],\n",
    "                            dropout_rate = dropout_rate,\n",
    "                            learning_rate = learning_rate\n",
    "                           )\n",
    "\n",
    "crim_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# plot model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(crim_model, to_file='CRIM_alternate_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None', 'Learning rate: ', 0.001)\n",
      "('Epoch:', 1, 'Loss:', 154.7653611600399, 'Test Loss:', 154.85199791193008)\n",
      "('Epoch:', 2, 'Loss:', 100.31694516539574, 'Test Loss:', 99.247761875391)\n",
      "('Epoch:', 3, 'Loss:', 87.97930534183979, 'Test Loss:', 87.04231545329094)\n",
      "('Epoch:', 4, 'Loss:', 78.69284763932228, 'Test Loss:', 76.97447828948498)\n",
      "('Epoch:', 5, 'Loss:', 72.10867649316788, 'Test Loss:', 69.92591404914856)\n",
      "('Epoch:', 6, 'Loss:', 67.23415367305279, 'Test Loss:', 64.80040496587753)\n",
      "('Epoch:', 7, 'Loss:', 63.75109326094389, 'Test Loss:', 61.24854889512062)\n",
      "('Epoch:', 8, 'Loss:', 59.589780896902084, 'Test Loss:', 58.83554546535015)\n",
      "('Epoch:', 9, 'Loss:', 56.71147071570158, 'Test Loss:', 56.90971930325031)\n",
      "('Epoch:', 10, 'Loss:', 54.37444880232215, 'Test Loss:', 55.833690986037254)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "predict = False\n",
    "print ('Epochs: ', epochs, 'Batch size: ', batch_size, 'm: ', m, 'pki_k: ', phi_k, 'train_embeddings: ', train_embeddings,\n",
    "      'Negative sampling: ', negative_option, 'Phi Init: ', phi_init_option, 'Dropout rate: ', dropout_rate, \n",
    "      'Kernel constraint: ', kernel_constraint_option, 'Learning rate: ', learning_rate)\n",
    "\n",
    "train(crim_model, epochs, batch_size, m, data, neg_sampling_options[negative_option])\n",
    "\n",
    "# evaluate\n",
    "if predict:\n",
    "    print (\"Generating predictions...\")\n",
    "    crim_predictions = predict_crim_hypernyms(data, crim_model, alt_get_hypernym)\n",
    "\n",
    "    print (\"CRIM evaluation:\")\n",
    "    score_names, all_scores = get_evaluation_scores((data.test_query, data.test_hyper), crim_predictions)\n",
    "    for k in range(len(score_names)):\n",
    "        print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epochs: ', 5, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_plus_identity', 'Dropout rate: ', 0.4, 'Kernel constraint: ', 'None')\n",
      "Generating predictions...\n",
      "('Done', 100)\n",
      "('Done', 200)\n",
      "('Done', 300)\n",
      "('Done', 400)\n",
      "('Done', 500)\n",
      "('Done', 600)\n",
      "('Done', 700)\n",
      "('Done', 800)\n",
      "('Done', 900)\n",
      "('Done', 1000)\n",
      "('Done', 1100)\n",
      "('Done', 1200)\n",
      "('Done', 1300)\n",
      "('Done', 1400)\n",
      "CRIM evaluation:\n",
      "MRR: 0.29684\n",
      "P@1: 0.24483\n",
      "P@5: 0.13789\n",
      "P@10: 0.13077\n"
     ]
    }
   ],
   "source": [
    "# run further training if required\n",
    "m = 10\n",
    "negative_option = 'random'\n",
    "epochs = 5\n",
    "learning_rate = 0.00025\n",
    "\n",
    "predict = True\n",
    "\n",
    "dense = map(lambda x: x.get_weights()[0], [l for l in crim_model.layers if l.name.startswith('Phi')])\n",
    "dense = np.asarray(dense)\n",
    "lr_weights = crim_model.get_layer(name='Prediction').get_weights()\n",
    "\n",
    "crim_model_2 = get_CRIM_model_freeze_phi(phi_k=phi_k, \n",
    "                                         train_phi=False, phi_weights=dense,\n",
    "                                         lr_weights=lr_weights,\n",
    "                                         embeddings_dim = data.embeddings_dim, vocab_size = len(data.tokenizer.word_counts),\n",
    "                                         embeddings_matrix = data.embedding_matrix,\n",
    "                                         phi_init = phi_init_options[phi_init_option],                            \n",
    "                                         sigmoid_kernel_regularisation = None,\n",
    "                                         sigmoid_bias_regularisation = None,\n",
    "                                         sigmoid_kernel_constraint = kernel_constraints[kernel_constraint_option],\n",
    "                                         dropout_rate = dropout_rate,\n",
    "                                         learning_rate = learning_rate\n",
    "                                        )\n",
    "                                         \n",
    "crim_model_2.summary()                                         \n",
    "\n",
    "print ('Epochs: ', epochs, 'Batch size: ', batch_size, 'm: ', m, 'pki_k: ', phi_k, 'train_embeddings: ', train_embeddings,\n",
    "      'Negative sampling: ', negative_option, 'Phi Init: ', phi_init_option, 'Dropout rate: ', dropout_rate, \n",
    "      'Kernel constraint: ', kernel_constraint_option, 'Learning rate: ', learning_rate)\n",
    "\n",
    "#train(crim_model_2, epochs, batch_size, m, data, neg_sampling_options[negative_option])\n",
    "if predict:\n",
    "    print (\"Generating predictions...\")\n",
    "    crim_predictions_2 = predict_crim_hypernyms(data, crim_model_2, alt_get_hypernym)\n",
    "\n",
    "    print (\"CRIM evaluation:\")\n",
    "    score_names, all_scores = get_evaluation_scores((data.test_query, data.test_hyper), crim_predictions_2)\n",
    "    for k in range(len(score_names)):\n",
    "        print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save models\n",
    "#crim_model.save_weights('models/crim_phi2_e25_drop35.h5')\n",
    "#crim_model_2.save_weights('models/crim_embeddings_e5_drop35.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-2.0710576]], dtype=float32), array([-1.8059509], dtype=float32)]\n",
      "(1, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "# print crim_model parameters\n",
    "\n",
    "print (crim_model.get_layer(name='Prediction').get_weights())\n",
    "dense = map(lambda x: x.get_weights()[0], [l for l in crim_model.layers if l.name.startswith('Phi')])\n",
    "dense = np.asarray(dense)\n",
    "print (dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.00509 0.23016633\n"
     ]
    }
   ],
   "source": [
    "# print phi mean and standard deviation\n",
    "\n",
    "for i in range(dense.shape[0]):\n",
    "    print i, np.round(np.mean(dense[i]),5), np.std(dense[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation  code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-eb51bf235e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample_terms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malt_get_hypernym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dense' is not defined"
     ]
    }
   ],
   "source": [
    "# generate a few example hypernyms\n",
    "#example_terms = ['suzy_favor_hamilton', 'wicketkeeper','aquamarine','tenpence','vegetarian','blackfly']\n",
    "example_terms = ['cat']\n",
    "\n",
    "for t in example_terms:\n",
    "    print (t)\n",
    "    for h in alt_get_hypernym(t, cluster_list[0].model, data, dense, 15):\n",
    "        print h\n",
    "    print (\"-\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36369914]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether two words are related by hypernymy\n",
    "i = data.tokenizer.word_index['ebert']\n",
    "j = data.tokenizer.word_index['tennis_player']\n",
    "crim_model.predict([[i], [j]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidate hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crim_get_top_hypernyms('robert_de_niro', candidates, crim_model, data, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some readings\n",
    "\n",
    "* ('Epochs: ', 30, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout 0.5: ', True)\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.13002<br>\n",
    "P@1: 0.08806<br>\n",
    "P@5: 0.06368<br>\n",
    "P@10: 0.06131<br>\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'None')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.16015<br>\n",
    "P@1: 0.12675<br>\n",
    "P@5: 0.07046<br>\n",
    "P@10: 0.06632<br>\n",
    "\n",
    "* ('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5: ')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.15522<br>\n",
    "P@1: 0.11741<br>\n",
    "P@5: 0.07256<br>\n",
    "P@10: 0.06844<br>\n",
    "\n",
    "* ('Epochs: ', 5, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', True, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout (0.5): ', True)\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.22385<br>\n",
    "P@1: 0.1968<br>\n",
    "P@5: 0.08815<br>\n",
    "P@10: 0.08352<br>\n",
    "\n",
    "Even though these results are superior on the outset, in reality, tuning the embeddings reduces the model to Most Frequent Hypernym.\n",
    "\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout: ', True)\n",
    "\n",
    "Reducing the number of random samples has a negative impact.\n",
    "\n",
    "    CRIM evaluation:<br>\n",
    "MRR: 0.06439<br>\n",
    "P@1: 0.04536<br>\n",
    "P@5: 0.02777<br>\n",
    "P@10: 0.02703<br>\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'ForceToOne')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.12434<br>\n",
    "P@1: 0.09006<br>\n",
    "P@5: 0.05667<br>\n",
    "P@10: 0.0549<br>\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 5, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'None')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.1346<br>\n",
    "P@1: 0.0974<br>\n",
    "P@5: 0.0627<br>\n",
    "P@10: 0.05998<br>\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 10, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'None')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.12567<br>\n",
    "P@1: 0.08539<br>\n",
    "P@5: 0.06105<br>\n",
    "P@10: 0.05916<br>\n",
    "\n",
    "\n",
    "* ('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 12, 'train_embeddings: ', False, 'Negative sampling: ', 'synonym', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 155.5154161900282, 'Test Loss:', 485.83162450790405)\n",
    "('Epoch:', 2, 'Loss:', 88.82768769562244, 'Test Loss:', 348.13482135534286)\n",
    "('Epoch:', 3, 'Loss:', 66.63960940390825, 'Test Loss:', 258.7310974597931)\n",
    "('Epoch:', 4, 'Loss:', 56.400326274335384, 'Test Loss:', 216.1373891234398)\n",
    "('Epoch:', 5, 'Loss:', 51.70219925045967, 'Test Loss:', 207.98559176921844)\n",
    "('Epoch:', 6, 'Loss:', 48.9724283516407, 'Test Loss:', 204.41960680484772)\n",
    "('Epoch:', 7, 'Loss:', 46.962790466845036, 'Test Loss:', 195.1675413697958)\n",
    "('Epoch:', 8, 'Loss:', 45.41264865081757, 'Test Loss:', 198.74169850349426)\n",
    "('Epoch:', 9, 'Loss:', 45.02060864120722, 'Test Loss:', 191.37398713827133)\n",
    "('Epoch:', 10, 'Loss:', 44.76244197413325, 'Test Loss:', 188.13010711967945)\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.01452<br>\n",
    "P@1: 0.00801<br>\n",
    "P@5: 0.00653<br>\n",
    "P@10: 0.00623<br>\n",
    "\n",
    "* ('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 15, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.5, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 173.66530799865723, 'Test Loss:', 422.2526289820671)\n",
    "('Epoch:', 2, 'Loss:', 89.43182794749737, 'Test Loss:', 888.1642265319824)\n",
    "('Epoch:', 3, 'Loss:', 82.11764796078205, 'Test Loss:', 897.4773137569427)\n",
    "('Epoch:', 4, 'Loss:', 77.61969149112701, 'Test Loss:', 871.3285944461823)\n",
    "('Epoch:', 5, 'Loss:', 73.61261868476868, 'Test Loss:', 838.6195100545883)\n",
    "('Epoch:', 6, 'Loss:', 70.76602215319872, 'Test Loss:', 790.632804274559)\n",
    "('Epoch:', 7, 'Loss:', 68.09195621311665, 'Test Loss:', 758.9123626947403)\n",
    "('Epoch:', 8, 'Loss:', 66.28909918665886, 'Test Loss:', 722.3105019330978)\n",
    "('Epoch:', 9, 'Loss:', 64.0174068659544, 'Test Loss:', 701.5663638114929)\n",
    "('Epoch:', 10, 'Loss:', 62.1955054551363, 'Test Loss:', 685.4047366380692)\n",
    "('Epoch:', 11, 'Loss:', 61.18628938496113, 'Test Loss:', 666.7885119915009)\n",
    "('Epoch:', 12, 'Loss:', 60.10102154314518, 'Test Loss:', 654.0900322198868)\n",
    "('Epoch:', 13, 'Loss:', 59.095360577106476, 'Test Loss:', 653.2110993862152)\n",
    "('Epoch:', 14, 'Loss:', 58.58073855936527, 'Test Loss:', 634.9157860279083)\n",
    "('Epoch:', 15, 'Loss:', 57.885950952768326, 'Test Loss:', 631.2514699697495)\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.15946\n",
    "P@1: 0.11674\n",
    "P@5: 0.07472\n",
    "P@10: 0.06976\n",
    "\n",
    "-----------------------------------------------\n",
    "Tried new technique involving mix of ensemble and transfer learning.<br>\n",
    "* Developed two models: one is the standard CRIM model I've always used.  The embeddings are frozen and populated with the word2vec vectors provided by Gabriel.\n",
    "* After training this model for an adequate number of epochs (i.e. until optimal fitting; early stopping implemented manually);\n",
    "* After the first cycle of training is over I extracted the phi and LR weights;\n",
    "* These weights were subsequently injected in a new model.  This time the embeddings later was set to trainable and the Phi dense layer was frozen;\n",
    "* The model was encouraged to use the same projection weights and fine-tune the embeddings to learn a better hypernym generation model;\n",
    "\n",
    "#### 1 Projection; First Cycle \n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 177.9488000869751, 'Test Loss:', 178.53641974925995)\n",
    "('Epoch:', 2, 'Loss:', 103.15769128501415, 'Test Loss:', 102.5895478874445)\n",
    "('Epoch:', 3, 'Loss:', 90.33858793973923, 'Test Loss:', 90.0393942296505)\n",
    "('Epoch:', 4, 'Loss:', 80.81938941776752, 'Test Loss:', 79.73569859564304)\n",
    "('Epoch:', 5, 'Loss:', 74.30441601574421, 'Test Loss:', 73.00559163093567)\n",
    "('Epoch:', 6, 'Loss:', 68.31351159512997, 'Test Loss:', 67.29328979551792)\n",
    "('Epoch:', 7, 'Loss:', 64.69703111797571, 'Test Loss:', 64.18054696917534)\n",
    "('Epoch:', 8, 'Loss:', 61.36914176493883, 'Test Loss:', 60.54891411960125)\n",
    "('Epoch:', 9, 'Loss:', 57.571076557040215, 'Test Loss:', 58.882518880069256)\n",
    "('Epoch:', 10, 'Loss:', 54.78656556457281, 'Test Loss:', 57.673407919704914)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.14914\n",
    "P@1: 0.10874\n",
    "P@5: 0.0687\n",
    "P@10: 0.06505\n",
    "\n",
    "\n",
    "#### 1 Projection; Second Cycle\n",
    "('Epochs: ', 5, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', True, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 29.900435734540224, 'Test Loss:', 50.436300061643124)\n",
    "('Epoch:', 2, 'Loss:', 8.763934534537839, 'Test Loss:', 46.93847519904375)\n",
    "('Epoch:', 3, 'Loss:', 4.247505006627762, 'Test Loss:', 51.40926795452833)\n",
    "('Epoch:', 4, 'Loss:', 2.347970368108463, 'Test Loss:', 56.144402757287025)\n",
    "('Epoch:', 5, 'Loss:', 1.38899088197104, 'Test Loss:', 63.76580411195755)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.28634\n",
    "P@1: 0.24483\n",
    "P@5: 0.1263\n",
    "P@10: 0.11961\n",
    "\n",
    "---------------------------------------------------------------------------------------\n",
    "#### 24 Projections; First Cycle\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 109.97628237307072, 'Test Loss:', 110.91941311955452)\n",
    "('Epoch:', 2, 'Loss:', 61.31632810086012, 'Test Loss:', 65.23697778582573)\n",
    "('Epoch:', 3, 'Loss:', 48.39542027562857, 'Test Loss:', 59.35354737192392)\n",
    "('Epoch:', 4, 'Loss:', 41.11544480547309, 'Test Loss:', 60.305753372609615)\n",
    "('Epoch:', 5, 'Loss:', 36.67992676049471, 'Test Loss:', 64.0475360751152)\n",
    "('Epoch:', 6, 'Loss:', 34.28844119235873, 'Test Loss:', 63.37563705444336)\n",
    "('Epoch:', 7, 'Loss:', 32.068154136650264, 'Test Loss:', 64.99528855085373)\n",
    "('Epoch:', 8, 'Loss:', 31.266997564584017, 'Test Loss:', 64.47643724828959)\n",
    "('Epoch:', 9, 'Loss:', 30.232711946591735, 'Test Loss:', 63.88381798565388)\n",
    "('Epoch:', 10, 'Loss:', 29.51552465558052, 'Test Loss:', 65.2800731509924)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.10304\n",
    "P@1: 0.07138\n",
    "P@5: 0.04817\n",
    "P@10: 0.04453\n",
    "\n",
    "#### 24 Projections; Second Cycle\n",
    "('Epochs: ', 5, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 24, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "\n",
    "('Epoch:', 1, 'Loss:', 13.566975106863538, 'Test Loss:', 59.5253451615572)\n",
    "('Epoch:', 2, 'Loss:', 2.032865798302737, 'Test Loss:', 60.16112444549799)\n",
    "('Epoch:', 3, 'Loss:', 0.46449146703412225, 'Test Loss:', 64.23307839781046)\n",
    "('Epoch:', 4, 'Loss:', 0.1069298386116202, 'Test Loss:', 68.53682653605938)\n",
    "('Epoch:', 5, 'Loss:', 0.027284826716368116, 'Test Loss:', 73.16503396630287)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.30386\n",
    "P@1: 0.24817\n",
    "P@5: 0.14445\n",
    "P@10: 0.13732\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "### Experiment with single projection but changing: i) Random initialiser (random + identity); ii) 2nd phase keeps trainining Phi; iii) Change Learning Rate of 2nd phase.\n",
    "\n",
    "#### 1 Projections; First Cycle\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 176.2122738659382, 'Test Loss:', 177.55351921916008)\n",
    "('Epoch:', 2, 'Loss:', 104.77461278438568, 'Test Loss:', 105.96388858556747)\n",
    "('Epoch:', 3, 'Loss:', 93.10278041660786, 'Test Loss:', 95.61527897417545)\n",
    "('Epoch:', 4, 'Loss:', 83.65695556998253, 'Test Loss:', 86.35148683190346)\n",
    "('Epoch:', 5, 'Loss:', 76.63541767001152, 'Test Loss:', 80.32908068597317)\n",
    "('Epoch:', 6, 'Loss:', 71.39560843259096, 'Test Loss:', 74.56421269476414)\n",
    "('Epoch:', 7, 'Loss:', 67.28997546434402, 'Test Loss:', 70.91447427868843)\n",
    "('Epoch:', 8, 'Loss:', 63.82748632133007, 'Test Loss:', 67.26722575724125)\n",
    "('Epoch:', 9, 'Loss:', 60.12399164587259, 'Test Loss:', 65.32279951870441)\n",
    "('Epoch:', 10, 'Loss:', 57.98733665794134, 'Test Loss:', 63.860010385513306)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.15094\n",
    "P@1: 0.11474\n",
    "P@5: 0.0674\n",
    "P@10: 0.065\n",
    "\n",
    "#### 24 Projections; Second Cycle\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.3, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 37.44226049259305, 'Test Loss:', 62.1908989623189)\n",
    "('Epoch:', 2, 'Loss:', 14.947778060100973, 'Test Loss:', 74.91842666268349)\n",
    "('Epoch:', 3, 'Loss:', 7.418942770687863, 'Test Loss:', 98.69380746781826)\n",
    "('Epoch:', 4, 'Loss:', 4.618407022906467, 'Test Loss:', 120.58737960457802)\n",
    "('Epoch:', 5, 'Loss:', 2.871887466167209, 'Test Loss:', 142.8387492597103)\n",
    "('Epoch:', 6, 'Loss:', 1.9543396250010119, 'Test Loss:', 163.086307823658)\n",
    "('Epoch:', 7, 'Loss:', 1.3212263471978076, 'Test Loss:', 183.57101076841354)\n",
    "('Epoch:', 8, 'Loss:', 0.9992299735413326, 'Test Loss:', 209.59179404377937)\n",
    "('Epoch:', 9, 'Loss:', 0.8104324492987871, 'Test Loss:', 228.58731454610825)\n",
    "('Epoch:', 10, 'Loss:', 0.5762173827174593, 'Test Loss:', 254.7513089776039)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.27074\n",
    "P@1: 0.22148\n",
    "P@5: 0.13155\n",
    "P@10: 0.12557\n",
    "\n",
    "\n",
    "* Overfit manifests for sure.  Person, work-of-art feature heavily as hypernyms even for query terms which are unrelated\n",
    "\n",
    "### Experiment with dual projections.  Extended epochs of first cycle to 25.  Reduced learning rate of second cycle to reduce overfit and training for 5 epochs\n",
    "\n",
    "#### First Cycle\n",
    "('Epochs: ', 25, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 2, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.35, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 160.74599489569664, 'Test Loss:', 160.929179251194)\n",
    "('Epoch:', 2, 'Loss:', 96.68699912726879, 'Test Loss:', 96.50307583808899)\n",
    "('Epoch:', 3, 'Loss:', 82.84739479422569, 'Test Loss:', 82.26891721785069)\n",
    "('Epoch:', 4, 'Loss:', 73.6079184487462, 'Test Loss:', 72.73883473873138)\n",
    "('Epoch:', 5, 'Loss:', 67.65975984185934, 'Test Loss:', 67.05146090686321)\n",
    "('Epoch:', 6, 'Loss:', 63.158750265836716, 'Test Loss:', 62.18984428048134)\n",
    "('Epoch:', 7, 'Loss:', 59.12502943724394, 'Test Loss:', 59.458139173686504)\n",
    "('Epoch:', 8, 'Loss:', 55.11561170220375, 'Test Loss:', 56.17893383651972)\n",
    "('Epoch:', 9, 'Loss:', 52.53363721072674, 'Test Loss:', 54.554423585534096)\n",
    "('Epoch:', 10, 'Loss:', 50.518984742462635, 'Test Loss:', 53.33502481132746)\n",
    "('Epoch:', 11, 'Loss:', 48.320613726973534, 'Test Loss:', 51.69379674643278)\n",
    "('Epoch:', 12, 'Loss:', 46.352201879024506, 'Test Loss:', 51.4660424888134)\n",
    "('Epoch:', 13, 'Loss:', 45.39920901507139, 'Test Loss:', 51.60422394424677)\n",
    "('Epoch:', 14, 'Loss:', 44.41912394762039, 'Test Loss:', 50.1107277572155)\n",
    "('Epoch:', 15, 'Loss:', 43.92356888204813, 'Test Loss:', 50.16625649482012)\n",
    "('Epoch:', 16, 'Loss:', 42.618239261209965, 'Test Loss:', 49.68374668061733)\n",
    "('Epoch:', 17, 'Loss:', 41.93088800087571, 'Test Loss:', 49.52678156644106)\n",
    "('Epoch:', 18, 'Loss:', 42.28301604837179, 'Test Loss:', 49.41834541410208)\n",
    "('Epoch:', 19, 'Loss:', 41.13799152523279, 'Test Loss:', 48.8290878534317)\n",
    "('Epoch:', 20, 'Loss:', 40.83357220888138, 'Test Loss:', 48.22099205851555)\n",
    "('Epoch:', 21, 'Loss:', 40.20593152567744, 'Test Loss:', 48.0087883323431)\n",
    "('Epoch:', 22, 'Loss:', 40.49513000249863, 'Test Loss:', 47.994462229311466)\n",
    "('Epoch:', 23, 'Loss:', 39.6955735757947, 'Test Loss:', 48.454381965100765)\n",
    "('Epoch:', 24, 'Loss:', 39.38141195476055, 'Test Loss:', 48.52395910024643)\n",
    "('Epoch:', 25, 'Loss:', 39.834498304873705, 'Test Loss:', 48.30932606011629)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.11931\n",
    "P@1: 0.07872\n",
    "P@5: 0.05491\n",
    "P@10: 0.05259\n",
    "\n",
    "#### Second cycle\n",
    "('Epochs: ', 5, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 2, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.35, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 30.962760228663683, 'Test Loss:', 46.86121924221516)\n",
    "('Epoch:', 2, 'Loss:', 18.49303602334112, 'Test Loss:', 43.47840115427971)\n",
    "('Epoch:', 3, 'Loss:', 12.296762353973463, 'Test Loss:', 41.97538521140814)\n",
    "('Epoch:', 4, 'Loss:', 8.417490374995396, 'Test Loss:', 40.455049715936184)\n",
    "('Epoch:', 5, 'Loss:', 6.027016263862606, 'Test Loss:', 40.4208921790123)\n",
    "Generating predictions...\n",
    "('Done', 100)\n",
    "('Done', 200)\n",
    "('Done', 300)\n",
    "('Done', 400)\n",
    "('Done', 500)\n",
    "('Done', 600)\n",
    "('Done', 700)\n",
    "('Done', 800)\n",
    "('Done', 900)\n",
    "('Done', 1000)\n",
    "('Done', 1100)\n",
    "('Done', 1200)\n",
    "('Done', 1300)\n",
    "('Done', 1400)\n",
    "CRIM evaluation:\n",
    "MRR: 0.30662\n",
    "P@1: 0.2515\n",
    "P@5: 0.14699\n",
    "P@10: 0.13961\n",
    "\n",
    "-------------------------------------------------------------------------------------------------\n",
    "### Experiment with two Phi layers. \n",
    "#### I insert dropout after the hyponym and hypernym embeddings and after the second phi.  Training goes on for 25 epochs.  The solution converges much more slowly than if were using a single affine layer.\n",
    "\n",
    "('Epochs: ', 25, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'Dropout rate: ', 0.35, 'Kernel constraint: ', 'None')\n",
    "('Epoch:', 1, 'Loss:', 132.44914108514786, 'Test Loss:', 129.82295709848404)\n",
    "('Epoch:', 2, 'Loss:', 95.7211739718914, 'Test Loss:', 92.90788823366165)\n",
    "('Epoch:', 3, 'Loss:', 93.52208907902241, 'Test Loss:', 90.88466887176037)\n",
    "('Epoch:', 4, 'Loss:', 91.18207442760468, 'Test Loss:', 89.7815543860197)\n",
    "('Epoch:', 5, 'Loss:', 88.62042617797852, 'Test Loss:', 87.20721289515495)\n",
    "('Epoch:', 6, 'Loss:', 85.08720774948597, 'Test Loss:', 83.62825165688992)\n",
    "('Epoch:', 7, 'Loss:', 82.6170591711998, 'Test Loss:', 79.72680546343327)\n",
    "('Epoch:', 8, 'Loss:', 78.23334312438965, 'Test Loss:', 74.8099833726883)\n",
    "('Epoch:', 9, 'Loss:', 76.347172498703, 'Test Loss:', 72.44513177871704)\n",
    "('Epoch:', 10, 'Loss:', 74.18829296529293, 'Test Loss:', 69.96501626074314)\n",
    "('Epoch:', 11, 'Loss:', 72.2932140827179, 'Test Loss:', 67.46124893426895)\n",
    "('Epoch:', 12, 'Loss:', 71.2011769413948, 'Test Loss:', 65.32345585525036)\n",
    "('Epoch:', 13, 'Loss:', 70.07241632044315, 'Test Loss:', 63.981889829039574)\n",
    "('Epoch:', 14, 'Loss:', 68.04309992492199, 'Test Loss:', 62.387570425868034)\n",
    "('Epoch:', 15, 'Loss:', 67.2650830000639, 'Test Loss:', 61.06334821879864)\n",
    "('Epoch:', 16, 'Loss:', 67.25092969834805, 'Test Loss:', 60.93150553107262)\n",
    "('Epoch:', 17, 'Loss:', 66.12663190811872, 'Test Loss:', 60.26040391623974)\n",
    "('Epoch:', 18, 'Loss:', 64.85526475310326, 'Test Loss:', 59.32122567296028)\n",
    "('Epoch:', 19, 'Loss:', 64.72251350432634, 'Test Loss:', 58.52266174554825)\n",
    "('Epoch:', 20, 'Loss:', 63.96191988885403, 'Test Loss:', 57.419067934155464)\n",
    "('Epoch:', 21, 'Loss:', 62.82784986868501, 'Test Loss:', 56.976065531373024)\n",
    "('Epoch:', 22, 'Loss:', 62.8604651093483, 'Test Loss:', 57.06210967898369)\n",
    "('Epoch:', 23, 'Loss:', 62.14057156443596, 'Test Loss:', 56.878740444779396)\n",
    "('Epoch:', 24, 'Loss:', 61.323521822690964, 'Test Loss:', 56.8377401381731)\n",
    "('Epoch:', 25, 'Loss:', 61.14446556568146, 'Test Loss:', 56.42692677676678)\n",
    "\n",
    "* Having one than one linear layer does not increase the hypothesis space of the model.\n",
    "* Attempting model with 2 hidden layer each with non-linear activation functions stopped impeded the model from learning anything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hard Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_model(dim, embedding_matrix):\n",
    "    hypo_input = Input(shape=(1,))\n",
    "    hyper_input = Input(shape=(1,))\n",
    "\n",
    "    word_embedding = Embedding(embedding_matrix.shape[0], dim, name='WE')\n",
    "\n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    embedding_model = Model(inputs=[hypo_input, hyper_input], outputs=[hypo_embedding, hyper_embedding])\n",
    "\n",
    "    # inject pre-trained embeddings into this mini, resusable model/layer\n",
    "    embedding_model.get_layer(name='WE').set_weights([embedding_matrix])\n",
    "    embedding_model.get_layer(name='WE').trainable = False\n",
    "    \n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_CRIM_model(phi_k=1,\n",
    "                           embeddings_layer=None,\n",
    "                           embeddings_dim = 200,\n",
    "                           phi_init = None,\n",
    "                           phi_activity_regularisation = None,\n",
    "                           sigmoid_kernel_regularisation = None,\n",
    "                           sigmoid_bias_regularisation = None,\n",
    "                           sigmoid_kernel_constraint = None,\n",
    "                           dropout_rate = 0.,\n",
    "                           learning_rate = 0.001\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "        \n",
    "    hypo_embedding, hyper_embedding = embeddings_layer([hypo_input, hyper_input])\n",
    "            \n",
    "    # Add Dropout to avoid overfit    \n",
    "    hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "    hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init,                               \n",
    "                               name='Phi%d' % (i)) (hypo_embedding))            \n",
    "\n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "    \n",
    "    phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "    \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer='random_normal',\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer='random_normal',                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       ) (phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)        \n",
    "\n",
    "    adam = Adam(lr = learning_rate, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# light-weight data class containing only \n",
    "class ClusterHybrid:\n",
    "            \n",
    "    def __init__(self, cluster, cluster_predictions,\n",
    "                 orig_data,                 \n",
    "                 phi_init, sigmoid_kernel_constraint,\n",
    "                 embeddings_layer, dropout_rate, learning_rate\n",
    "                 ):\n",
    "        \n",
    "        # create model\n",
    "        self.model = self._init_model(phi_init=phi_init,                                       \n",
    "                                      embeddings_layer = embedding_layer, \n",
    "                                      embeddings_dim = orig_data.embeddings_dim,\n",
    "                                      sigmoid_kernel_constraint = sigmoid_kernel_constraint,\n",
    "                                      dropout_rate = dropout_rate, learning_rate = learning_rate)\n",
    "        \n",
    "        self.cluster_id = cluster\n",
    "        self.tokenizer = orig_data.tokenizer\n",
    "        \n",
    "        self.train_query = map(lambda x: orig_data.train_query[x], np.where(cluster_predictions == cluster)[0])\n",
    "        self.train_hyper = map(lambda x: orig_data.train_hyper[x], np.where(cluster_predictions == cluster)[0])\n",
    "                                \n",
    "        self.valid_query = orig_data.valid_query\n",
    "        self.valid_hyper = orig_data.valid_hyper\n",
    "        \n",
    "        self.train_query_seq, self.train_hyper_seq, self.valid_query_seq, self.valid_hyper_seq =\\\n",
    "        map(lambda x: orig_data.tokenizer.texts_to_sequences(x),\\\n",
    "            [self.train_query, self.train_hyper, self.valid_query, self.valid_hyper])\n",
    "        \n",
    "        self.train_query_seq, self.train_hyper_seq, self.valid_query_seq, self.valid_hyper_seq =\\\n",
    "        map(lambda x: np.asarray(x, dtype='int32'),\\\n",
    "            [self.train_query_seq, self.train_hyper_seq, self.valid_query_seq, self.valid_hyper_seq])\n",
    "        \n",
    "\n",
    "        self.loss = 0.\n",
    "        self.test_loss = 0\n",
    "    \n",
    "    def _init_model(self, phi_init, sigmoid_kernel_constraint, \n",
    "                    embeddings_layer, embeddings_dim,\n",
    "                    dropout_rate, learning_rate):\n",
    "        \n",
    "        return get_cluster_CRIM_model(phi_init=phi_init, \n",
    "                                      embeddings_layer = embeddings_layer, embeddings_dim = embeddings_dim,\n",
    "                                      sigmoid_kernel_constraint = sigmoid_kernel_constraint,\n",
    "                                      dropout_rate = dropout_rate, learning_rate = learning_rate)\n",
    "    \n",
    "    def update_loss(self, new_loss):\n",
    "        self.loss += new_loss\n",
    "        \n",
    "    def update_test_loss(self, new_loss):\n",
    "        self.test_loss += new_loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Clustering Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate offsets \n",
    "training_query_vector = np.zeros((len(data.train_query), data.embeddings_dim))\n",
    "training_hyper_vector = np.zeros((len(data.train_hyper), data.embeddings_dim))\n",
    "for i in range(len(data.train_query)):\n",
    "    query_embedding_lookup = data.tokenizer.word_index[data.train_query[i]]\n",
    "    hyper_embedding_lookup = data.tokenizer.word_index[data.train_hyper[i]]\n",
    "    \n",
    "    training_query_vector[i] = data.embedding_matrix[query_embedding_lookup]\n",
    "    training_hyper_vector[i] = data.embedding_matrix[hyper_embedding_lookup]\n",
    "    \n",
    "\n",
    "train_offsets = training_hyper_vector - training_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3, ..., 4, 3, 0], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters = cluster_k, n_jobs=-1, random_state=42)\n",
    "#km.fit_predict(train_offsets)\n",
    "km.fit_predict(train_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 310, 1: 2762, 2: 2665, 3: 2318, 4: 3724})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(km.predict(train_offsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'turonian', u'geologic_timescale'),\n",
       " (u'turonian', u'geological_period'),\n",
       " (u'abhorrence', u'distaste'),\n",
       " (u'abhorrence', u'disgust'),\n",
       " (u'tropical_storm', u'windstorm'),\n",
       " (u'tropical_storm', u'violent_storm'),\n",
       " (u'tropical_storm', u'storm_damage'),\n",
       " (u'tropical_storm', u'storm'),\n",
       " (u'tropical_storm', u'tempest'),\n",
       " (u'tropical_storm', u'wind'),\n",
       " (u'pollution', u'dirtiness'),\n",
       " (u'pollution', u'dirtying'),\n",
       " (u'pollution', u'sanitary_condition'),\n",
       " (u'pollution', u'uncleanness'),\n",
       " (u'photomicrograph', u'picture'),\n",
       " (u'photomicrograph', u'photograph'),\n",
       " (u'photomicrograph', u'photo'),\n",
       " (u'swamp_gum', u'eucalyptus_tree'),\n",
       " (u'swamp_gum', u'gum_tree'),\n",
       " (u'wing', u'emblem'),\n",
       " (u'wing', u'air_unit'),\n",
       " (u'cumulus', u'natural_phenomenon'),\n",
       " (u'cumulus', u'phenomenon'),\n",
       " (u'cumulus', u'atmospheric_phenomenon'),\n",
       " (u'silver', u'gray'),\n",
       " (u'silver', u'badge_of_honour'),\n",
       " (u'silver', u'riches'),\n",
       " (u'silver', u'hoarded_wealth'),\n",
       " (u'silver', u'treasure'),\n",
       " (u'silver', u'cutlery'),\n",
       " (u'sand_verbena', u'flower'),\n",
       " (u'sand_verbena', u'wild_flower'),\n",
       " (u'louse', u'unpleasant_person'),\n",
       " (u'sodium_nitrite', u'bond'),\n",
       " (u'navigator', u'military_branch'),\n",
       " (u'navigator', u'explorer'),\n",
       " (u'navigator', u'military_machine'),\n",
       " (u'navigator', u'travel'),\n",
       " (u'navigator', u'adventurer'),\n",
       " (u'navigator', u'seaman'),\n",
       " (u'navigator', u'crewmember'),\n",
       " (u'navigator', u'crew_member'),\n",
       " (u'navigator', u'military_force'),\n",
       " (u'navigator', u'military_man'),\n",
       " (u'navigator', u'military'),\n",
       " (u'navigator', u'combatant'),\n",
       " (u'navigator', u'military_unit'),\n",
       " (u'navigator', u'enlisted_person'),\n",
       " (u'gasworks', u'manufactory'),\n",
       " (u'failure_rate', u'rate'),\n",
       " (u'kim_il-sung_university', u'ceremony'),\n",
       " (u'kim_il-sung_university', u'ceremonial_occasion'),\n",
       " (u'chess', u'party_game'),\n",
       " (u'chess', u'parlor_game'),\n",
       " (u'chess', u'board_game'),\n",
       " (u'chess', u'parlour_game'),\n",
       " (u'sachs_harbour', u'hamlet'),\n",
       " (u'myrdal', u'economist'),\n",
       " (u'myrdal', u'politician'),\n",
       " (u'homophone', u'homonym'),\n",
       " (u'availability', u'convenience'),\n",
       " (u'deck', u'upper_deck'),\n",
       " (u'jet_stream', u'phenomenon'),\n",
       " (u'jet_stream', u'air_current'),\n",
       " (u'jet_stream', u'weather'),\n",
       " (u'jet_stream', u'current_of_air'),\n",
       " (u'jet_stream', u'weather_condition'),\n",
       " (u'jet_stream', u'airstream'),\n",
       " (u'jet_stream', u'wind'),\n",
       " (u'habit', u'affair'),\n",
       " (u'habit', u'suit_of_clothes'),\n",
       " (u'bennington', u'town'),\n",
       " (u'bennington', u'city'),\n",
       " (u'bennington', u'township'),\n",
       " (u'subject', u'medical_research'),\n",
       " (u'subject', u'philosophy'),\n",
       " (u'subject', u'know-how'),\n",
       " (u'subject', u'political_doctrine'),\n",
       " (u'planetary_science', u'schoolbook'),\n",
       " (u'planetary_science', u'macrocosm_and_microcosm'),\n",
       " (u'planetary_science', u'textbook'),\n",
       " (u'planetary_science', u'book'),\n",
       " (u'planetary_science', u'branch_of_science'),\n",
       " (u'planetary_science', u'astrophysics'),\n",
       " (u'planetary_science', u'cosmos'),\n",
       " (u'planetary_science', u'physics'),\n",
       " (u'helen_of_troy', u'mythical_being'),\n",
       " (u'disservice', u'wrongdoing'),\n",
       " (u'carbon', u'coated_paper'),\n",
       " (u'carbon', u'non-metal'),\n",
       " (u'carbon', u'molecular_entity'),\n",
       " (u'carbon', u'chemical_group'),\n",
       " (u'carbon', u'main_group'),\n",
       " (u'carbon', u'paper'),\n",
       " (u'carbon', u'nonmetal'),\n",
       " (u'carbon', u'copy'),\n",
       " (u'owner-occupier', u'resident'),\n",
       " (u'owner-occupier', u'occupier'),\n",
       " (u'owner-occupier', u'occupant'),\n",
       " (u'fault', u'opening'),\n",
       " (u'fault', u'crack'),\n",
       " (u'north_carolina', u'province'),\n",
       " (u'north_carolina', u'city'),\n",
       " (u'north_carolina', u'state'),\n",
       " (u'contribution', u'giving'),\n",
       " (u'contribution', u'effort'),\n",
       " (u'contribution', u'attempt'),\n",
       " (u'forest_way', u'piece_of_ground'),\n",
       " (u'starling', u'bird'),\n",
       " (u'spinal_fusion', u'health_professional'),\n",
       " (u'spinal_fusion', u'physician'),\n",
       " (u'spinal_fusion', u'surgical_procedure'),\n",
       " (u'spinal_fusion', u'health_care_provider'),\n",
       " (u'spinal_fusion', u'medical_aid'),\n",
       " (u'spinal_fusion', u'medicine'),\n",
       " (u'spinal_fusion', u'surgery'),\n",
       " (u'spinal_fusion', u'orthopedic_surgery'),\n",
       " (u'spinal_fusion', u'md'),\n",
       " (u'spinal_fusion', u'orthopaedics'),\n",
       " (u'spinal_fusion', u'orthopedist'),\n",
       " (u'spinal_fusion', u'doctor'),\n",
       " (u'spinal_fusion', u'medical_doctor'),\n",
       " (u'spinal_fusion', u'surgical_operation'),\n",
       " (u'spinal_fusion', u'medical_procedure'),\n",
       " (u'spinal_fusion', u'orthopaedist'),\n",
       " (u'spinal_fusion', u'orthopedics'),\n",
       " (u'spinal_fusion', u'dr.'),\n",
       " (u'spinal_fusion', u'medical_care'),\n",
       " (u'spinal_fusion', u'practice_of_medicine'),\n",
       " (u'spinal_fusion', u'surgical_process'),\n",
       " (u'spinal_fusion', u'medical_specialist'),\n",
       " (u'trust', u'private_corporation'),\n",
       " (u'trust', u'syndicate'),\n",
       " (u'trust', u'brotherhood'),\n",
       " (u'self-confidence', u'certainty'),\n",
       " (u'backwash', u'movement'),\n",
       " (u'younger_dryas', u'geological_period'),\n",
       " (u'younger_dryas', u'geological_time'),\n",
       " (u'younger_dryas', u'geologic_time'),\n",
       " (u'younger_dryas', u'era'),\n",
       " (u'harvesting', u'collecting'),\n",
       " (u'harvesting', u'assembling'),\n",
       " (u'harvesting', u'gather'),\n",
       " (u'hosiery', u'clothing'),\n",
       " (u'hosiery', u'clothes'),\n",
       " (u'ph.d.', u'bookman'),\n",
       " (u'ph.d.', u'scholar'),\n",
       " (u'ph.d.', u'scholarly_person'),\n",
       " (u'ph.d.', u'doctor'),\n",
       " (u'ph.d.', u'affix'),\n",
       " (u'ph.d.', u'dr.'),\n",
       " (u'ph.d.', u'honorific'),\n",
       " (u'ph.d.', u'intellect'),\n",
       " (u'new_zealand', u'civil_authority'),\n",
       " (u'new_zealand', u'city'),\n",
       " (u'new_zealand', u'island_country'),\n",
       " (u'new_zealand', u'estate'),\n",
       " (u'new_zealand', u'state'),\n",
       " (u'new_zealand', u'landed_property'),\n",
       " (u'new_zealand', u'island'),\n",
       " (u'new_zealand', u'boundary_line'),\n",
       " (u'new_zealand', u'island_nation'),\n",
       " (u'new_zealand', u'public_building'),\n",
       " (u'new_zealand', u'dependent_territory'),\n",
       " (u'new_zealand', u'border_checkpoint'),\n",
       " (u'new_zealand', u'real_estate'),\n",
       " (u'new_zealand', u'acres'),\n",
       " (u'new_zealand', u'border'),\n",
       " (u'new_zealand', u'real_property'),\n",
       " (u'certified_check', u'bank_check'),\n",
       " (u'certified_check', u'bill_of_exchange'),\n",
       " (u'certified_check', u'written_document'),\n",
       " (u'certified_check', u'cheque'),\n",
       " (u'certified_check', u'order_of_payment'),\n",
       " (u'pearl', u'jewel'),\n",
       " (u'playwriting', u'writer'),\n",
       " (u'biosatellite', u'electrical_appliance'),\n",
       " (u'biosatellite', u'means_of_transport'),\n",
       " (u'biosatellite', u'artificial_satellite'),\n",
       " (u'biosatellite', u'spaceship'),\n",
       " (u'passage', u'journey'),\n",
       " (u'passage', u'journeying'),\n",
       " (u'passage', u'voyage'),\n",
       " (u'passage', u'travelling'),\n",
       " (u'syzygy', u'natural_phenomenon'),\n",
       " (u'tax_haven', u'country'),\n",
       " (u'magnet', u'cardinal'),\n",
       " (u'autonomous_region', u'constituent_state'),\n",
       " (u'city_of_whitehorse', u'local_government_area'),\n",
       " (u'explosion', u'shot'),\n",
       " (u'explosion', u'move'),\n",
       " (u'explosion', u'outburst'),\n",
       " (u'cyclone_gafilo', u'violent_storm'),\n",
       " (u'earthquake', u'natural_disaster'),\n",
       " (u'earthquake', u'disaster'),\n",
       " (u'earthquake', u'tragedy'),\n",
       " (u'earthquake', u'catastrophe'),\n",
       " (u'earthquake', u'commotion'),\n",
       " (u'earthquake', u'dance_orchestra'),\n",
       " (u'tectonic', u'marking'),\n",
       " (u'leaf_curl', u'illness'),\n",
       " (u'leaf_curl', u'plant_disease'),\n",
       " (u'leaf_curl', u'sickness'),\n",
       " (u'leaf_curl', u'disease'),\n",
       " (u'leaf_curl', u'plant_pathology'),\n",
       " (u'leaf_curl', u'disorder'),\n",
       " (u'rainfall', u'downfall'),\n",
       " (u'rainfall', u'natural_phenomenon'),\n",
       " (u'rainfall', u'weather'),\n",
       " (u'rainfall', u'weather_condition'),\n",
       " (u'endemic_goitre', u'disease'),\n",
       " (u'endemic_goitre', u'sickness'),\n",
       " (u'club', u'baseball_team'),\n",
       " (u'curriculum', u'course_of_instruction'),\n",
       " (u'curriculum', u'course_of_study'),\n",
       " (u'cat', u'woman'),\n",
       " (u'cat', u'female'),\n",
       " (u'cat', u'female_person'),\n",
       " (u'cat', u'excitant'),\n",
       " (u'cat', u'sexuality'),\n",
       " (u'cat', u'feline'),\n",
       " (u'printing', u'written_language'),\n",
       " (u'photojournalism', u'story'),\n",
       " (u'photojournalism', u'short_story'),\n",
       " (u'photojournalism', u'journalism'),\n",
       " (u'photojournalism', u'piece_of_work'),\n",
       " (u'photojournalism', u'narrative'),\n",
       " (u'photojournalism', u'news_media'),\n",
       " (u'photojournalism', u'book'),\n",
       " (u'photojournalism', u'mass_medium'),\n",
       " (u'el_nino', u'dramatics'),\n",
       " (u'horizontal_bar', u'durable_good'),\n",
       " (u'horizontal_bar', u'durable_goods'),\n",
       " (u'horizontal_bar', u'gymnastic_apparatus'),\n",
       " (u'horizontal_bar', u'exerciser'),\n",
       " (u'horizontal_bar', u'sports_equipment'),\n",
       " (u'parameter', u'computer_code'),\n",
       " (u'parameter', u'constant'),\n",
       " (u'parameter', u'technical_specification'),\n",
       " (u'parameter', u'reference'),\n",
       " (u'parameter', u'specifications'),\n",
       " (u'parameter', u'invariable'),\n",
       " (u'parameter', u'code'),\n",
       " (u'parameter', u'data_type'),\n",
       " (u'parameter', u'constant_quantity'),\n",
       " (u'parameter', u'computer_address'),\n",
       " (u'cocktail', u'mixed_drink'),\n",
       " (u'cocktail', u'intoxicant'),\n",
       " (u'cocktail', u'drink'),\n",
       " (u'cocktail', u'alcoholic_drink'),\n",
       " (u'cocktail', u'drinkable'),\n",
       " (u'sacrifice', u'putting_to_death'),\n",
       " (u'sphagnum', u'nonvascular_plant'),\n",
       " (u'sphagnum', u'plant'),\n",
       " (u'sphagnum', u'moss'),\n",
       " (u'neck', u'cut_of_meat'),\n",
       " (u'liquid', u'state_of_matter'),\n",
       " (u'liquid', u'fluid'),\n",
       " (u'switch', u'computing_device'),\n",
       " (u'switch', u'rail_transportation'),\n",
       " (u'switch', u'electrical_network'),\n",
       " (u'switch', u'computer_circuit'),\n",
       " (u'switch', u'add-in'),\n",
       " (u'switch', u'railroad_track'),\n",
       " (u'switch', u'plug-in'),\n",
       " (u'switch', u'integrated_circuit'),\n",
       " (u'switch', u'wig'),\n",
       " (u'switch', u'card'),\n",
       " (u'switch', u'slot'),\n",
       " (u'switch', u'circuit_board'),\n",
       " (u'switch', u'appliance'),\n",
       " (u'switch', u'semiconductor'),\n",
       " (u'switch', u'fitting'),\n",
       " (u'switch', u'electronic_equipment'),\n",
       " (u'switch', u'board'),\n",
       " (u'switch', u'circuit'),\n",
       " (u'switch', u'attire'),\n",
       " (u'switch', u'microcircuit'),\n",
       " (u'switch', u'computer_chip'),\n",
       " (u'switch', u'circuit_card'),\n",
       " (u'metrolink', u'public_transit'),\n",
       " (u'metrolink', u'move'),\n",
       " (u'metrolink', u'traveling'),\n",
       " (u'metrolink', u'public_service'),\n",
       " (u'metrolink', u'bus_service'),\n",
       " (u'metrolink', u'public_transport'),\n",
       " (u'metrolink', u'corporation'),\n",
       " (u'metrolink', u'travel'),\n",
       " (u'metrolink', u'travelling'),\n",
       " (u'metrolink', u'public_utility'),\n",
       " (u'metrolink', u'movement'),\n",
       " (u'metrolink', u'public_utility_company'),\n",
       " (u'metrolink', u'transportation'),\n",
       " (u'metrolink', u'passenger_transport'),\n",
       " (u'metrolink', u'bus_company'),\n",
       " (u'metrolink', u'means_of_transport'),\n",
       " (u'metrolink', u'transportation_system'),\n",
       " (u'metrolink', u'public-service_corporation'),\n",
       " (u'metrolink', u'transportation_company'),\n",
       " (u'turnstile', u'gate'),\n",
       " (u'turnstile', u'impediment'),\n",
       " (u'grimm', u'season'),\n",
       " (u'grimm', u'moving-picture_show'),\n",
       " (u'grimm', u'comedy'),\n",
       " (u'grimm', u'writer'),\n",
       " (u'foreclosure', u'proceeding'),\n",
       " (u'foreclosure', u'due_process'),\n",
       " (u'foreclosure', u'legal_proceeding'),\n",
       " (u'foreclosure', u'legal_proceedings'),\n",
       " (u'course', u'way'),\n",
       " (u'course', u'series'),\n",
       " (u'course', u'itinerary'),\n",
       " (u'course', u'course_of_study'),\n",
       " (u'course', u'arrangement'),\n",
       " (u'course', u'route'),\n",
       " (u'course', u'steering'),\n",
       " (u'course', u'piloting'),\n",
       " (u'escutcheon', u'suit_of_armour'),\n",
       " (u'escutcheon', u'armour'),\n",
       " (u'suspension', u'state'),\n",
       " (u'suspension', u'inaction'),\n",
       " (u'norfolk', u'town'),\n",
       " (u'norfolk', u'township'),\n",
       " (u'unsafe_abortion', u'termination'),\n",
       " (u'unsafe_abortion', u'abortion'),\n",
       " (u'market_value', u'net_worth'),\n",
       " (u'market_value', u'price'),\n",
       " (u'market_value', u'retail'),\n",
       " (u'richard_posner', u'officeholder'),\n",
       " (u'richard_posner', u'economic_expert'),\n",
       " (u'richard_posner', u'jurist'),\n",
       " (u'richard_posner', u'economist'),\n",
       " (u'richard_posner', u'jurisconsult'),\n",
       " (u'agogo', u'bell'),\n",
       " (u'bigtooth_aspen', u'binomen'),\n",
       " (u'bigtooth_aspen', u'aspen'),\n",
       " (u'bigtooth_aspen', u'perennial'),\n",
       " (u'bigtooth_aspen', u'woody_plant'),\n",
       " (u'bigtooth_aspen', u'flowering_tree'),\n",
       " (u'bigtooth_aspen', u'native_plant'),\n",
       " (u'bigtooth_aspen', u'binomial_nomenclature'),\n",
       " (u'ghrelin', u'body_substance'),\n",
       " (u'ghrelin', u'bodily_fluid'),\n",
       " (u'ghrelin', u'hormone'),\n",
       " (u'ghrelin', u'humor'),\n",
       " (u'ghrelin', u'body_fluid'),\n",
       " (u'ghrelin', u'moiety'),\n",
       " (u'ghrelin', u'bond'),\n",
       " (u'benefaction', u'kindness'),\n",
       " (u'abdication', u'resignation'),\n",
       " (u'tebuconazole', u'fungicide'),\n",
       " (u'heritage', u'transferred_property'),\n",
       " (u'delay', u'break'),\n",
       " (u'crossover', u'elector'),\n",
       " (u'crossover', u'borrowing'),\n",
       " (u'crossover', u'economic_science'),\n",
       " (u'jon_cassar', u'television_director'),\n",
       " (u'bramble', u'bramble_bush'),\n",
       " (u'property_owner', u'owner'),\n",
       " (u'workplace', u'making'),\n",
       " (u'workplace', u'fashioning'),\n",
       " (u'epistemology', u'philosophy'),\n",
       " (u'epistemology', u'inquiry'),\n",
       " (u'self-justification', u'asseveration'),\n",
       " (u'self-justification', u'pleading'),\n",
       " (u'declaration', u'evidence'),\n",
       " (u'true_life', u'moving-picture_show'),\n",
       " (u'pressing', u'push'),\n",
       " (u'pressing', u'pushing'),\n",
       " (u'israel', u'political_organization'),\n",
       " (u'israel', u'country'),\n",
       " (u'israel', u'republic'),\n",
       " (u'israel', u'form_of_government'),\n",
       " (u'israel', u'gov'),\n",
       " (u'edger', u'garden_tool'),\n",
       " (u'edger', u'needleworker'),\n",
       " (u'termination', u'written_record'),\n",
       " (u'termination', u'finish'),\n",
       " (u'termination', u'end'),\n",
       " (u'termination', u'conclusion'),\n",
       " (u'termination', u'written_account'),\n",
       " (u'termination', u'ending'),\n",
       " (u'white_house', u'official_residence'),\n",
       " (u'white_house', u'public_building'),\n",
       " (u'white_house', u'government_building'),\n",
       " (u'white_house', u'manse'),\n",
       " (u'white_house', u'mansion'),\n",
       " (u'white_house', u'mansion_house'),\n",
       " (u'german_mark', u'monetary_unit'),\n",
       " (u'preston_manning', u'politician'),\n",
       " (u'preston_manning', u'boss'),\n",
       " (u'preston_manning', u'chief'),\n",
       " (u'preston_manning', u'political_leader'),\n",
       " (u'grit', u'courageousness'),\n",
       " (u'grit', u'stone'),\n",
       " (u'grit', u'rock'),\n",
       " (u'grit', u'braveness'),\n",
       " (u'grit', u'bravery'),\n",
       " (u'nuclear_war', u'warfare'),\n",
       " (u'nuclear_war', u'military_action'),\n",
       " (u'nuclear_war', u'war'),\n",
       " (u'world', u'wonder'),\n",
       " (u'particle', u'part_of_speech'),\n",
       " (u'particle', u'grammatical_category'),\n",
       " (u'verbal_abuse', u'offence'),\n",
       " (u'verbal_abuse', u'offensive_activity'),\n",
       " (u'verbal_abuse', u'ill-usage'),\n",
       " (u'verbal_abuse', u'offense'),\n",
       " (u'expiration_date', u'period_of_time'),\n",
       " (u'power', u'man_of_affairs'),\n",
       " (u'tony_trujillo', u'skateboarder'),\n",
       " (u'thiocyanate', u'crystal'),\n",
       " (u'thiocyanate', u'salt'),\n",
       " (u'curling', u'winter_sport'),\n",
       " (u'camelpox', u'disorder'),\n",
       " (u'camelpox', u'illness'),\n",
       " (u'camelpox', u'sickness'),\n",
       " (u'psychopathology', u'medical_research'),\n",
       " (u'psychopathology', u'psychology'),\n",
       " (u'psychopathology', u'learned_profession'),\n",
       " (u'psychopathology', u'enquiry'),\n",
       " (u'psychopathology', u'psychological_science'),\n",
       " (u'psychopathology', u'inquiry'),\n",
       " (u'psychopathology', u'investigation'),\n",
       " (u'psychopathology', u'branch_of_science'),\n",
       " (u'psychopathology', u'medicine'),\n",
       " (u'psychopathology', u'history_of_medicine'),\n",
       " (u'psychopathology', u'practice_of_medicine'),\n",
       " (u'canyonlands', u'piece_of_land'),\n",
       " (u'canyonlands', u'national_park'),\n",
       " (u'canyonlands', u'parcel'),\n",
       " (u'canyonlands', u'tract'),\n",
       " (u'canyonlands', u'piece_of_ground'),\n",
       " (u'canyonlands', u'parcel_of_land'),\n",
       " (u'tim_dwight', u'ball_player'),\n",
       " (u'grass', u'herb'),\n",
       " (u'grass', u'soft_drug'),\n",
       " (u'general', u'warrior'),\n",
       " (u'general', u'war'),\n",
       " (u'general', u'military_branch'),\n",
       " (u'general', u'combat'),\n",
       " (u'general', u'military_rank'),\n",
       " (u'general', u'boss'),\n",
       " (u'general', u'military_man'),\n",
       " (u'general', u'battle'),\n",
       " (u'general', u'post'),\n",
       " (u'general', u'commissioned_officer'),\n",
       " (u'general', u'armed_forces'),\n",
       " (u'general', u'warfare'),\n",
       " (u'general', u'chief'),\n",
       " (u'general', u'fight'),\n",
       " (u'general', u'soldier'),\n",
       " (u'general', u'commissioned_military_officer'),\n",
       " (u'general', u'military'),\n",
       " (u'general', u'military_unit'),\n",
       " (u'general', u'leader'),\n",
       " (u'knowth', u'piece_of_ground'),\n",
       " (u'knowth', u'sepulchre'),\n",
       " (u'graduate', u'scholarly_person'),\n",
       " (u'graduate', u'student'),\n",
       " (u'graduate', u'scholar'),\n",
       " (u'transantarctic_mountains', u'mountain'),\n",
       " (u'transantarctic_mountains', u'mountain_range'),\n",
       " (u'transantarctic_mountains', u'mountain_chain'),\n",
       " (u'transantarctic_mountains', u'mount'),\n",
       " (u'transantarctic_mountains', u'range_of_mountains'),\n",
       " (u'transantarctic_mountains', u'chain_of_mountains'),\n",
       " (u'tropical_storm_helene', u'windstorm'),\n",
       " (u'tropical_storm_helene', u'violent_storm'),\n",
       " (u'tropical_storm_helene', u'storm'),\n",
       " (u'udp_lite', u'file_format'),\n",
       " (u'udp_lite', u'textfile'),\n",
       " (u'udp_lite', u'prescript'),\n",
       " (u'udp_lite', u'rule'),\n",
       " (u'udp_lite', u'measure'),\n",
       " (u'udp_lite', u'text_file'),\n",
       " (u'pool', u'poker'),\n",
       " (u'pool', u'investment_fund'),\n",
       " (u'pool', u'net_earnings'),\n",
       " (u'pool', u'poker_game'),\n",
       " (u'pool', u'competition'),\n",
       " (u'pool', u'olympic_sports'),\n",
       " (u'pool', u'net_profit'),\n",
       " (u'pool', u'net_income'),\n",
       " (u'pool', u'body_of_water'),\n",
       " (u'pool', u'sport'),\n",
       " (u'pool', u'assets'),\n",
       " (u'pool', u'profit'),\n",
       " (u'pool', u'billards'),\n",
       " (u'pool', u'athlete'),\n",
       " (u'pool', u'table_game'),\n",
       " (u'pool', u'gamble'),\n",
       " (u'pool', u'income'),\n",
       " (u'pool', u'collective_investment_scheme'),\n",
       " (u'pool', u'net'),\n",
       " (u'pool', u'investment_funds'),\n",
       " (u'pool', u'profits'),\n",
       " (u'pool', u'earnings'),\n",
       " (u'dollar', u'bill'),\n",
       " (u'dollar', u'money'),\n",
       " (u'dollar', u'bank_bill'),\n",
       " (u'red_planet', u'planet'),\n",
       " (u'red_planet', u'story'),\n",
       " (u'red_planet', u'movie'),\n",
       " (u'red_planet', u'superior_planet'),\n",
       " (u'red_planet', u'fiction'),\n",
       " (u'red_planet', u'tale'),\n",
       " (u'red_planet', u'film_genre'),\n",
       " (u'red_planet', u'moving-picture_show'),\n",
       " (u'red_planet', u'motion_picture'),\n",
       " (u'red_planet', u'major_planet'),\n",
       " (u'red_planet', u'rocky_planet'),\n",
       " (u'red_planet', u'film'),\n",
       " (u'red_planet', u'picture_show'),\n",
       " (u'pyrrhus', u'potentate'),\n",
       " (u'program', u'flyer'),\n",
       " (u'program', u'promotional_material'),\n",
       " (u'program', u'social_event'),\n",
       " (u'program', u'portraying'),\n",
       " (u'program', u'advertisement'),\n",
       " (u'program', u'portrayal'),\n",
       " (u'program', u'course_of_instruction'),\n",
       " (u'program', u'announcement'),\n",
       " (u'program', u'promulgation'),\n",
       " (u'program', u'entertainment'),\n",
       " (u'program', u'advert'),\n",
       " (u'program', u'advertising'),\n",
       " (u'program', u'mass_media'),\n",
       " (u'program', u'print_media'),\n",
       " (u'program', u'presentment'),\n",
       " (u'program', u'throwaway'),\n",
       " (u'antony_leung', u'owner'),\n",
       " (u'antony_leung', u'holder'),\n",
       " (u'antony_leung', u'possessor'),\n",
       " (u'emery_cloth', u'benefactor'),\n",
       " (u'emery_cloth', u'advert'),\n",
       " (u'capillary', u'cylinder'),\n",
       " (u'capillary', u'vasculature'),\n",
       " (u'capillary', u'closed_circulatory_system'),\n",
       " (u'capillary', u'tube'),\n",
       " (u'capillary', u'body_structure'),\n",
       " (u'capillary', u'anatomical_structure'),\n",
       " (u'capillary', u'vascular_system'),\n",
       " (u'capillary', u'body_part'),\n",
       " (u'capillary', u'bodily_structure'),\n",
       " (u'capillary', u'vessel'),\n",
       " (u'capillary', u'tubing'),\n",
       " (u'capillary', u'plane_figure'),\n",
       " (u'capillary', u'blood_supply'),\n",
       " (u'condition', u'mention'),\n",
       " (u'condition', u'good_health'),\n",
       " (u'condition', u'written_document'),\n",
       " (u'condition', u'written_agreement'),\n",
       " (u'condition', u'commentary'),\n",
       " (u'landscape_arch', u'natural_arch'),\n",
       " (u'subsoil', u'soil'),\n",
       " (u'subsoil', u'dirt'),\n",
       " (u'federal_records', u'firm'),\n",
       " (u'federal_records', u'intellectual_property'),\n",
       " (u'federal_records', u'company'),\n",
       " (u'federal_records', u'portraying'),\n",
       " (u'federal_records', u'portrayal'),\n",
       " (u'federal_records', u'depicting'),\n",
       " (u'federal_records', u'concern'),\n",
       " (u'federal_records', u'industrial_property'),\n",
       " (u'federal_records', u'corporation'),\n",
       " (u'federal_records', u'prerogative'),\n",
       " (u'federal_records', u'juridical_person'),\n",
       " (u'federal_records', u'branch'),\n",
       " (u'small', u'adjective'),\n",
       " (u'san_andreas', u'town'),\n",
       " (u'san_andreas', u'picture'),\n",
       " (u'san_andreas', u'moving_picture'),\n",
       " (u'san_andreas', u'country'),\n",
       " (u'san_andreas', u'census_place'),\n",
       " (u'san_andreas', u'moving-picture_show'),\n",
       " (u'san_andreas', u'movie'),\n",
       " (u'san_andreas', u'motion_picture'),\n",
       " (u'san_andreas', u'city'),\n",
       " (u'san_andreas', u'picture_show'),\n",
       " (u'fence', u'merchant'),\n",
       " (u'zero', u'grammatical_category'),\n",
       " (u'zero', u'decimal_digit'),\n",
       " (u'zero', u'imaginary'),\n",
       " (u'zero', u'natural_number'),\n",
       " (u'zero', u'complex_number'),\n",
       " (u'zero', u'numerical_quantity'),\n",
       " (u'zero', u'imaginary_number'),\n",
       " (u'zero', u'counting_number'),\n",
       " (u'zero', u'natural_numbers'),\n",
       " (u'zero', u'whole_number'),\n",
       " (u'investment', u'possession'),\n",
       " (u'investment', u'financing'),\n",
       " (u'investment', u'finance'),\n",
       " (u'investment', u'protection'),\n",
       " (u'investment', u'assets'),\n",
       " (u'golden_gate_park', u'parkland'),\n",
       " (u'golden_gate_park', u'country'),\n",
       " (u'golden_gate_park', u'park'),\n",
       " (u'golden_gate_park', u'parcel_of_land'),\n",
       " (u'korea', u'city'),\n",
       " (u'korea', u'plot_of_land'),\n",
       " (u'korea', u'parcel_of_land'),\n",
       " (u'korea', u'piece_of_land'),\n",
       " (u'korea', u'country'),\n",
       " (u'korea', u'building_site'),\n",
       " (u'perfidiousness', u'prevarication'),\n",
       " (u'dirt', u'poo'),\n",
       " (u'dirt', u'shit'),\n",
       " (u'dirt', u'excrement'),\n",
       " (u'dirt', u'dog_shit'),\n",
       " (u'gary_fong', u'lensman'),\n",
       " (u'aramaic', u'semitic_people'),\n",
       " (u'aramaic', u'semite'),\n",
       " (u'aramaic', u'semitic'),\n",
       " (u'mohammed', u'saint'),\n",
       " (u'seafloor', u'natural_depression'),\n",
       " (u'seafloor', u'type_of_sport'),\n",
       " (u'seafloor', u'depression'),\n",
       " (u'seafloor', u'bottom'),\n",
       " (u'gigahertz', u'unit_of_measure'),\n",
       " (u'gigahertz', u'magnitude'),\n",
       " (u'tragedy', u'misfortune'),\n",
       " (u'weather_forecasting', u'forecasting'),\n",
       " (u'weather_forecasting', u'foretelling'),\n",
       " (u'phonetic_transcription', u'written_language'),\n",
       " (u'phonetic_transcription', u'written_text'),\n",
       " (u'phonetic_transcription', u'transcription'),\n",
       " (u'striatum', u'body_structure'),\n",
       " (u'striatum', u'brain'),\n",
       " (u'striatum', u'body_part'),\n",
       " (u'striatum', u'nucleus'),\n",
       " (u'striatum', u'bodily_structure'),\n",
       " (u'leveling', u'destruction'),\n",
       " (u'scott_city', u'town'),\n",
       " (u'scott_city', u'county_town'),\n",
       " (u'harmonia_axyridis', u'beetle'),\n",
       " (u'harmonia_axyridis', u'beetles'),\n",
       " (u'airport', u'landing_field'),\n",
       " (u'glycogen_synthase', u'glucosyltransferase'),\n",
       " (u'glycogen_synthase', u'glycosyltransferase'),\n",
       " (u'clastic', u'stone'),\n",
       " (u'clastic', u'pebble'),\n",
       " (u'state_highway_17', u'thoroughfare'),\n",
       " (u'state_highway_17', u'main_road'),\n",
       " (u'state_highway_17', u'motorway'),\n",
       " (u'state_highway_17', u'throughway'),\n",
       " (u'state_highway_17', u'controlled-access_highway'),\n",
       " (u'state_highway_12', u'thoroughfare'),\n",
       " (u'state_highway_12', u'main_road'),\n",
       " (u'state_highway_12', u'throughway'),\n",
       " (u'full_stop', u'mark'),\n",
       " (u'full_stop', u'sign'),\n",
       " (u'transit', u'transportation'),\n",
       " (u'transit', u'travel'),\n",
       " (u'transit', u'move'),\n",
       " (u'transit', u'passenger_transport'),\n",
       " (u'transit', u'traveling'),\n",
       " (u'transit', u'travelling'),\n",
       " (u'transit', u'movement'),\n",
       " (u'abutilon_theophrasti', u'plant'),\n",
       " (u'abutilon_theophrasti', u'perennial'),\n",
       " (u'abutilon_theophrasti', u'annual_plant'),\n",
       " (u's4', u'band'),\n",
       " (u's4', u'musical_group'),\n",
       " (u'louisville', u'county_courthouse'),\n",
       " (u'louisville', u'town'),\n",
       " (u'louisville', u'city'),\n",
       " (u'louisville', u'country'),\n",
       " (u'louisville', u'provincial_capital'),\n",
       " (u'regiment', u'military_group'),\n",
       " (u'cherryholmes', u'bandmember'),\n",
       " (u'cherryholmes', u'dance_band'),\n",
       " (u'europe', u'public_building'),\n",
       " (u'europe', u'traveling'),\n",
       " (u'europe', u'railway_system'),\n",
       " (u'europe', u'railway_line'),\n",
       " (u'europe', u'memorial'),\n",
       " (u'europe', u'travel'),\n",
       " (u'europe', u'subway_system'),\n",
       " (u'europe', u'means_of_transportation'),\n",
       " (u'europe', u'dance_orchestra'),\n",
       " (u'europe', u'travelling'),\n",
       " (u'europe', u'railroad_line'),\n",
       " (u'omiya', u'city'),\n",
       " (u'cowpox', u'pox'),\n",
       " (u'cowpox', u'sickness'),\n",
       " (u'cowpox', u'excoriation'),\n",
       " (u'cowpox', u'skin_disease'),\n",
       " (u'cowpox', u'blister'),\n",
       " (u'cowpox', u'contagion'),\n",
       " (u'cowpox', u'scrape'),\n",
       " (u'supermajority', u'relative_majority'),\n",
       " (u'interception', u'snatch'),\n",
       " (u'interception', u'touching'),\n",
       " (u'interception', u'physical_intimacy'),\n",
       " (u'interception', u'hindrance'),\n",
       " (u'interception', u'grab'),\n",
       " (u'interception', u'hinderance'),\n",
       " (u'cori', u'family_name'),\n",
       " (u'cori', u'personal_name'),\n",
       " (u'cori', u'surname'),\n",
       " (u'pollock', u'seafood'),\n",
       " (u'pollock', u'animal'),\n",
       " (u'pollock', u'fish'),\n",
       " (u'pollock', u'saltwater_fish'),\n",
       " (u'teleconference', u'conference'),\n",
       " (u'teleconference', u'discussion'),\n",
       " (u'catoctin_mountain_park', u'conservation_area'),\n",
       " (u'hegemony', u'political_system'),\n",
       " (u'hegemony', u'form_of_government'),\n",
       " (u'chestnut', u'wood'),\n",
       " (u'kennedy', u'president'),\n",
       " (u'kennedy', u'politician'),\n",
       " (u'kennedy', u'international_airport'),\n",
       " (u'kennedy', u'leader'),\n",
       " (u'quickening', u'hurrying'),\n",
       " (u'finalization', u'culmination'),\n",
       " (u'finalization', u'completion'),\n",
       " (u'finalization', u'closing'),\n",
       " (u'finalization', u'conclusion'),\n",
       " (u'finalization', u'termination'),\n",
       " (u'flak', u'weapon'),\n",
       " (u'flak', u'gun'),\n",
       " (u'zhao_ziyang', u'politician'),\n",
       " (u'zhao_ziyang', u'leader'),\n",
       " (u'zhao_ziyang', u'officeholder'),\n",
       " (u'zhao_ziyang', u'political_leader'),\n",
       " (u'voltage_divider', u'electrical_element'),\n",
       " (u'voltage_divider', u'electrical_device'),\n",
       " (u'counterclaim', u'proceedings'),\n",
       " (u'counterclaim', u'legal_action'),\n",
       " (u'counterclaim', u'action_at_law'),\n",
       " (u'counterclaim', u'due_process'),\n",
       " (u'counterclaim', u'legal_proceedings'),\n",
       " (u'counterclaim', u'legal_proceeding'),\n",
       " (u'thread', u'cord'),\n",
       " (u'thread', u'rib'),\n",
       " (u'recall', u'abrogation'),\n",
       " (u'recall', u'annulment'),\n",
       " (u'recall', u'call'),\n",
       " (u'coalition', u'executive'),\n",
       " (u'coalition', u'union'),\n",
       " (u'coalition', u'party'),\n",
       " (u'coalition', u'government'),\n",
       " (u'coalition', u'mention'),\n",
       " (u'abnegation', u'self-denial'),\n",
       " (u'wrestler', u'scrapper'),\n",
       " (u'wrestler', u'fighter'),\n",
       " (u'wrestler', u'battler'),\n",
       " (u'gram_parsons', u'writer'),\n",
       " (u'gram_parsons', u'piano_player'),\n",
       " (u'gram_parsons', u'lyrist'),\n",
       " (u'gram_parsons', u'songster'),\n",
       " (u'needlepoint', u'work_of_art'),\n",
       " (u'needlepoint', u'needlework'),\n",
       " (u'needlepoint', u'embroidery'),\n",
       " (u'needlepoint', u'needlecraft'),\n",
       " (u'needlepoint', u'handwork'),\n",
       " (u'ibm', u'firm'),\n",
       " (u'ibm', u'corporation'),\n",
       " (u'ibm', u'company'),\n",
       " (u'ibm', u'business_organization'),\n",
       " (u'ibm', u'business_firm'),\n",
       " (u'ibm', u'corp'),\n",
       " (u'ibm', u'venture'),\n",
       " (u'ibm', u'enterprise'),\n",
       " (u'ibm', u'business_organisation'),\n",
       " (u'petrology', u'geophysics'),\n",
       " (u'petrology', u'geophysical_science'),\n",
       " (u'petrology', u'geology'),\n",
       " (u'petrology', u'geomorphology'),\n",
       " (u'ibc', u'manufacturer'),\n",
       " (u'ibc', u'company'),\n",
       " (u'ibc', u'enterprise'),\n",
       " (u'share', u'possession'),\n",
       " (u'share', u'apportioning'),\n",
       " (u'share', u'assets'),\n",
       " (u'share', u'apportionment'),\n",
       " (u'share', u'asset'),\n",
       " (u'share', u'allotment'),\n",
       " (u'synchronizer', u'measuring_device'),\n",
       " (u'literary_critic', u'judge'),\n",
       " (u'literary_critic', u'critic'),\n",
       " (u'dress', u'clothes'),\n",
       " (u'footman', u'house_servant'),\n",
       " (u'footman', u'manservant'),\n",
       " (u'footman', u'servant'),\n",
       " (u'activator', u'biomolecule'),\n",
       " (u'chromosome_9', u'dna'),\n",
       " (u'chromosome_9', u'double-stranded_dna'),\n",
       " (u'chromosome_9', u'somatic_chromosome'),\n",
       " (u'chromosome_9', u'acid'),\n",
       " (u'chromosome_9', u'chromosome'),\n",
       " (u'top_side', u'country'),\n",
       " (u'top_side', u'real_estate'),\n",
       " (u'top_side', u'realty'),\n",
       " (u'algebra', u'pure_mathematics'),\n",
       " (u'algebra', u'area_of_mathematics'),\n",
       " (u'tom_hanks', u'film_director'),\n",
       " (u'tom_hanks', u'writer'),\n",
       " (u'tom_hanks', u'actor'),\n",
       " (u'tom_hanks', u'filmmaker'),\n",
       " (u'tom_hanks', u'filming'),\n",
       " (u'syria', u'political_organization'),\n",
       " (u'syria', u'city'),\n",
       " (u'syria', u'country'),\n",
       " (u'mexico', u'republic'),\n",
       " (u'mexico', u'provincial_capital'),\n",
       " (u'mexico', u'economy'),\n",
       " (u'mexico', u'government'),\n",
       " (u'mexico', u'piece_of_land'),\n",
       " (u'mexico', u'township'),\n",
       " (u'mexico', u'country'),\n",
       " (u'mexico', u'federal_district'),\n",
       " (u'register', u'written_account'),\n",
       " (u'register', u'evidence'),\n",
       " (u'register', u'written_record'),\n",
       " (u'register', u'card'),\n",
       " (u'register', u'electronic_equipment'),\n",
       " (u'register', u'circuit'),\n",
       " (u'register', u'written_document'),\n",
       " (u'register', u'scientific_evidence'),\n",
       " (u'coal', u'stone'),\n",
       " (u'coal', u'fuel'),\n",
       " (u'lucrezia_borgia', u'noble'),\n",
       " (u'lucrezia_borgia', u'patrician'),\n",
       " (u'lucrezia_borgia', u'aristocrat'),\n",
       " (u'lucrezia_borgia', u'lady'),\n",
       " (u'lucrezia_borgia', u'noblewoman'),\n",
       " (u'lucrezia_borgia', u'moving-picture_show'),\n",
       " (u'turnpike', u'main_road'),\n",
       " (u'backstay', u'strengthener'),\n",
       " (u'euphemism', u'saying'),\n",
       " (u'euphemism', u'wit'),\n",
       " (u'anticyclone', u'atmosphere'),\n",
       " (u'anticyclone', u'natural_phenomenon'),\n",
       " (u'anticyclone', u'weather_condition'),\n",
       " (u'lyman', u'polity'),\n",
       " (u'lyman', u'township'),\n",
       " (u'copper', u'conductor'),\n",
       " (u'copper', u'native_metal'),\n",
       " (u'copper', u'legal_tender'),\n",
       " (u'copper', u'coin'),\n",
       " (u'copper', u'stone'),\n",
       " (u'copper', u'metal'),\n",
       " (u'copper', u'venetian_red'),\n",
       " (u'copper', u'negotiable_instrument'),\n",
       " (u'closeness', u'belonging'),\n",
       " (u'square_sail', u'sail'),\n",
       " (u'maximian', u'title_of_respect'),\n",
       " (u'maximian', u'emperor_of_rome'),\n",
       " (u'maximian', u'emperor'),\n",
       " (u'syneresis', u'sound_change'),\n",
       " (u'syneresis', u'chemical_action'),\n",
       " (u'syneresis', u'natural_action'),\n",
       " (u'syneresis', u'natural_process'),\n",
       " (u'bobby_williams', u'brotherhood'),\n",
       " (u'bobby_williams', u'jock'),\n",
       " (u'bobby_williams', u'national_sports_team'),\n",
       " (u'bobby_williams', u'sportsperson'),\n",
       " (u'avenue', u'thruway'),\n",
       " (u'avenue', u'street'),\n",
       " (u'avenue', u'main_road'),\n",
       " (u'avenue', u'expressway'),\n",
       " (u'texas', u'city'),\n",
       " (u'unix_shell', u'computer_programme'),\n",
       " (u'unix_shell', u'shell'),\n",
       " (u'unix_shell', u'fundamental_interaction'),\n",
       " (u'papillon', u'dog_type'),\n",
       " (u'papillon', u'toy_dog'),\n",
       " (u'lats', u'monetary_unit'),\n",
       " (u'ideal_gas', u'gaseous_state'),\n",
       " (u'ideal_gas', u'state'),\n",
       " (u'ideal_gas', u'gas'),\n",
       " (u'ideal_gas', u'state_of_matter'),\n",
       " (u'boson', u'subatomic_particle'),\n",
       " (u'saltire', u'cross'),\n",
       " (u'brookline', u'town'),\n",
       " (u'brookline', u'township'),\n",
       " (u'brookline', u'new_england_town'),\n",
       " (u'child_care', u'aid'),\n",
       " (u'child_care', u'help'),\n",
       " (u'van', u'camper_van'),\n",
       " (u'van', u'motorhome'),\n",
       " (u'van', u'military_force'),\n",
       " (u'van', u'travelling'),\n",
       " (u'van', u'lorry'),\n",
       " (u'van', u'railway_car'),\n",
       " (u'van', u'military_group'),\n",
       " (u'van', u'wagon'),\n",
       " (u'van', u'means_of_transportation'),\n",
       " (u'van', u'means_of_transport'),\n",
       " (u'van', u'campervan'),\n",
       " (u'van', u'conveyance'),\n",
       " (u'van', u'truck'),\n",
       " (u'van', u'railroad_car'),\n",
       " (u'van', u'military_organization'),\n",
       " (u'winterthur', u'town'),\n",
       " (u'viral_marketing', u'social_relation'),\n",
       " (u'viral_marketing', u'buzz_word'),\n",
       " (u'viral_marketing', u'buzzword'),\n",
       " (u'sucker', u'dupe'),\n",
       " (u'sucker', u'goody'),\n",
       " (u'sucker', u'confection'),\n",
       " (u'sucker', u'candy'),\n",
       " (u'sucker', u'sweets'),\n",
       " (u'sucker', u'sweet'),\n",
       " (u'pusey', u'scholarly_person'),\n",
       " (u'shuttle', u'means_of_transport'),\n",
       " (u'shuttle', u'travel'),\n",
       " (u'shuttle', u'move'),\n",
       " (u'shuttle', u'missile'),\n",
       " (u'shuttle', u'motion'),\n",
       " (u'shuttle', u'public_transport'),\n",
       " (u'shuttle', u'conveyance'),\n",
       " (u'shuttle', u'traveling'),\n",
       " (u'shuttle', u'travelling'),\n",
       " (u'shuttle', u'movement'),\n",
       " (u'bed_rest', u'attention'),\n",
       " (u'bed_rest', u'rest'),\n",
       " (u'bed_rest', u'medical_aid'),\n",
       " (u'bed_rest', u'aid'),\n",
       " (u'bed_rest', u'care'),\n",
       " (u'compound', u'constructed_structure'),\n",
       " (u'compound', u'edifice'),\n",
       " (u'compound', u'public_building'),\n",
       " (u'release_notes', u'computer_file'),\n",
       " (u'release_notes', u'evidence'),\n",
       " (u'release_notes', u'write-up'),\n",
       " (u'release_notes', u'written_document'),\n",
       " (u'release_notes', u'scientific_evidence'),\n",
       " (u'release_notes', u'file'),\n",
       " (u'release_notes', u'text_file'),\n",
       " (u'release_notes', u'textfile'),\n",
       " (u'derbyshire_peak', u'natural_elevation'),\n",
       " (u'larderello', u'city'),\n",
       " (u'siliana', u'town'),\n",
       " (u'siliana', u'city'),\n",
       " (u'granadilla', u'passion_flower'),\n",
       " (u'granadilla', u'genus_passiflora'),\n",
       " (u'granadilla', u'edible_fruit'),\n",
       " (u'granadilla', u'passiflora'),\n",
       " (u'granadilla', u'passionflower_vine'),\n",
       " (u'granadilla', u'passionflower'),\n",
       " (u'granadilla', u'passion_fruit'),\n",
       " (u'autocracy', u'form_of_government'),\n",
       " (u'elizabeth_eisenstein', u'scholarly_person'),\n",
       " (u'elizabeth_eisenstein', u'historiographer'),\n",
       " (u'fire_service', u'government_department'),\n",
       " (u'fire_service', u'dept'),\n",
       " (u'fire_service', u'branch'),\n",
       " (u'fire_service', u'local_department'),\n",
       " (u'stokes_drift', u'speed'),\n",
       " (u'stokes_drift', u'temporal_property'),\n",
       " (u'stokes_drift', u'swiftness'),\n",
       " (u'union_station', u'plaza'),\n",
       " (u'union_station', u'transportation_stop'),\n",
       " (u'union_station', u'the_mall'),\n",
       " (u'union_station', u'shopping_centre'),\n",
       " (u'union_station', u'railroad_terminal'),\n",
       " (u'union_station', u'train_station'),\n",
       " (u'union_station', u'parkland'),\n",
       " (u'union_station', u'bus_stop'),\n",
       " (u'union_station', u'depot'),\n",
       " (u'union_station', u'station'),\n",
       " (u'union_station', u'eating_house'),\n",
       " (u'union_station', u'public_park'),\n",
       " (u'union_station', u'railroad_station'),\n",
       " (u'union_station', u'stop'),\n",
       " (u'union_station', u'piece_of_land'),\n",
       " (u'union_station', u'public_building'),\n",
       " (u'union_station', u'metro_station'),\n",
       " (u'union_station', u'railway_station'),\n",
       " (u'union_station', u'shopping_precinct'),\n",
       " (u'union_station', u'shopping_mall'),\n",
       " (u'union_station', u'restaurant'),\n",
       " (u'union_station', u'shopping_center'),\n",
       " (u'union_station', u'mercantile_establishment'),\n",
       " (u'union_station', u'parcel_of_land'),\n",
       " (u'union_station', u'mall'),\n",
       " (u'union_station', u'train_depot'),\n",
       " (u'capability', u'susceptibility'),\n",
       " (u'capability', u'state'),\n",
       " (u'capability', u'technical_specification'),\n",
       " (u'transmission', u'sending'),\n",
       " (u'cnidaria', u'animal'),\n",
       " (u'turbofan', u'engine'),\n",
       " (u'turbofan', u'jet_plane'),\n",
       " (u'turbofan', u'heavier-than-air_craft'),\n",
       " (u'turbofan', u'internal-combustion_engine'),\n",
       " (u'turbofan', u'multi-cylinder_engine'),\n",
       " (u'turbofan', u'piston_engine'),\n",
       " (u'turbofan', u'fixed-wing_aircraft'),\n",
       " (u'turbofan', u'engine_configuration'),\n",
       " (u'turbofan', u'aircraft'),\n",
       " (u'turbofan', u'plane'),\n",
       " (u'turbofan', u'mode_of_transport'),\n",
       " (u'turbofan', u'turbine'),\n",
       " (u'turbofan', u'airplane'),\n",
       " ...]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.where(km.predict(train_offsets) == 0)\n",
    "map(lambda x: (data.train_query[x], data.train_hyper[x]), np.where(km.predict(train_offsets) == 4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement training algorithm modification to deal with clusters\n",
    "def train_on_clusters(models,      # the clustered models on which parameters will be learnt\n",
    "                      epochs,      # number of epochs to run          \n",
    "                      batch_size,  # size of mini-batch \n",
    "                      m,           # number of negative samples\n",
    "                      #data,        # data required for training                              \n",
    "                      neg_strategy\n",
    "                     ):                                   \n",
    "    \n",
    "    # train algorithm\n",
    "    for epoch in range(epochs):\n",
    "        # train each model on their corresponding data\n",
    "        for model in models:            \n",
    "            print (\"Doing model: \", model.cluster_id, \"; epoch: \", epoch)\n",
    "            # reset loss\n",
    "            model.loss = 0.\n",
    "            model.test_loss = 0.\n",
    "            \n",
    "            samples = np.arange(len(model.train_query_seq))\n",
    "            validation_samples = np.arange(len(model.valid_query_seq))\n",
    "                                        \n",
    "            np.random.shuffle(samples)                    \n",
    "                        \n",
    "            for b in range(0, len(samples), batch_size):    \n",
    "                if ((b + 1) % 500) == 0:\n",
    "                    print ('Model: ', model.cluster_id, '; processed ', idx+1, 'samples.')\n",
    "                                    \n",
    "                batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "                    extend_batch_with_negatives(model.train_query_seq[b:b + batch_size], \n",
    "                                                model.train_hyper_seq[b:b + batch_size],\n",
    "                                                neg_strategy,\n",
    "                                                model.tokenizer, m\n",
    "                                               )  \n",
    "\n",
    "                #print model.cluster_id, len(batch_X_term)\n",
    "                model.update_loss(model.model.train_on_batch([batch_X_term, batch_X_hyper], batch_y_label)[0])                                \n",
    "                \n",
    "        # validate on entire validation set after training each model\n",
    "            \n",
    "        test_query, test_hyper, test_y_label =\\\n",
    "            extend_batch_with_negatives(model.valid_query_seq, \n",
    "                                        model.valid_hyper_seq,\n",
    "                                        neg_strategy,\n",
    "                                        model.tokenizer, m\n",
    "                                       )  \n",
    "        #batch_label = [1.] * batch_query.shape[0]\n",
    "        for q, h, l in zip(test_query, test_hyper, test_y_label):                                    \n",
    "            test_losses = list(map(lambda c: c.model.test_on_batch([q, h], [l])[0], models))\n",
    "            best_cluster = np.argmin(test_losses)\n",
    "            models[best_cluster].update_test_loss(test_losses[best_cluster])\n",
    "                            \n",
    "        print('Epoch:', epoch+1,\\\n",
    "              'Loss:', np.mean([model.loss for model in models]),\\\n",
    "              'Test Loss:', np.mean([model.test_loss for model in models]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise clusters\n",
    "embeddings_layer = get_embeddings_model(dim=data.embeddings_dim, embedding_matrix=data.embedding_matrix)\n",
    "\n",
    "cluster_list = []\n",
    "cluster_predictions = km.predict(train_offsets)\n",
    "\n",
    "\n",
    "\n",
    "for c in range(cluster_k):\n",
    "    cluster_list.append(ClusterHybrid(cluster=c, \n",
    "                                      cluster_predictions = cluster_predictions, \n",
    "                                      orig_data = data, \n",
    "                                      phi_init = random_identity, \n",
    "                                      sigmoid_kernel_constraint = None,\n",
    "                                      embeddings_layer = embeddings_layer,\n",
    "                                      dropout_rate = 0.3, \n",
    "                                      learning_rate = 0.001 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "('Doing model: ', 0, '; epoch: ', 0)\n",
      "('Doing model: ', 1, '; epoch: ', 0)\n",
      "('Doing model: ', 2, '; epoch: ', 0)\n",
      "('Doing model: ', 3, '; epoch: ', 0)\n",
      "('Doing model: ', 4, '; epoch: ', 0)\n",
      "('Epoch:', 1, 'Loss:', 51.20275011062622, 'Test Loss:', 54.16907963752747)\n",
      "('Doing model: ', 0, '; epoch: ', 1)\n",
      "('Doing model: ', 1, '; epoch: ', 1)\n",
      "('Doing model: ', 2, '; epoch: ', 1)\n",
      "('Doing model: ', 3, '; epoch: ', 1)\n",
      "('Doing model: ', 4, '; epoch: ', 1)\n",
      "('Epoch:', 2, 'Loss:', 49.9156032204628, 'Test Loss:', 51.34425748586655)\n",
      "('Doing model: ', 0, '; epoch: ', 2)\n",
      "('Doing model: ', 1, '; epoch: ', 2)\n",
      "('Doing model: ', 2, '; epoch: ', 2)\n",
      "('Doing model: ', 3, '; epoch: ', 2)\n",
      "('Doing model: ', 4, '; epoch: ', 2)\n",
      "('Epoch:', 3, 'Loss:', 47.629679238796236, 'Test Loss:', 47.31288024187088)\n",
      "('Doing model: ', 0, '; epoch: ', 3)\n",
      "('Doing model: ', 1, '; epoch: ', 3)\n",
      "('Doing model: ', 2, '; epoch: ', 3)\n",
      "('Doing model: ', 3, '; epoch: ', 3)\n",
      "('Doing model: ', 4, '; epoch: ', 3)\n",
      "('Epoch:', 4, 'Loss:', 44.66189357638359, 'Test Loss:', 42.583502805233)\n",
      "('Doing model: ', 0, '; epoch: ', 4)\n",
      "('Doing model: ', 1, '; epoch: ', 4)\n",
      "('Doing model: ', 2, '; epoch: ', 4)\n",
      "('Doing model: ', 3, '; epoch: ', 4)\n",
      "('Doing model: ', 4, '; epoch: ', 4)\n",
      "('Epoch:', 5, 'Loss:', 41.36941378712654, 'Test Loss:', 37.747225880622864)\n",
      "('Doing model: ', 0, '; epoch: ', 5)\n",
      "('Doing model: ', 1, '; epoch: ', 5)\n",
      "('Doing model: ', 2, '; epoch: ', 5)\n",
      "('Doing model: ', 3, '; epoch: ', 5)\n",
      "('Doing model: ', 4, '; epoch: ', 5)\n",
      "('Epoch:', 6, 'Loss:', 38.09736560583114, 'Test Loss:', 33.176356403529645)\n",
      "('Doing model: ', 0, '; epoch: ', 6)\n",
      "('Doing model: ', 1, '; epoch: ', 6)\n",
      "('Doing model: ', 2, '; epoch: ', 6)\n",
      "('Doing model: ', 3, '; epoch: ', 6)\n",
      "('Doing model: ', 4, '; epoch: ', 6)\n",
      "('Epoch:', 7, 'Loss:', 34.89849599599838, 'Test Loss:', 29.117837768793105)\n",
      "('Doing model: ', 0, '; epoch: ', 7)\n",
      "('Doing model: ', 1, '; epoch: ', 7)\n",
      "('Doing model: ', 2, '; epoch: ', 7)\n",
      "('Doing model: ', 3, '; epoch: ', 7)\n",
      "('Doing model: ', 4, '; epoch: ', 7)\n",
      "('Epoch:', 8, 'Loss:', 32.04033596813679, 'Test Loss:', 25.581615015119315)\n",
      "('Doing model: ', 0, '; epoch: ', 8)\n",
      "('Doing model: ', 1, '; epoch: ', 8)\n",
      "('Doing model: ', 2, '; epoch: ', 8)\n",
      "('Doing model: ', 3, '; epoch: ', 8)\n",
      "('Doing model: ', 4, '; epoch: ', 8)\n",
      "('Epoch:', 9, 'Loss:', 29.56286123096943, 'Test Loss:', 22.568919814378024)\n",
      "('Doing model: ', 0, '; epoch: ', 9)\n",
      "('Doing model: ', 1, '; epoch: ', 9)\n",
      "('Doing model: ', 2, '; epoch: ', 9)\n",
      "('Doing model: ', 3, '; epoch: ', 9)\n",
      "('Doing model: ', 4, '; epoch: ', 9)\n",
      "('Epoch:', 10, 'Loss:', 27.37933742403984, 'Test Loss:', 20.065976665169)\n"
     ]
    }
   ],
   "source": [
    "# negative sampling options\n",
    "neg_sampling_options = {'synonym':data.synonyms,                                                 \n",
    "                        'random':data.random_words\n",
    "                       }\n",
    "\n",
    "# phi random init options\n",
    "phi_init_options = {'random_plus_identity': random_plus_identity,\n",
    "                    'random_identity': random_identity, \n",
    "                    'random_normal': random_normal}\n",
    "\n",
    "kernel_constraints = {'None': None, 'ForceToOne': ForceToOne()}\n",
    "\n",
    "# positive batch size\n",
    "batch_size = 32\n",
    "\n",
    "# implement mini-batch stochastic training\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# number of negative samples\n",
    "m = 1\n",
    "# number of projections\n",
    "phi_k = 1\n",
    "# train (True) or freeze\n",
    "train_embeddings = False\n",
    "# negative sample strategy\n",
    "negative_option = 'random'\n",
    "# initialise phi strategy\n",
    "phi_init_option = 'random_identity'\n",
    "# constrain LR parameter\n",
    "kernel_constraint_option = 'None'\n",
    "# dropout rate\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "print (\"Start training\")\n",
    "train_on_clusters(cluster_list, epochs, batch_size, m, neg_sampling_options[negative_option])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "('Done', 100)\n",
      "('Done', 200)\n",
      "('Done', 300)\n",
      "('Done', 400)\n",
      "('Done', 500)\n",
      "('Done', 600)\n",
      "('Done', 700)\n",
      "('Done', 800)\n",
      "('Done', 900)\n",
      "('Done', 1000)\n",
      "('Done', 1100)\n",
      "('Done', 1200)\n",
      "('Done', 1300)\n",
      "('Done', 1400)\n",
      "CRIM evaluation:\n",
      "MRR: 0.03292\n",
      "P@1: 0.02402\n",
      "P@5: 0.01587\n",
      "P@10: 0.01505\n"
     ]
    }
   ],
   "source": [
    "print (\"Generating predictions...\")\n",
    "cluster_predictions = predict_cluster_hypernyms(data, cluster_list)\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores((data.test_query, data.test_hyper), cluster_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3763.100256009726 48.5477934544906 [[1.]]\n",
      "1 1261.7794247418642 52.96302166581154 [[1.]]\n",
      "[0.95837396]\n",
      "[0.26617956]\n"
     ]
    }
   ],
   "source": [
    "#cluster_list[0].model.get_layer(name='Phi0').get_weights()[0]\n",
    "\n",
    "for idx, m in enumerate(clusters):\n",
    "    print idx, m.loss, m.test_loss, m.model.get_layer(name='Prediction').get_weights()[0]\n",
    "\n",
    "i = data.tokenizer.word_index['tussle']\n",
    "j = data.tokenizer.word_index['student']\n",
    "for c in clusters:\n",
    "    print c.model.predict([[i], [j]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Clustering Results\n",
    "\n",
    "* batch_size = 32\n",
    "* epochs = 10\n",
    "* m = 1\n",
    "* phi_k = 1\n",
    "* train_embeddings = False\n",
    "* negative_option = 'random'\n",
    "* phi_init_option = 'random_identity'\n",
    "* kernel_constraint_option = 'None'\n",
    "* dropout_rate = 0.3\n",
    "* learning_rate = 0.001\n",
    "\n",
    "* Performed hard clustering using k = 5.  Vector offset used as basis for clustering.\n",
    "\n",
    "Start training\n",
    "('Doing model: ', 0, '; epoch: ', 0)\n",
    "('Doing model: ', 1, '; epoch: ', 0)\n",
    "('Doing model: ', 2, '; epoch: ', 0)\n",
    "('Doing model: ', 3, '; epoch: ', 0)\n",
    "('Doing model: ', 4, '; epoch: ', 0)\n",
    "('Epoch:', 1, 'Loss:', 51.20275011062622, 'Test Loss:', 54.16907963752747)\n",
    "('Doing model: ', 0, '; epoch: ', 1)\n",
    "('Doing model: ', 1, '; epoch: ', 1)\n",
    "('Doing model: ', 2, '; epoch: ', 1)\n",
    "('Doing model: ', 3, '; epoch: ', 1)\n",
    "('Doing model: ', 4, '; epoch: ', 1)\n",
    "('Epoch:', 2, 'Loss:', 49.9156032204628, 'Test Loss:', 51.34425748586655)\n",
    "('Doing model: ', 0, '; epoch: ', 2)\n",
    "('Doing model: ', 1, '; epoch: ', 2)\n",
    "('Doing model: ', 2, '; epoch: ', 2)\n",
    "('Doing model: ', 3, '; epoch: ', 2)\n",
    "('Doing model: ', 4, '; epoch: ', 2)\n",
    "('Epoch:', 3, 'Loss:', 47.629679238796236, 'Test Loss:', 47.31288024187088)\n",
    "('Doing model: ', 0, '; epoch: ', 3)\n",
    "('Doing model: ', 1, '; epoch: ', 3)\n",
    "('Doing model: ', 2, '; epoch: ', 3)\n",
    "('Doing model: ', 3, '; epoch: ', 3)\n",
    "('Doing model: ', 4, '; epoch: ', 3)\n",
    "('Epoch:', 4, 'Loss:', 44.66189357638359, 'Test Loss:', 42.583502805233)\n",
    "('Doing model: ', 0, '; epoch: ', 4)\n",
    "('Doing model: ', 1, '; epoch: ', 4)\n",
    "('Doing model: ', 2, '; epoch: ', 4)\n",
    "('Doing model: ', 3, '; epoch: ', 4)\n",
    "('Doing model: ', 4, '; epoch: ', 4)\n",
    "('Epoch:', 5, 'Loss:', 41.36941378712654, 'Test Loss:', 37.747225880622864)\n",
    "('Doing model: ', 0, '; epoch: ', 5)\n",
    "('Doing model: ', 1, '; epoch: ', 5)\n",
    "('Doing model: ', 2, '; epoch: ', 5)\n",
    "('Doing model: ', 3, '; epoch: ', 5)\n",
    "('Doing model: ', 4, '; epoch: ', 5)\n",
    "('Epoch:', 6, 'Loss:', 38.09736560583114, 'Test Loss:', 33.176356403529645)\n",
    "('Doing model: ', 0, '; epoch: ', 6)\n",
    "('Doing model: ', 1, '; epoch: ', 6)\n",
    "('Doing model: ', 2, '; epoch: ', 6)\n",
    "('Doing model: ', 3, '; epoch: ', 6)\n",
    "('Doing model: ', 4, '; epoch: ', 6)\n",
    "('Epoch:', 7, 'Loss:', 34.89849599599838, 'Test Loss:', 29.117837768793105)\n",
    "('Doing model: ', 0, '; epoch: ', 7)\n",
    "('Doing model: ', 1, '; epoch: ', 7)\n",
    "('Doing model: ', 2, '; epoch: ', 7)\n",
    "('Doing model: ', 3, '; epoch: ', 7)\n",
    "('Doing model: ', 4, '; epoch: ', 7)\n",
    "('Epoch:', 8, 'Loss:', 32.04033596813679, 'Test Loss:', 25.581615015119315)\n",
    "('Doing model: ', 0, '; epoch: ', 8)\n",
    "('Doing model: ', 1, '; epoch: ', 8)\n",
    "('Doing model: ', 2, '; epoch: ', 8)\n",
    "('Doing model: ', 3, '; epoch: ', 8)\n",
    "('Doing model: ', 4, '; epoch: ', 8)\n",
    "('Epoch:', 9, 'Loss:', 29.56286123096943, 'Test Loss:', 22.568919814378024)\n",
    "('Doing model: ', 0, '; epoch: ', 9)\n",
    "('Doing model: ', 1, '; epoch: ', 9)\n",
    "('Doing model: ', 2, '; epoch: ', 9)\n",
    "('Doing model: ', 3, '; epoch: ', 9)\n",
    "('Doing model: ', 4, '; epoch: ', 9)\n",
    "('Epoch:', 10, 'Loss:', 27.37933742403984, 'Test Loss:', 20.065976665169)\n",
    "\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.03292\n",
    "P@1: 0.02402\n",
    "P@5: 0.01587\n",
    "P@10: 0.01505\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* We could try to train a simple model for a few epochs.  \n",
    "* Once that is done we can refine the results by creating clusters, each initialised with the phi weights from the previous attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YamaneCluster(ClusterHybrid):\n",
    "    def __init__(self, phi_init, \n",
    "                 embeddings_layer, embeddings_dim,\n",
    "                 sigmoid_kernel_constraint, \n",
    "                 dropout_rate, learning_rate):        \n",
    "        \n",
    "        # create Keras model\n",
    "        self.model = self._init_model(phi_init = phi_init, \n",
    "                                      embeddings_layer = embeddings_layer,\n",
    "                                      embeddings_dim = embeddings_dim,\n",
    "                                      sigmoid_kernel_constraint = sigmoid_kernel_constraint,\n",
    "                                      dropout_rate  = dropout_rate,\n",
    "                                      learning_rate = learning_rate)\n",
    "        # initialise variables     \n",
    "        self.epoch_count = 0\n",
    "        self.loss = 0.\n",
    "        self.test_loss = 0.\n",
    "        self.mrr = []\n",
    "    \n",
    "    def increment_epoch(self):\n",
    "        self.epoch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yamane_train(\n",
    "    epochs,      # number of epochs to run\n",
    "    m,           # number of negative samples\n",
    "    data,        # class instance containing all the data required for training/testing        \n",
    "    embedding_layer,     # shared embeddings layer\n",
    "    threshold    = 0.15,     # threshold; similarity below this score will trigger new cluster\n",
    "    negative_option = 'random', # pass dictionary of random terms \n",
    "    phi_init_option = None,     # phi dense layer initialisation strategy\n",
    "    sigmoid_constraint_option = 'None',\n",
    "    dropout_rate = 0.,\n",
    "    learning_rate = 0.001,\n",
    "    cluster_max = 5\n",
    "): \n",
    "    \n",
    "    phi_init_options = {'random_identity': random_identity, \n",
    "                        'random_normal': random_normal, \n",
    "                        'random_plus_identity': random_plus_identity}\n",
    "    neg_sampling_options = {'synonym':data.synonyms, 'random':data.random_words}\n",
    "    sigmoid_constraint_options = {'ForceToOne': ForceToOne(), 'None': None}\n",
    "    \n",
    "    sigmoid_kernel_constraint = sigmoid_constraint_options[sigmoid_constraint_option]\n",
    "    \n",
    "    neg_strategy = neg_sampling_options[negative_option]\n",
    "            \n",
    "    # create sequences\n",
    "    # we have two sets of inputs: one for training query and hypernym terms;\n",
    "    #                             another for the validation query/hyper terms;\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    #term_test_seq = data.tokenizer.texts_to_sequences(data.valid_query)\n",
    "    #hyper_test_seq = data.tokenizer.texts_to_sequences(data.valid_hyper)\n",
    "    \n",
    "    # convert all to arrays\n",
    "    #term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq =\\\n",
    "    #[np.asarray(x, dtype='int32') for x in [term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq]]\n",
    "    \n",
    "    term_train_seq, hyper_train_seq = [np.asarray(x, dtype='int32') for x in [term_train_seq, hyper_train_seq]]\n",
    "            \n",
    "    # this list stores which cluster each training sequence pertains to\n",
    "    sample_clusters = np.zeros(len(term_train_seq), dtype='int32')\n",
    "    \n",
    "    print (\"m: \", m, \"lambda: \", threshold, \"max epoch per cluster: \", epochs, \n",
    "           \"Negative sampling: \", negative_option, \"Phi Init: \", phi_init_option,\n",
    "           \"sigmoid_kernel_constraint: \", sigmoid_constraint_option, \n",
    "           \"dropout: \", dropout_rate, \"learning_rate: \", learning_rate, \n",
    "           \"cluster_max: \", cluster_max          \n",
    "          )\n",
    "    \n",
    "    \n",
    "    print (\"Sample clusters size: \", len(sample_clusters))\n",
    "    # list containing 1 model per cluster\n",
    "    clusters = []    \n",
    "        \n",
    "    clusters.append(YamaneCluster(phi_init = phi_init_options[phi_init_option],\n",
    "                                  embeddings_layer = embedding_layer,\n",
    "                                  embeddings_dim = data.embeddings_dim,\n",
    "                                  sigmoid_kernel_constraint = sigmoid_kernel_constraint,\n",
    "                                  dropout_rate = dropout_rate,\n",
    "                                  learning_rate = learning_rate))\n",
    "                    \n",
    "    # get training set indices\n",
    "    indices = np.arange(len(term_train_seq))  \n",
    "    \n",
    "    # get test set indices\n",
    "    #test_indices = np.arange(len(term_test_seq))\n",
    "            \n",
    "    # initialise each training sample to cluster 0\n",
    "    sample_clusters[indices] = 0        \n",
    "    \n",
    "    # seed random generator\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # indicator of \"current\" sample cluster index\n",
    "    z_i = 0\n",
    "                    \n",
    "    while np.min([c.epoch_count for c in clusters]) < epochs:\n",
    "        # reset loss for each cluster                        \n",
    "        for c in clusters:\n",
    "            if c.epoch_count < epochs:                \n",
    "                c.loss = 0.\n",
    "            c.test_loss = 0.                \n",
    "        \n",
    "        # shuffle indices every epoch\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # train algorithm by stochastic gradient descent, one sample at a time\n",
    "        # learn 1 matrix of first epoch only\n",
    "        for idx, i in enumerate(indices):                        \n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print (\"Processed \", idx+1, \"samples...\")\n",
    "            \n",
    "            # calculate similarity on all clusters\n",
    "            sim = list(map(lambda x: x.model.predict([term_train_seq[i], hyper_train_seq[i]]), clusters))            \n",
    "            max_sim = np.argmax(sim)\n",
    "            #print \"Term:\", tokenizer.index_word[term_train_seq[i][0]], 'Hyper:', tokenizer.index_word[hyper_train_seq[i][0]], \"Max Similarity cluster:\", max_sim, \"(sim = %0.8f)\" % (sim[max_sim])\n",
    "            # limit cluster creation to a max of 25.\n",
    "            if ((clusters[0].epoch_count > 0) and (sim[max_sim] < threshold) and (len(clusters) < cluster_max)): \n",
    "                # add new cluster to list of clusters\n",
    "                print data.tokenizer.index_word[term_train_seq[i][0]], data.tokenizer.index_word[hyper_train_seq[i][0]]\n",
    "                print max_sim, sim[max_sim]\n",
    "                clusters.append(YamaneCluster(phi_init = phi_init_options[phi_init_option],\n",
    "                                               embeddings_layer = embedding_layer,\n",
    "                                               embeddings_dim = data.embeddings_dim,\n",
    "                                               sigmoid_kernel_constraint = sigmoid_kernel_constraint,\n",
    "                                               dropout_rate = dropout_rate,\n",
    "                                               learning_rate = learning_rate))\n",
    "                \n",
    "                # assign current cluster index to latest model\n",
    "                z_i = len(clusters) - 1\n",
    "                sample_clusters[i] = z_i\n",
    "            else:            \n",
    "                z_i = max_sim\n",
    "                sample_clusters[i] = z_i                \n",
    "                        \n",
    "            # if current cluster reached/exceeded epoch count, skip current sample (i.e don't update cluster)\n",
    "            if clusters[z_i].epoch_count < epochs:                                            \n",
    "                # extend samples in cluster with negative samples\n",
    "                batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "                    extend_batch_with_negatives(term_train_seq[i], \n",
    "                                                hyper_train_seq[i],\n",
    "                                                neg_strategy,\n",
    "                                                data.tokenizer, m\n",
    "                                               )  \n",
    "\n",
    "                # update parameters of cluster \n",
    "                clusters[z_i].update_loss(\n",
    "                    clusters[z_i].model.train_on_batch([batch_X_term, batch_X_hyper], batch_y_label)[0]\n",
    "                )\n",
    "        \n",
    "            ####################### END OF EPOCH #######################\n",
    "        \n",
    "        # measure test loss at end of epoch\n",
    "        # every 100 samples (and updates are processed), we will test performance on validation set\n",
    "        # of 32 randomly chosen samples. We will record test loss of every cluster and report on \n",
    "        # lowest loss                                \n",
    "        \n",
    "        #batch_query, batch_hyper = term_test_seq[test_indices[:32]], hyper_test_seq[test_indices[:32]]\n",
    "        \n",
    "        #batch_query, batch_hyper, test_y_label =\\\n",
    "        #    extend_batch_with_negatives(term_test_seq, \n",
    "        #                                hyper_test_seq,\n",
    "        #                                neg_strategy,\n",
    "        #                                data.tokenizer, m\n",
    "        #                               )  \n",
    "                \n",
    "        #for q, h, l in zip(batch_query, batch_hyper, test_y_label):                                    \n",
    "        #    test_losses = list(map(lambda c: c.model.test_on_batch([q, h], [l])[0], clusters))\n",
    "        #    best_cluster = np.argmin(test_losses)\n",
    "        #    clusters[best_cluster].update_test_loss(\n",
    "        #        test_losses[best_cluster]\n",
    "        #    )                    \n",
    "        \n",
    "        # instead of test loss, measure MRR as a more indicative validation metric\n",
    "        print (\"Running evaluation on trial data set...\")\n",
    "        predictions = predict_cluster_hypernyms(data.valid_query, data.tokenizer, clusters)\n",
    "        _, all_scores = get_evaluation_scores((data.valid_query, data.valid_hyper), predictions)\n",
    "        mrr = round(sum([score_list[0] for score_list in all_scores]) / len(all_scores), 5)\n",
    "        clusters[0].mrr.append(mrr)\n",
    "        \n",
    "        # increase epoch count for clusters\n",
    "        for cluster in clusters:            \n",
    "            cluster.epoch_count += 1\n",
    "                \n",
    "        print('Epoch:', max([c.epoch_count for c in clusters]), 'Cluster #:', len(clusters) ,\n",
    "              'Loss:', np.mean([c.loss for c in clusters]),\n",
    "              'Test MRR:', mrr)\n",
    "    return clusters, sample_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2018-12-12 21:48:50.279247\n",
      "('m: ', 10, 'lambda: ', 0.15, 'max epoch per cluster: ', 10, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_plus_identity', 'sigmoid_kernel_constraint: ', 'ForceToOne', 'dropout: ', 0.3, 'learning_rate: ', 0.001, 'cluster_max: ', 25)\n",
      "('Sample clusters size: ', 11779)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 1, 'Cluster #:', 1, 'Loss:', 2645.7672815788537, 'Test MRR:', 0.10333)\n",
      "coalition annotation\n",
      "0 [[0.14232686]]\n",
      "battersea_arts_centre constructed_structure\n",
      "1 [[0.12914188]]\n",
      "lyceum constructed_structure\n",
      "1 [[0.14685696]]\n",
      "('Processed ', 1000, 'samples...')\n",
      "storyteller beguiler\n",
      "1 [[0.14815803]]\n",
      "paris center\n",
      "2 [[0.146684]]\n",
      "('Processed ', 2000, 'samples...')\n",
      "union_station moving-picture_show\n",
      "5 [[0.11579892]]\n",
      "world wonder\n",
      "1 [[0.11191753]]\n",
      "('Processed ', 3000, 'samples...')\n",
      "atmosphere magnitude_relation\n",
      "5 [[0.13706593]]\n",
      "true_life moving-picture_show\n",
      "8 [[0.12733255]]\n",
      "sight perspective\n",
      "9 [[0.14652736]]\n",
      "('Processed ', 4000, 'samples...')\n",
      "young inebriant\n",
      "8 [[0.14763217]]\n",
      "('Processed ', 5000, 'samples...')\n",
      "lucrezia_borgia moving-picture_show\n",
      "8 [[0.11355487]]\n",
      "tulsa moving-picture_show\n",
      "12 [[0.08012623]]\n",
      "('Processed ', 6000, 'samples...')\n",
      "under_the_moon series\n",
      "13 [[0.13277091]]\n",
      "dale natural_depression\n",
      "14 [[0.14759865]]\n",
      "dale vale\n",
      "15 [[0.1460675]]\n",
      "('Processed ', 7000, 'samples...')\n",
      "seaquake trouble\n",
      "16 [[0.14310668]]\n",
      "scott hamlet\n",
      "14 [[0.13601676]]\n",
      "('Processed ', 8000, 'samples...')\n",
      "hamlet moving-picture_show\n",
      "18 [[0.11416043]]\n",
      "sight moving-picture_show\n",
      "19 [[0.08516396]]\n",
      "('Processed ', 9000, 'samples...')\n",
      "power probality\n",
      "19 [[0.1498438]]\n",
      "touch_and_go moving-picture_show\n",
      "21 [[0.14001596]]\n",
      "('Processed ', 10000, 'samples...')\n",
      "star separable_space\n",
      "20 [[0.1300958]]\n",
      "scott mathematical_relation\n",
      "23 [[0.12929982]]\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 2, 'Cluster #:', 25, 'Loss:', 156.81297644650564, 'Test MRR:', 0.05333)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 3, 'Cluster #:', 25, 'Loss:', 87.73115104873665, 'Test MRR:', 0.06417)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 4, 'Cluster #:', 25, 'Loss:', 73.36540189261082, 'Test MRR:', 0.05733)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 5, 'Cluster #:', 25, 'Loss:', 64.38504860263784, 'Test MRR:', 0.059)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 6, 'Cluster #:', 25, 'Loss:', 56.81815980266779, 'Test MRR:', 0.078)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 7, 'Cluster #:', 25, 'Loss:', 51.290908942183016, 'Test MRR:', 0.094)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 8, 'Cluster #:', 25, 'Loss:', 47.23172779464861, 'Test MRR:', 0.08517)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 9, 'Cluster #:', 25, 'Loss:', 43.46400957543636, 'Test MRR:', 0.08822)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 10, 'Cluster #:', 25, 'Loss:', 40.14030005659443, 'Test MRR:', 0.07756)\n",
      "('Processed ', 1000, 'samples...')\n",
      "('Processed ', 2000, 'samples...')\n",
      "('Processed ', 3000, 'samples...')\n",
      "('Processed ', 4000, 'samples...')\n",
      "('Processed ', 5000, 'samples...')\n",
      "('Processed ', 6000, 'samples...')\n",
      "('Processed ', 7000, 'samples...')\n",
      "('Processed ', 8000, 'samples...')\n",
      "('Processed ', 9000, 'samples...')\n",
      "('Processed ', 10000, 'samples...')\n",
      "('Processed ', 11000, 'samples...')\n",
      "Running evaluation on trial data set...\n",
      "('Epoch:', 11, 'Cluster #:', 25, 'Loss:', 38.91978675389197, 'Test MRR:', 0.06556)\n",
      "Training concluded at: 2018-12-12 23:16:56.594990\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# initialise embedding later which will be shared among all clusters\n",
    "embedding_layer = get_embeddings_model(dim=data.embeddings_dim, embedding_matrix=data.embedding_matrix)\n",
    "epochs = 10\n",
    "m = 10\n",
    "\n",
    "print (\"Training started at: %s\" %  (datetime.datetime.now()))\n",
    "clusters, sample_clusters =\\\n",
    "    yamane_train(epochs, m, \n",
    "                 data,\n",
    "                 embedding_layer,\n",
    "                 threshold = 0.15,\n",
    "                 negative_option = 'random',\n",
    "                 phi_init_option = 'random_plus_identity',\n",
    "                 sigmoid_constraint_option = 'ForceToOne',\n",
    "                 dropout_rate = 0.3,\n",
    "                 learning_rate = 0.001,\n",
    "                 cluster_max = 25\n",
    "                )\n",
    "\n",
    "print (\"Training concluded at: %s\" % (datetime.datetime.now()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353.48588428401854 0.0 11 [0.10333, 0.05333, 0.06417, 0.05733, 0.059, 0.078, 0.094, 0.08517, 0.08822, 0.07756, 0.06556]\n",
      "38.093901408836246 0.0 10 []\n",
      "13.049090251326561 0.0 10 []\n",
      "47.21333260461688 0.0 10 []\n",
      "19.317077357321978 0.0 10 []\n",
      "30.870450855232775 0.0 10 []\n",
      "12.441514313220978 0.0 10 []\n",
      "12.182713583111763 0.0 10 []\n",
      "15.092968666460365 0.0 10 []\n",
      "20.62631884170696 0.0 10 []\n",
      "16.031787231564522 0.0 10 []\n",
      "7.054915741086006 0.0 10 []\n",
      "9.180294819176197 0.0 10 []\n",
      "21.313419584184885 0.0 10 []\n",
      "65.07340764114633 0.0 10 []\n",
      "11.535719199106097 0.0 10 []\n",
      "13.696264486759901 0.0 10 []\n",
      "47.03833513520658 0.0 10 []\n",
      "26.871564440894872 0.0 10 []\n",
      "12.247797943651676 0.0 10 []\n",
      "68.35207805968821 0.0 10 []\n",
      "8.792612202465534 0.0 10 []\n",
      "13.14311645552516 0.0 10 []\n",
      "63.05965260975063 0.0 10 []\n",
      "27.230451131239533 0.0 10 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'blackfly', u'insect'),\n",
       " (u'turonian', u'physical_property'),\n",
       " (u'turonian', u'geological_period'),\n",
       " (u'turonian', u'geological_time'),\n",
       " (u'turonian', u'geologic_time'),\n",
       " (u'tropical_storm', u'atmosphere'),\n",
       " (u'tropical_storm', u'windstorm'),\n",
       " (u'tropical_storm', u'violent_storm'),\n",
       " (u'tropical_storm', u'atmospheric_state'),\n",
       " (u'tropical_storm', u'storm_damage'),\n",
       " (u'tropical_storm', u'atmospheric_phenomenon'),\n",
       " (u'tropical_storm', u'storm'),\n",
       " (u'tropical_storm', u'cyclone'),\n",
       " (u'tropical_storm', u'natural_phenomenon'),\n",
       " (u'tropical_storm', u'tempest'),\n",
       " (u'tropical_storm', u'wind'),\n",
       " (u'militarization', u'social_control'),\n",
       " (u'pollution', u'environmental_condition'),\n",
       " (u'photomicrograph', u'digital_image'),\n",
       " (u'swamp_gum', u'plant'),\n",
       " (u'swamp_gum', u'woody_plant'),\n",
       " (u'swamp_gum', u'eucalypt'),\n",
       " (u'song', u'sound'),\n",
       " (u'song', u'work_of_art'),\n",
       " (u'song', u'musical_composition'),\n",
       " (u'song', u'vocal_music'),\n",
       " (u'song', u'musical_work'),\n",
       " (u'song', u'piece_of_music'),\n",
       " (u'song', u'human_language'),\n",
       " (u'song', u'speech'),\n",
       " (u'wing', u'emblem'),\n",
       " (u'wing', u'animal'),\n",
       " (u'hero', u'piece_of_music'),\n",
       " (u'hero', u'video'),\n",
       " (u'hero', u'film'),\n",
       " (u'hero', u'periodical_publication'),\n",
       " (u'hero', u'musical_work'),\n",
       " (u'hero', u'personal_name'),\n",
       " (u'hero', u'tv_station'),\n",
       " (u'hero', u'tv_show'),\n",
       " (u'hero', u'tv_series'),\n",
       " (u'hero', u'literary_composition'),\n",
       " (u'hero', u'mass_media'),\n",
       " (u'hero', u'periodical_literature'),\n",
       " (u'hero', u'adventure_story'),\n",
       " (u'hero', u'honorific'),\n",
       " (u'hero', u'television_show'),\n",
       " (u'hero', u'bollywood'),\n",
       " (u'hero', u'tv_program'),\n",
       " (u'hero', u'film_genre'),\n",
       " (u'hero', u'documentary'),\n",
       " (u'hero', u'philology'),\n",
       " (u'hero', u'television_program'),\n",
       " (u'hero', u'drama'),\n",
       " (u'hero', u'person'),\n",
       " (u'hero', u'broadcaster'),\n",
       " (u'hero', u'movie'),\n",
       " (u'hero', u'visual_art'),\n",
       " (u'hero', u'motion_picture'),\n",
       " (u'hero', u'video_game'),\n",
       " (u'cumulus', u'natural_phenomenon'),\n",
       " (u'cumulus', u'atmospheric_phenomenon'),\n",
       " (u'silver', u'historic_house'),\n",
       " (u'silver', u'tableware'),\n",
       " (u'silver', u'dishware'),\n",
       " (u'silver', u'silverware'),\n",
       " (u'silver', u'medal'),\n",
       " (u'silver', u'prize'),\n",
       " (u'silver', u'precious_metal'),\n",
       " (u'silver', u'cutlery'),\n",
       " (u'silver', u'contestant'),\n",
       " (u'sand_verbena', u'wildflower'),\n",
       " (u'carly_fiorina', u'person'),\n",
       " (u'louse', u'serial_publication'),\n",
       " (u'louse', u'insect'),\n",
       " (u'navigator', u'explorer'),\n",
       " (u'navigator', u'travel'),\n",
       " (u'navigator', u'crewmember'),\n",
       " (u'navigator', u'crew_member'),\n",
       " (u'erna_brodber', u'person'),\n",
       " (u'erna_brodber', u'writer'),\n",
       " (u'raoul_dufy', u'painter'),\n",
       " (u'gasworks', u'manufacturing_plant'),\n",
       " (u'gasworks', u'factory'),\n",
       " (u'gasworks', u'workplace'),\n",
       " (u'gasworks', u'industrial_plant'),\n",
       " (u'farming', u'economic_sector'),\n",
       " (u'farming', u'manufacture'),\n",
       " (u'skara_brae', u'archaeological_site'),\n",
       " (u'skara_brae', u'piece_of_land'),\n",
       " (u'skara_brae', u'mapmaking'),\n",
       " (u'skara_brae', u'site'),\n",
       " (u'skara_brae', u'land_site'),\n",
       " (u'murder_one', u'electronic_media'),\n",
       " (u'murder_one', u'video_recording'),\n",
       " (u'murder_one', u'video'),\n",
       " (u'murder_one', u'communication_medium'),\n",
       " (u'murder_one', u'film'),\n",
       " (u'murder_one', u'movie'),\n",
       " (u'murder_one', u'tv_show'),\n",
       " (u'murder_one', u'tv_series'),\n",
       " (u'murder_one', u'moving_picture'),\n",
       " (u'murder_one', u'television_show'),\n",
       " (u'murder_one', u'tv_program'),\n",
       " (u'murder_one', u'television_series'),\n",
       " (u'murder_one', u'television_program'),\n",
       " (u'murder_one', u'recording'),\n",
       " (u'murder_one', u'motion_picture'),\n",
       " (u'emmett_tyrrell', u'newspaper_columnist'),\n",
       " (u'emmett_tyrrell', u'writer'),\n",
       " (u'emmett_tyrrell', u'person'),\n",
       " (u'emmett_tyrrell', u'columnist'),\n",
       " (u'emmett_tyrrell', u'editorialist'),\n",
       " (u'emmett_tyrrell', u'media_professional'),\n",
       " (u'emmett_tyrrell', u'journalist'),\n",
       " (u'kim_il-sung_university', u'ceremony'),\n",
       " (u'kim_il-sung_university', u'school'),\n",
       " (u'kim_il-sung_university', u'academy'),\n",
       " (u'kim_il-sung_university', u'educational_institution'),\n",
       " (u'chess', u'party_game'),\n",
       " (u'chess', u'board_game'),\n",
       " (u'sachs_harbour', u'city'),\n",
       " (u'myrdal', u'economist'),\n",
       " (u'myrdal', u'politician'),\n",
       " (u'myrdal', u'person'),\n",
       " (u'floorshow', u'entertainment'),\n",
       " (u'lion', u'big_cat'),\n",
       " (u'lion', u'animal'),\n",
       " (u'crowds_and_power', u'book'),\n",
       " (u'crowds_and_power', u'piece_of_work'),\n",
       " (u'homophone', u'homonym'),\n",
       " (u'availability', u'convenience'),\n",
       " (u'a1_motorway', u'freeway'),\n",
       " (u'a1_motorway', u'thruway'),\n",
       " (u'a1_motorway', u'route'),\n",
       " (u'a1_motorway', u'transport_infrastructure'),\n",
       " (u'a1_motorway', u'main_road'),\n",
       " (u'a1_motorway', u'motorway'),\n",
       " (u'a1_motorway', u'throughway'),\n",
       " (u'a1_motorway', u'state_highway'),\n",
       " (u'a1_motorway', u'superhighway'),\n",
       " (u'a1_motorway', u'turnpike'),\n",
       " (u'a1_motorway', u'expressway'),\n",
       " (u'deck', u'work_of_art'),\n",
       " (u'deck', u'upper_deck'),\n",
       " (u'jet_stream', u'weather'),\n",
       " (u'jet_stream', u'atmospheric_phenomenon'),\n",
       " (u'jet_stream', u'physical_phenomenon'),\n",
       " (u'jet_stream', u'weather_condition'),\n",
       " (u'jet_stream', u'airstream'),\n",
       " (u'jet_stream', u'wind'),\n",
       " (u'muhammad_ali_jinnah', u'person'),\n",
       " (u'muhammad_ali_jinnah', u'national_leader'),\n",
       " (u'muhammad_ali_jinnah', u'statesman'),\n",
       " (u'muhammad_ali_jinnah', u'politician'),\n",
       " (u'muhammad_ali_jinnah', u'owner'),\n",
       " (u'muhammad_ali_jinnah', u'political_leader'),\n",
       " (u'muhammad_ali_jinnah', u'leader'),\n",
       " (u'habit', u'exercise'),\n",
       " (u'bennington', u'town'),\n",
       " (u'bennington', u'city'),\n",
       " (u'bennington', u'new_england_town'),\n",
       " (u'bennington', u'municipal_corporation'),\n",
       " (u'bennington', u'township'),\n",
       " (u'bennington', u'municipal_government'),\n",
       " (u'subject', u'text'),\n",
       " (u'subject', u'philosophy'),\n",
       " (u'subject', u'textbook'),\n",
       " (u'subject', u'know-how'),\n",
       " (u'subject', u'string_of_words'),\n",
       " (u'subject', u'database'),\n",
       " (u'subject', u'scientific_research'),\n",
       " (u'subject', u'philology'),\n",
       " (u'subject', u'person'),\n",
       " (u'subject', u'political_doctrine'),\n",
       " (u'planetary_science', u'textbook'),\n",
       " (u'planetary_science', u'branch_of_science'),\n",
       " (u'planetary_science', u'astrophysics'),\n",
       " (u'planetary_science', u'physics'),\n",
       " (u'helen_of_troy', u'thespian'),\n",
       " (u'helen_of_troy', u'electronic_media'),\n",
       " (u'helen_of_troy', u'series'),\n",
       " (u'helen_of_troy', u'television_show'),\n",
       " (u'helen_of_troy', u'actor'),\n",
       " (u'helen_of_troy', u'film'),\n",
       " (u'helen_of_troy', u'work_of_art'),\n",
       " (u'helen_of_troy', u'tv_program'),\n",
       " (u'helen_of_troy', u'television_series'),\n",
       " (u'helen_of_troy', u'cinematography'),\n",
       " (u'helen_of_troy', u'miniseries'),\n",
       " (u'helen_of_troy', u'television_program'),\n",
       " (u'helen_of_troy', u'recording'),\n",
       " (u'helen_of_troy', u'person'),\n",
       " (u'helen_of_troy', u'tv_show'),\n",
       " (u'helen_of_troy', u'tv_series'),\n",
       " (u'helen_of_troy', u'visual_art'),\n",
       " (u'helen_of_troy', u'motion_picture'),\n",
       " (u'disservice', u'wrongful_conduct'),\n",
       " (u'disservice', u'misconduct'),\n",
       " (u'disservice', u'wrongdoing'),\n",
       " (u'owner-occupier', u'resident'),\n",
       " (u'owner-occupier', u'occupier'),\n",
       " (u'owner-occupier', u'occupant'),\n",
       " (u'fault', u'equipment_failure'),\n",
       " (u'north_carolina', u'province'),\n",
       " (u'north_carolina', u'city'),\n",
       " (u'north_carolina', u'american_state'),\n",
       " (u'north_carolina', u'state'),\n",
       " (u'north_carolina', u'overseas_territory'),\n",
       " (u'north_carolina', u'u.s._state'),\n",
       " (u'contribution', u'giving'),\n",
       " (u'contribution', u'effort'),\n",
       " (u'forest_way', u'parkland'),\n",
       " (u'forest_way', u'parcel_of_land'),\n",
       " (u'forest_way', u'green_area'),\n",
       " (u'forest_way', u'park'),\n",
       " (u'forest_way', u'conservation_area'),\n",
       " (u'forest_way', u'piece_of_land'),\n",
       " (u'forest_way', u'tract'),\n",
       " (u'forest_way', u'protected_area'),\n",
       " (u'forest_way', u'country_park'),\n",
       " (u'starling', u'bird'),\n",
       " (u'starling', u'animal'),\n",
       " (u'spinal_fusion', u'specialist'),\n",
       " (u'spinal_fusion', u'health_professional'),\n",
       " (u'spinal_fusion', u'physician'),\n",
       " (u'spinal_fusion', u'surgical_procedure'),\n",
       " (u'spinal_fusion', u'health_care_provider'),\n",
       " (u'spinal_fusion', u'bioscience'),\n",
       " (u'spinal_fusion', u'medicine'),\n",
       " (u'spinal_fusion', u'health_care'),\n",
       " (u'spinal_fusion', u'surgery'),\n",
       " (u'spinal_fusion', u'life_science'),\n",
       " (u'spinal_fusion', u'orthopedic_surgery'),\n",
       " (u'spinal_fusion', u'md'),\n",
       " (u'spinal_fusion', u'orthopaedics'),\n",
       " (u'spinal_fusion', u'orthopedist'),\n",
       " (u'spinal_fusion', u'doctor'),\n",
       " (u'spinal_fusion', u'medical_doctor'),\n",
       " (u'spinal_fusion', u'surgical_operation'),\n",
       " (u'spinal_fusion', u'medical_procedure'),\n",
       " (u'spinal_fusion', u'orthopaedist'),\n",
       " (u'spinal_fusion', u'orthopedics'),\n",
       " (u'spinal_fusion', u'specialist_degree'),\n",
       " (u'spinal_fusion', u'treatment'),\n",
       " (u'spinal_fusion', u'dr.'),\n",
       " (u'spinal_fusion', u'medical_care'),\n",
       " (u'spinal_fusion', u'practice_of_medicine'),\n",
       " (u'spinal_fusion', u'surgical_process'),\n",
       " (u'spinal_fusion', u'medical_specialist'),\n",
       " (u'trust', u'partnership'),\n",
       " (u'trust', u'private_corporation'),\n",
       " (u'trust', u'corporation'),\n",
       " (u'trust', u'company'),\n",
       " (u'trust', u'syndicate'),\n",
       " (u'trust', u'business_organization'),\n",
       " (u'trust', u'agreement'),\n",
       " (u'trust', u'business_organisation'),\n",
       " (u'trust', u'voluntary_association'),\n",
       " (u'trust', u'privately_held_corporation'),\n",
       " (u'trust', u'fraternity'),\n",
       " (u'trust', u'corp'),\n",
       " (u'trust', u'association'),\n",
       " (u'beneficiary', u'person'),\n",
       " (u'beneficiary', u'natural_person'),\n",
       " (u'beneficiary', u'legal_person'),\n",
       " (u'beneficiary', u'acquirer'),\n",
       " (u'backwash', u'flow'),\n",
       " (u'younger_dryas', u'geological_period'),\n",
       " (u'younger_dryas', u'geological_time'),\n",
       " (u'younger_dryas', u'geologic_time'),\n",
       " (u'younger_dryas', u'state'),\n",
       " (u'harvesting', u'collecting'),\n",
       " (u'harvesting', u'assembling'),\n",
       " (u'harvesting', u'gather'),\n",
       " (u'hosiery', u'clothing'),\n",
       " (u'hosiery', u'footwear'),\n",
       " (u'hosiery', u'clothes'),\n",
       " (u'ph.d.', u'scholar'),\n",
       " (u'ph.d.', u'person'),\n",
       " (u'ph.d.', u'honorific'),\n",
       " (u'megabyte', u'unit_of_information'),\n",
       " (u'megabyte', u'byte'),\n",
       " (u'new_zealand', u'civil_authority'),\n",
       " (u'new_zealand', u'city'),\n",
       " (u'new_zealand', u'island_country'),\n",
       " (u'new_zealand', u'national_boundary'),\n",
       " (u'new_zealand', u'estate'),\n",
       " (u'new_zealand', u'state'),\n",
       " (u'new_zealand', u'island'),\n",
       " (u'new_zealand', u'boundary_line'),\n",
       " (u'new_zealand', u'island_nation'),\n",
       " (u'new_zealand', u'public_building'),\n",
       " (u'new_zealand', u'dependent_territory'),\n",
       " (u'new_zealand', u'real_estate'),\n",
       " (u'new_zealand', u'acres'),\n",
       " (u'new_zealand', u'real_property'),\n",
       " (u'newdale', u'city'),\n",
       " (u'belfast_lough', u'body_of_water'),\n",
       " (u'certified_check', u'computer_file'),\n",
       " (u'certified_check', u'bank_check'),\n",
       " (u'certified_check', u'bill_of_exchange'),\n",
       " (u'certified_check', u'written_document'),\n",
       " (u'certified_check', u'cheque'),\n",
       " (u'certified_check', u'check'),\n",
       " (u'certified_check', u'order_of_payment'),\n",
       " (u'playwriting', u'writer'),\n",
       " (u'sajid_mahmood', u'person'),\n",
       " (u'sajid_mahmood', u'sportswoman'),\n",
       " (u'sajid_mahmood', u'competitor'),\n",
       " (u'sajid_mahmood', u'cricketer'),\n",
       " (u'sajid_mahmood', u'sportsperson'),\n",
       " (u'smew', u'waterfowl'),\n",
       " (u'smew', u'waterbird'),\n",
       " (u'smew', u'animal'),\n",
       " (u'biosatellite', u'satellite'),\n",
       " (u'biosatellite', u'computer_code'),\n",
       " (u'biosatellite', u'transportation'),\n",
       " (u'biosatellite', u'means_of_transport'),\n",
       " (u'biosatellite', u'software_application'),\n",
       " (u'biosatellite', u'electronic_computer'),\n",
       " (u'biosatellite', u'spaceship'),\n",
       " (u'passage', u'musical_composition'),\n",
       " (u'passage', u'transportation'),\n",
       " (u'passage', u'journey'),\n",
       " (u'passage', u'traveling'),\n",
       " (u'passage', u'voyage'),\n",
       " (u'passage', u'travel'),\n",
       " (u'passage', u'travelling'),\n",
       " (u'passage', u'religious_ritual'),\n",
       " (u'tax_haven', u'country'),\n",
       " (u'eric_becklin', u'astrophysicist'),\n",
       " (u'eric_becklin', u'person'),\n",
       " (u'bevis_hillier', u'art_historian'),\n",
       " (u'bevis_hillier', u'person'),\n",
       " (u'city_of_whitehorse', u'local_government_area'),\n",
       " (u'city_of_whitehorse', u'land_site'),\n",
       " (u'dormouse', u'animal'),\n",
       " (u'dormouse', u'mammal'),\n",
       " (u'explosion', u'noise'),\n",
       " (u'chromehounds', u'computing_device'),\n",
       " (u'chromehounds', u'computing_system'),\n",
       " (u'chromehounds', u'electronic_game'),\n",
       " (u'chromehounds', u'application_program'),\n",
       " (u'chromehounds', u'computer'),\n",
       " (u'chromehounds', u'computing_machine'),\n",
       " (u'chromehounds', u'videogaming'),\n",
       " (u'chromehounds', u'computer_programme'),\n",
       " (u'chromehounds', u'console'),\n",
       " (u'chromehounds', u'computer_game'),\n",
       " (u'chromehounds', u'software_package'),\n",
       " (u'chromehounds', u'video_game_console'),\n",
       " (u'chromehounds', u'game_console'),\n",
       " (u'chromehounds', u'server'),\n",
       " (u'chromehounds', u'network_host'),\n",
       " (u'chromehounds', u'applications_software'),\n",
       " (u'chromehounds', u'electronic_computer'),\n",
       " (u'chromehounds', u'video_game'),\n",
       " (u'chromehounds', u'software'),\n",
       " (u'cyclone_gafilo', u'atmosphere'),\n",
       " (u'cyclone_gafilo', u'windstorm'),\n",
       " (u'cyclone_gafilo', u'tropical_storm'),\n",
       " (u'cyclone_gafilo', u'violent_storm'),\n",
       " (u'cyclone_gafilo', u'weather'),\n",
       " (u'cyclone_gafilo', u'tropical_depression'),\n",
       " (u'cyclone_gafilo', u'tropical_cyclone'),\n",
       " (u'cyclone_gafilo', u'hurricane'),\n",
       " (u'cyclone_gafilo', u'storm_damage'),\n",
       " (u'cyclone_gafilo', u'atmospheric_phenomenon'),\n",
       " (u'cyclone_gafilo', u'storm'),\n",
       " (u'cyclone_gafilo', u'cyclone'),\n",
       " (u'cyclone_gafilo', u'weather_condition'),\n",
       " (u'cyclone_gafilo', u'wind'),\n",
       " (u'earthquake', u'natural_disaster'),\n",
       " (u'earthquake', u'musical_composition'),\n",
       " (u'earthquake', u'disaster'),\n",
       " (u'earthquake', u'natural_phenomenon'),\n",
       " (u'earthquake', u'catastrophe'),\n",
       " (u'earthquake', u'musical_work'),\n",
       " (u'vendue', u'merchant'),\n",
       " (u'vendue', u'merchandiser'),\n",
       " (u'vendue', u'selling'),\n",
       " (u'vendue', u'merchandising'),\n",
       " (u'vendue', u'sales_agent'),\n",
       " (u'vendue', u'marketing'),\n",
       " (u'vendue', u'sale'),\n",
       " (u'vendue', u'seller'),\n",
       " (u'leaf_curl', u'illness'),\n",
       " (u'leaf_curl', u'plant_disease'),\n",
       " (u'leaf_curl', u'sickness'),\n",
       " (u'leaf_curl', u'disease'),\n",
       " (u'leaf_curl', u'plant_pathology'),\n",
       " (u'crisis', u'state'),\n",
       " (u'curiosa', u'piece_of_work'),\n",
       " (u'rainfall', u'natural_phenomenon'),\n",
       " (u'rainfall', u'weather'),\n",
       " (u'rainfall', u'weather_condition'),\n",
       " (u'rainfall', u'precipitation'),\n",
       " (u'endemic_goitre', u'disease'),\n",
       " (u'endemic_goitre', u'sickness'),\n",
       " (u'club', u'baseball_team'),\n",
       " (u'club', u'voluntary_association'),\n",
       " (u'club', u'olympic_sport'),\n",
       " (u'club', u'sport'),\n",
       " (u'club', u'association'),\n",
       " (u'club', u'business_establishment'),\n",
       " (u'club', u'athlete'),\n",
       " (u'club', u'place_of_business'),\n",
       " (u'curriculum', u'course_of_instruction'),\n",
       " (u'curriculum', u'course_of_study'),\n",
       " (u'alexandre_gustave_eiffel', u'person'),\n",
       " (u'alexandre_gustave_eiffel', u'designer'),\n",
       " (u'alexandre_gustave_eiffel', u'architect'),\n",
       " (u'alexandre_gustave_eiffel', u'civil_engineer'),\n",
       " (u'alexandre_gustave_eiffel', u'engineer'),\n",
       " (u'cat', u'code'),\n",
       " (u'cat', u'utility_program'),\n",
       " (u'cat', u'means_of_transportation'),\n",
       " (u'cat', u'computer_code'),\n",
       " (u'cat', u'means_of_transport'),\n",
       " (u'cat', u'pet'),\n",
       " (u'cat', u'animal_testing'),\n",
       " (u'cat', u'feline'),\n",
       " (u'cat', u'animal'),\n",
       " (u'cat', u'vehicle'),\n",
       " (u'cat', u'animal_experiment'),\n",
       " (u'printing', u'written_language'),\n",
       " (u'printing', u'written_communication'),\n",
       " (u'printing', u'communication_medium'),\n",
       " (u'photojournalism', u'language'),\n",
       " (u'photojournalism', u'short_story'),\n",
       " (u'photojournalism', u'journalism'),\n",
       " (u'photojournalism', u'piece_of_work'),\n",
       " (u'photojournalism', u'mass_media'),\n",
       " (u'photojournalism', u'narrative'),\n",
       " (u'photojournalism', u'print_media'),\n",
       " (u'photojournalism', u'news_media'),\n",
       " (u'photojournalism', u'communication_medium'),\n",
       " (u'photojournalism', u'book'),\n",
       " (u'photojournalism', u'mass_medium'),\n",
       " (u'spaceport', u'installation'),\n",
       " (u'spaceport', u'facility'),\n",
       " (u'till', u'parcel'),\n",
       " (u'horizontal_bar', u'equipment'),\n",
       " (u'horizontal_bar', u'sports_equipment'),\n",
       " (u'parameter', u'computer_code'),\n",
       " (u'parameter', u'technical_specification'),\n",
       " (u'parameter', u'reference'),\n",
       " (u'parameter', u'specifications'),\n",
       " (u'parameter', u'code'),\n",
       " (u'parameter', u'data_type'),\n",
       " (u'parameter', u'computer_address'),\n",
       " (u'cocktail', u'alcoholic_beverage'),\n",
       " (u'sacrifice', u'possession'),\n",
       " (u'sphagnum', u'plant'),\n",
       " (u'noun', u'substantive'),\n",
       " (u'neck', u'body_part'),\n",
       " (u'liquid', u'state_of_matter'),\n",
       " (u'switch', u'computing_device'),\n",
       " (u'switch', u'rail_transportation'),\n",
       " (u'switch', u'electrical_network'),\n",
       " (u'switch', u'electric_circuit'),\n",
       " (u'switch', u'component'),\n",
       " (u'switch', u'computer_circuit'),\n",
       " (u'switch', u'hardware'),\n",
       " (u'switch', u'add-in'),\n",
       " (u'switch', u'circuitry'),\n",
       " (u'switch', u'plug-in'),\n",
       " (u'switch', u'integrated_circuit'),\n",
       " (u'switch', u'card'),\n",
       " (u'switch', u'slot'),\n",
       " (u'switch', u'circuit_board'),\n",
       " (u'switch', u'appliance'),\n",
       " (u'switch', u'semiconductor'),\n",
       " (u'switch', u'electronic_equipment'),\n",
       " (u'switch', u'board'),\n",
       " (u'switch', u'circuit'),\n",
       " (u'switch', u'microcircuit'),\n",
       " (u'switch', u'computer_chip'),\n",
       " (u'switch', u'circuit_card'),\n",
       " (u'metrolink', u'public_transit'),\n",
       " (u'metrolink', u'traveling'),\n",
       " (u'metrolink', u'public_service'),\n",
       " (u'metrolink', u'transport'),\n",
       " (u'metrolink', u'utility'),\n",
       " (u'metrolink', u'bus_service'),\n",
       " (u'metrolink', u'service'),\n",
       " (u'metrolink', u'public_transport'),\n",
       " (u'metrolink', u'corporation'),\n",
       " (u'metrolink', u'travel'),\n",
       " (u'metrolink', u'travelling'),\n",
       " (u'metrolink', u'public_utility'),\n",
       " (u'metrolink', u'public_utility_company'),\n",
       " (u'metrolink', u'transportation'),\n",
       " (u'metrolink', u'passenger_transport'),\n",
       " (u'metrolink', u'bus_company'),\n",
       " (u'metrolink', u'means_of_transport'),\n",
       " (u'metrolink', u'transportation_system'),\n",
       " (u'metrolink', u'transportation_company'),\n",
       " (u'turnstile', u'gate'),\n",
       " (u'leslie_caron', u'person'),\n",
       " (u'grimm', u'electronic_media'),\n",
       " (u'grimm', u'television_show'),\n",
       " (u'grimm', u'government_agency'),\n",
       " (u'grimm', u'film'),\n",
       " (u'grimm', u'work_of_art'),\n",
       " (u'grimm', u'comedy'),\n",
       " (u'grimm', u'television_series'),\n",
       " (u'grimm', u'writer'),\n",
       " (u'grimm', u'social_event'),\n",
       " (u'grimm', u'television_program'),\n",
       " (u'grimm', u'drama'),\n",
       " (u'grimm', u'person'),\n",
       " (u'grimm', u'tv_show'),\n",
       " (u'grimm', u'series'),\n",
       " (u'foreclosure', u'proceeding'),\n",
       " (u'foreclosure', u'due_process'),\n",
       " (u'foreclosure', u'legal_proceeding'),\n",
       " (u'foreclosure', u'proceedings'),\n",
       " (u'foreclosure', u'legal_proceedings'),\n",
       " (u'course', u'guidance'),\n",
       " (u'course', u'itinerary'),\n",
       " (u'course', u'course_of_study'),\n",
       " (u'course', u'navigation'),\n",
       " (u'course', u'computation'),\n",
       " (u'course', u'installation'),\n",
       " (u'course', u'route'),\n",
       " (u'course', u'piloting'),\n",
       " (u'suspension', u'state'),\n",
       " (u'norfolk', u'new_england_town'),\n",
       " (u'norfolk', u'transportation'),\n",
       " (u'norfolk', u'metro_station'),\n",
       " (u'norfolk', u'government'),\n",
       " (u'norfolk', u'railroad_station'),\n",
       " (u'norfolk', u'train_station'),\n",
       " (u'norfolk', u'county'),\n",
       " (u'norfolk', u'election_district'),\n",
       " (u'norfolk', u'station'),\n",
       " (u'norfolk', u'ship'),\n",
       " (u'norfolk', u'transportation_stop'),\n",
       " (u'norfolk', u'precinct'),\n",
       " (u'norfolk', u'railway_station'),\n",
       " (u'norfolk', u'independent_city'),\n",
       " (u'norfolk', u'town'),\n",
       " (u'norfolk', u'city'),\n",
       " (u'norfolk', u'means_of_transportation'),\n",
       " (u'norfolk', u'township'),\n",
       " (u'norfolk', u'city_district'),\n",
       " (u'norfolk', u'train_depot'),\n",
       " (u'norfolk', u'railroad_terminal'),\n",
       " (u'norfolk', u'municipal_government'),\n",
       " (u'norfolk', u'designated_place'),\n",
       " (u'norfolk', u'depot'),\n",
       " (u'carlo_levi', u'writer'),\n",
       " (u'carlo_levi', u'painter'),\n",
       " (u'carlo_levi', u'activist'),\n",
       " (u'carlo_levi', u'person'),\n",
       " (u'market_value', u'net_worth'),\n",
       " (u'market_value', u'price'),\n",
       " (u'market_value', u'retail'),\n",
       " (u'market_value', u'cost'),\n",
       " (u'market_value', u'retail_price'),\n",
       " (u'richard_posner', u'person'),\n",
       " (u'richard_posner', u'officeholder'),\n",
       " (u'richard_posner', u'jurist'),\n",
       " (u'richard_posner', u'economist'),\n",
       " (u'richard_posner', u'jurisconsult'),\n",
       " (u'bigtooth_aspen', u'woody_plant'),\n",
       " (u'bigtooth_aspen', u'native_plant'),\n",
       " (u'benefaction', u'donation'),\n",
       " (u'benefaction', u'gift'),\n",
       " (u'abdication', u'resignation'),\n",
       " (u'abdication', u'group_event'),\n",
       " (u'abdication', u'written_document'),\n",
       " (u'heritage', u'transferred_property'),\n",
       " (u'delay', u'code'),\n",
       " (u'delay', u'computer_code'),\n",
       " (u'delay', u'computer_program'),\n",
       " (u'jon_cassar', u'person'),\n",
       " (u'jon_cassar', u'television_director'),\n",
       " (u'jon_cassar', u'television_producer'),\n",
       " (u'jon_cassar', u'producer'),\n",
       " (u'property_owner', u'owner'),\n",
       " (u'property_owner', u'person'),\n",
       " (u'property_owner', u'holder'),\n",
       " (u'workplace', u'making'),\n",
       " (u'epistemology', u'philosophy'),\n",
       " (u'epistemology', u'inquiry'),\n",
       " (u'self-justification', u'claim'),\n",
       " (u'self-justification', u'justification'),\n",
       " (u'self-justification', u'explanation'),\n",
       " (u'self-justification', u'declaration'),\n",
       " (u'self-justification', u'string_of_words'),\n",
       " (u'self-justification', u'assertion'),\n",
       " (u'self-justification', u'pleading'),\n",
       " (u'self-justification', u'logic'),\n",
       " (u'self-justification', u'answer'),\n",
       " (u'daniel_bernard_roumain', u'person'),\n",
       " (u'daniel_bernard_roumain', u'musician'),\n",
       " (u'daniel_bernard_roumain', u'composer'),\n",
       " (u'declaration', u'written_document'),\n",
       " (u'true_life', u'moving_picture'),\n",
       " (u'true_life', u'video_recording'),\n",
       " (u'true_life', u'broadcast'),\n",
       " (u'true_life', u'video'),\n",
       " (u'true_life', u'film'),\n",
       " (u'true_life', u'tv_program'),\n",
       " (u'true_life', u'television_series'),\n",
       " (u'true_life', u'social_event'),\n",
       " (u'true_life', u'television_program'),\n",
       " (u'true_life', u'tv_show'),\n",
       " (u'true_life', u'mass_media'),\n",
       " (u'true_life', u'television_show'),\n",
       " (u'consultant', u'person'),\n",
       " (u'patrizia_ciofi', u'singing_voice'),\n",
       " (u'patrizia_ciofi', u'musician'),\n",
       " (u'israel', u'political_organization'),\n",
       " (u'israel', u'surname'),\n",
       " (u'israel', u'locale'),\n",
       " (u'israel', u'country'),\n",
       " (u'israel', u'government'),\n",
       " (u'israel', u'forename'),\n",
       " (u'israel', u'parliamentary_democracy'),\n",
       " (u'israel', u'substantive'),\n",
       " (u'israel', u'political_organisation'),\n",
       " (u'israel', u'republic'),\n",
       " (u'israel', u'form_of_government'),\n",
       " (u'israel', u'gov'),\n",
       " (u'israel', u'given_name'),\n",
       " (u'edger', u'label'),\n",
       " (u'termination', u'written_record'),\n",
       " (u'termination', u'point_in_time'),\n",
       " (u'white_house', u'designated_place'),\n",
       " (u'white_house', u'official_residence'),\n",
       " (u'white_house', u'office_building'),\n",
       " (u'white_house', u'public_building'),\n",
       " (u'white_house', u'government_building'),\n",
       " (u'white_house', u'residence'),\n",
       " (u'sandy_mitchell', u'person'),\n",
       " (u'german_mark', u'monetary_unit'),\n",
       " (u'preston_manning', u'politician'),\n",
       " (u'preston_manning', u'person'),\n",
       " (u'preston_manning', u'chief'),\n",
       " (u'preston_manning', u'political_leader'),\n",
       " (u'preston_manning', u'leader'),\n",
       " (u'world', u'serial_publication'),\n",
       " (u'solent', u'world_ocean'),\n",
       " (u'solent', u'strait'),\n",
       " (u'solent', u'waterbody'),\n",
       " (u'solent', u'sea'),\n",
       " (u'solent', u'stream_channel'),\n",
       " (u'solent', u'waterway'),\n",
       " (u'solent', u'body_of_water'),\n",
       " (u'martin_brest', u'film_director'),\n",
       " (u'martin_brest', u'television_producer'),\n",
       " (u'martin_brest', u'producer'),\n",
       " (u'martin_brest', u'photography'),\n",
       " (u'martin_brest', u'filmmaker'),\n",
       " (u'martin_brest', u'film_maker'),\n",
       " (u'martin_brest', u'visual_art'),\n",
       " (u'verbal_abuse', u'offence'),\n",
       " (u'verbal_abuse', u'offense'),\n",
       " (u'brady_smith', u'person'),\n",
       " (u'brady_smith', u'actor'),\n",
       " (u'campo_san_polo', u'thoroughfare'),\n",
       " (u'campo_san_polo', u'main_road'),\n",
       " (u'campo_san_polo', u'parcel_of_land'),\n",
       " (u'campo_san_polo', u'street'),\n",
       " (u'campo_san_polo', u'superhighway'),\n",
       " (u'campo_san_polo', u'town_square'),\n",
       " (u'expiration_date', u'time_period'),\n",
       " (u'expiration_date', u'period_of_time'),\n",
       " (u'true_grit', u'musical_composition'),\n",
       " (u'true_grit', u'social_event'),\n",
       " (u'true_grit', u'feature_film'),\n",
       " (u'true_grit', u'film_genre'),\n",
       " (u'true_grit', u'literary_work'),\n",
       " (u'true_grit', u'film'),\n",
       " (u'true_grit', u'movie'),\n",
       " (u'true_grit', u'motion_picture'),\n",
       " (u'true_grit', u'piece_of_music'),\n",
       " (u'power', u'operator'),\n",
       " (u'power', u'person'),\n",
       " (u'tony_trujillo', u'person'),\n",
       " (u'racoon', u'animal'),\n",
       " (u'curling', u'sport'),\n",
       " (u'curling', u'olympic_sports'),\n",
       " (u'curling', u'group_event'),\n",
       " (u'curling', u'team_sport'),\n",
       " (u'curling', u'type_of_sport'),\n",
       " (u'curling', u'winter_sport'),\n",
       " (u'camelpox', u'illness'),\n",
       " (u'camelpox', u'animal_disease'),\n",
       " (u'camelpox', u'disease'),\n",
       " (u'camelpox', u'sickness'),\n",
       " (u'psychopathology', u'medical_research'),\n",
       " (u'psychopathology', u'psychology'),\n",
       " (u'psychopathology', u'learned_profession'),\n",
       " (u'psychopathology', u'enquiry'),\n",
       " (u'psychopathology', u'psychological_science'),\n",
       " (u'psychopathology', u'inquiry'),\n",
       " (u'psychopathology', u'branch_of_science'),\n",
       " (u'psychopathology', u'applied_science'),\n",
       " (u'psychopathology', u'medicine'),\n",
       " (u'psychopathology', u'medical_specialty'),\n",
       " (u'psychopathology', u'history_of_medicine'),\n",
       " (u'psychopathology', u'practice_of_medicine'),\n",
       " (u'canyonlands', u'piece_of_land'),\n",
       " (u'canyonlands', u'national_park'),\n",
       " (u'canyonlands', u'parcel'),\n",
       " (u'canyonlands', u'map'),\n",
       " (u'canyonlands', u'locale'),\n",
       " (u'canyonlands', u'tract'),\n",
       " (u'canyonlands', u'parcel_of_land'),\n",
       " (u'john_mcpherson', u'athlete'),\n",
       " (u'john_mcpherson', u'person'),\n",
       " (u'john_mcpherson', u'political_leader'),\n",
       " (u'john_mcpherson', u'leader'),\n",
       " (u'tim_dwight', u'soccer'),\n",
       " (u'tim_dwight', u'football'),\n",
       " (u'tim_dwight', u'association_football'),\n",
       " (u'tim_dwight', u'team_sport'),\n",
       " (u'tim_dwight', u'person'),\n",
       " (u'tim_dwight', u'sport'),\n",
       " (u'tim_dwight', u'football_game'),\n",
       " (u'dorothy_richardson', u'person'),\n",
       " (u'dorothy_richardson', u'writer'),\n",
       " (u'dorothy_richardson', u'journalist'),\n",
       " (u'indisposition', u'illness'),\n",
       " (u'indisposition', u'sickness'),\n",
       " (u'darkey', u'person'),\n",
       " (u'grass', u'plant'),\n",
       " (u'general', u'post'),\n",
       " (u'general', u'commissioned_officer'),\n",
       " (u'general', u'person'),\n",
       " (u'general', u'chief'),\n",
       " (u'general', u'leader'),\n",
       " (u'knowth', u'piece_of_land'),\n",
       " (u'knowth', u'land_site'),\n",
       " (u'knowth', u'site'),\n",
       " (u'knowth', u'parcel_of_land'),\n",
       " (u'graduate', u'person'),\n",
       " (u'graduate', u'student'),\n",
       " (u'graduate', u'scholar'),\n",
       " (u'transantarctic_mountains', u'mountain'),\n",
       " (u'transantarctic_mountains', u'mountain_range'),\n",
       " (u'transantarctic_mountains', u'mountain_chain'),\n",
       " (u'transantarctic_mountains', u'mount'),\n",
       " (u'transantarctic_mountains', u'range_of_mountains'),\n",
       " (u'transantarctic_mountains', u'chain_of_mountains'),\n",
       " (u'tropical_storm_helene', u'windstorm'),\n",
       " (u'tropical_storm_helene', u'tropical_storm'),\n",
       " (u'tropical_storm_helene', u'violent_storm'),\n",
       " (u'tropical_storm_helene', u'weather'),\n",
       " (u'tropical_storm_helene', u'natural_phenomenon'),\n",
       " (u'tropical_storm_helene', u'tropical_cyclone'),\n",
       " (u'tropical_storm_helene', u'storm'),\n",
       " (u'tropical_storm_helene', u'cyclone'),\n",
       " (u'tropical_storm_helene', u'weather_condition'),\n",
       " (u'udp_lite', u'file_format'),\n",
       " (u'udp_lite', u'computer_file'),\n",
       " (u'udp_lite', u'protocol'),\n",
       " (u'udp_lite', u'technical_standard'),\n",
       " (u'udp_lite', u'data_format'),\n",
       " (u'udp_lite', u'rule'),\n",
       " (u'udp_lite', u'standard'),\n",
       " (u'udp_lite', u'data_file'),\n",
       " (u'udp_lite', u'communications_protocol'),\n",
       " (u'udp_lite', u'text_file'),\n",
       " (u'udp_lite', u'guideline'),\n",
       " (u'under_the_moon', u'work_of_art'),\n",
       " (u'under_the_moon', u'television_series'),\n",
       " (u'under_the_moon', u'television_program'),\n",
       " (u'andrew_anderson', u'corporate_executive'),\n",
       " (u'andrew_anderson', u'person'),\n",
       " (u'andrew_anderson', u'juridical_person'),\n",
       " (u'andrew_anderson', u'executive_director'),\n",
       " (u'andrew_anderson', u'political_leader'),\n",
       " (u'andrew_anderson', u'business_executive'),\n",
       " (u'andrew_anderson', u'decision_maker'),\n",
       " (u'andrew_anderson', u'leader'),\n",
       " (u'pool', u'investment_fund'),\n",
       " (u'pool', u'net_earnings'),\n",
       " (u'pool', u'competition'),\n",
       " (u'pool', u'olympic_sports'),\n",
       " (u'pool', u'net_profit'),\n",
       " (u'pool', u'net_income'),\n",
       " (u'pool', u'body_of_water'),\n",
       " (u'pool', u'sport'),\n",
       " (u'pool', u'investment'),\n",
       " (u'pool', u'assets'),\n",
       " (u'pool', u'profit'),\n",
       " (u'pool', u'athlete'),\n",
       " (u'pool', u'income'),\n",
       " (u'pool', u'collective_investment_scheme'),\n",
       " (u'pool', u'net'),\n",
       " (u'pool', u'investment_funds'),\n",
       " (u'pool', u'profits'),\n",
       " (u'pool', u'earnings'),\n",
       " (u'dollar', u'bank_note'),\n",
       " (u'dollar', u'banknote'),\n",
       " (u'dollar', u'debt_instrument'),\n",
       " (u'dollar', u'monetary_unit'),\n",
       " (u'dollar', u'paper_money'),\n",
       " (u'dollar', u'bill'),\n",
       " (u'dollar', u'paper_currency'),\n",
       " (u'dollar', u'legal_tender'),\n",
       " (u'dollar', u'money'),\n",
       " (u'dollar', u'negotiable_instrument'),\n",
       " (u'tasman_glacier', u'ice_mass'),\n",
       " (u'tasman_glacier', u'glacier'),\n",
       " (u'tasman_glacier', u'ice'),\n",
       " (u'tasman_glacier', u'waterbody'),\n",
       " (u'tasman_glacier', u'body_of_water'),\n",
       " (u'red_planet', u'film_genre'),\n",
       " (u'red_planet', u'motion_picture'),\n",
       " (u'natural_gas', u'gas'),\n",
       " (u'natural_gas', u'gaseous_state'),\n",
       " (u'compact_muon_solenoid', u'particle_detector'),\n",
       " (u'compact_muon_solenoid', u'scientific_research'),\n",
       " (u'pyrrhus', u'king'),\n",
       " (u'pyrrhus', u'chief_of_state'),\n",
       " (u'pyrrhus', u'rank'),\n",
       " (u'pyrrhus', u'person'),\n",
       " (u'pyrrhus', u'monarch'),\n",
       " (u'program', u'engineering_science'),\n",
       " (u'program', u'flyer'),\n",
       " (u'program', u'promotional_material'),\n",
       " (u'program', u'visual_communication'),\n",
       " (u'program', u'social_event'),\n",
       " (u'program', u'engineering'),\n",
       " (u'program', u'advertisement'),\n",
       " (u'program', u'graphics'),\n",
       " (u'program', u'course_of_instruction'),\n",
       " (u'program', u'technology'),\n",
       " (u'program', u'mass_medium'),\n",
       " (u'program', u'announcement'),\n",
       " (u'program', u'graphic'),\n",
       " (u'program', u'promulgation'),\n",
       " (u'program', u'entertainment'),\n",
       " (u'program', u'advertising'),\n",
       " (u'program', u'mass_media'),\n",
       " (u'program', u'print_media'),\n",
       " (u'program', u'presentation'),\n",
       " (u'castlereagh_river', u'river'),\n",
       " (u'castlereagh_river', u'body_of_water'),\n",
       " (u'hurricane_claudette', u'tropical_storm'),\n",
       " (u'hurricane_claudette', u'tropical_depression'),\n",
       " (u'hurricane_claudette', u'weather'),\n",
       " (u'hurricane_claudette', u'atmospheric_condition'),\n",
       " (u'hurricane_claudette', u'weather_condition'),\n",
       " (u'hurricane_claudette', u'tropical_cyclone'),\n",
       " (u'hurricane_claudette', u'natural_phenomenon'),\n",
       " (u'antony_leung', u'owner'),\n",
       " (u'antony_leung', u'person'),\n",
       " (u'antony_leung', u'holder'),\n",
       " (u'emery_cloth', u'person'),\n",
       " (u'fat', u'animal_tissue'),\n",
       " (u'capillary', u'vasculature'),\n",
       " (u'capillary', u'tube'),\n",
       " (u'capillary', u'body_structure'),\n",
       " (u'capillary', u'anatomical_structure'),\n",
       " (u'capillary', u'vascular_system'),\n",
       " (u'capillary', u'body_part'),\n",
       " (u'capillary', u'vessel'),\n",
       " (u'capillary', u'technology'),\n",
       " (u'capillary', u'blood_supply'),\n",
       " (u'condition', u'good_health'),\n",
       " (u'condition', u'understanding'),\n",
       " (u'condition', u'written_document'),\n",
       " (u'condition', u'written_agreement'),\n",
       " (u'palazzo_farnese', u'tourist_attraction'),\n",
       " (u'palazzo_farnese', u'memorial'),\n",
       " (u'palazzo_farnese', u'public_building'),\n",
       " (u'palazzo_farnese', u'work_of_art'),\n",
       " (u'palazzo_farnese', u'showcase'),\n",
       " (u'palazzo_farnese', u'installation_art'),\n",
       " (u'landscape_arch', u'locale'),\n",
       " (u'subsoil', u'soil'),\n",
       " (u'feudatory', u'person'),\n",
       " (u'federal_records', u'firm'),\n",
       " (u'federal_records', u'corporate_identity'),\n",
       " (u'federal_records', u'intellectual_property'),\n",
       " (u'federal_records', u'company'),\n",
       " (u'federal_records', u'industrial_property'),\n",
       " (u'federal_records', u'corporation'),\n",
       " (u'federal_records', u'person'),\n",
       " (u'federal_records', u'juridical_person'),\n",
       " (u'federal_records', u'branch'),\n",
       " (u'phonics', u'teaching_reading'),\n",
       " (u'small', u'body_part'),\n",
       " (u'san_andreas', u'town'),\n",
       " (u'san_andreas', u'country'),\n",
       " (u'san_andreas', u'authorities'),\n",
       " (u'san_andreas', u'city'),\n",
       " (u'san_andreas', u'local_government'),\n",
       " (u'la_dolce_musto', u'periodical_publication'),\n",
       " (u'la_dolce_musto', u'periodical'),\n",
       " (u'gerry_davis', u'competitor'),\n",
       " (u'gerry_davis', u'sport'),\n",
       " (u'investment', u'possession'),\n",
       " (u'investment', u'financing'),\n",
       " (u'investment', u'finance'),\n",
       " (u'investment', u'protection'),\n",
       " (u'investment', u'assets'),\n",
       " (u'golden_gate_park', u'parkland'),\n",
       " (u'golden_gate_park', u'country'),\n",
       " (u'golden_gate_park', u'green_area'),\n",
       " (u'golden_gate_park', u'park'),\n",
       " (u'golden_gate_park', u'parcel_of_land'),\n",
       " (u'golden_gate_park', u'protected_area'),\n",
       " (u'korea', u'city'),\n",
       " (u'korea', u'locale'),\n",
       " (u'korea', u'plot_of_land'),\n",
       " (u'korea', u'parcel_of_land'),\n",
       " (u'korea', u'piece_of_land'),\n",
       " (u'korea', u'state'),\n",
       " (u'korea', u'country'),\n",
       " (u'korea', u'building_site'),\n",
       " (u'sun', u'star'),\n",
       " (u'gary_fong', u'person'),\n",
       " (u'aramaic', u'ordinary_language'),\n",
       " (u'aramaic', u'family_relationship'),\n",
       " (u'aramaic', u'philosophy_of_language'),\n",
       " (u'long_tack_sam', u'person'),\n",
       " (u'long_tack_sam', u'performing_arts'),\n",
       " (u'michael_costa', u'person'),\n",
       " (u'michael_costa', u'political_leader'),\n",
       " (u'michael_costa', u'leader'),\n",
       " (u'mohammed', u'person'),\n",
       " (u'gigahertz', u'unit_of_measure'),\n",
       " (u'tragedy', u'drama'),\n",
       " (u'weather_forecasting', u'forecasting'),\n",
       " (u'weather_forecasting', u'prediction'),\n",
       " (u'phonetic_transcription', u'written_language'),\n",
       " (u'phonetic_transcription', u'written_communication'),\n",
       " (u'phonetic_transcription', u'written_text'),\n",
       " (u'phonetic_transcription', u'transcription'),\n",
       " (u'striatum', u'body_structure'),\n",
       " (u'striatum', u'brain'),\n",
       " (u'striatum', u'body_part'),\n",
       " (u'scott_city', u'town'),\n",
       " (u'scott_city', u'county_town'),\n",
       " (u'harmonia_axyridis', u'insect'),\n",
       " (u'harmonia_axyridis', u'beetle'),\n",
       " (u'harmonia_axyridis', u'beetles'),\n",
       " (u'airport', u'airfield'),\n",
       " (u'airport', u'landing_field'),\n",
       " (u'state_highway_17', u'freeway'),\n",
       " (u'state_highway_17', u'thruway'),\n",
       " (u'state_highway_17', u'thoroughfare'),\n",
       " (u'state_highway_17', u'route'),\n",
       " (u'state_highway_17', u'transport_infrastructure'),\n",
       " (u'state_highway_17', u'main_road'),\n",
       " (u'state_highway_17', u'motorway'),\n",
       " (u'state_highway_17', u'throughway'),\n",
       " (u'state_highway_17', u'state_highway'),\n",
       " (u'state_highway_17', u'expressway'),\n",
       " (u'9mm', u'work_of_art'),\n",
       " (u'9mm', u'musical_composition'),\n",
       " (u'state_highway_12', u'motorway'),\n",
       " (u'state_highway_12', u'thruway'),\n",
       " (u'state_highway_12', u'state_highway'),\n",
       " (u'state_highway_12', u'thoroughfare'),\n",
       " (u'state_highway_12', u'route'),\n",
       " (u'state_highway_12', u'superhighway'),\n",
       " (u'state_highway_12', u'transport_infrastructure'),\n",
       " (u'state_highway_12', u'main_road'),\n",
       " (u'state_highway_12', u'freeway'),\n",
       " (u'state_highway_12', u'throughway'),\n",
       " (u'state_highway_12', u'controlled-access_highway'),\n",
       " (u'state_highway_12', u'expressway'),\n",
       " (u'full_stop', u'punctuation_mark'),\n",
       " (u'david_t._ellwood', u'person'),\n",
       " (u'sugar-cane', u'plant'),\n",
       " (u'transit', u'transportation'),\n",
       " (u'transit', u'travel'),\n",
       " (u'transit', u'passenger_transport'),\n",
       " (u'transit', u'traveling'),\n",
       " (u'transit', u'travelling'),\n",
       " (u'transit', u'transport'),\n",
       " (u'abutilon_theophrasti', u'plant'),\n",
       " (u'louisville', u'county_courthouse'),\n",
       " (u'louisville', u'town'),\n",
       " (u'louisville', u'city'),\n",
       " (u'louisville', u'mass_media'),\n",
       " (u'louisville', u'country'),\n",
       " (u'louisville', u'provincial_capital'),\n",
       " (u'louisville', u'print_media'),\n",
       " (u'louisville', u'local_government'),\n",
       " (u'cherryholmes', u'rock_group'),\n",
       " (u'cherryholmes', u'musical_group'),\n",
       " (u'cherryholmes', u'musical_organization'),\n",
       " (u'europe', u'map'),\n",
       " (u'europe', u'public_building'),\n",
       " (u'europe', u'traveling'),\n",
       " (u'europe', u'railway_system'),\n",
       " (u'europe', u'railway_line'),\n",
       " (u'europe', u'travel'),\n",
       " (u'europe', u'means_of_transportation'),\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for c in clusters:\n",
    "    print c.loss, c.test_loss, c.epoch_count, c.mrr\n",
    "    \n",
    "    \n",
    "map(lambda x: (data.train_query[x], data.train_hyper[x]), np.where(sample_clusters == 0)[0])\n",
    "#c15 = Counter(map(lambda x: data.train_hyper[x], np.where(sample_clusters == 18)[0]))\n",
    "#sorted(c15.items(), key = lambda (k,v): v, reverse=True)\n",
    "#Counter(sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate without attempting to cluster test terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "('Done', 100)\n",
      "('Done', 200)\n",
      "('Done', 300)\n",
      "('Done', 400)\n",
      "('Done', 500)\n",
      "('Done', 600)\n",
      "('Done', 700)\n",
      "('Done', 800)\n",
      "('Done', 900)\n",
      "('Done', 1000)\n",
      "('Done', 1100)\n",
      "('Done', 1200)\n",
      "('Done', 1300)\n",
      "('Done', 1400)\n",
      "CRIM evaluation:\n",
      "MRR: 0.09636\n",
      "MAP: 0.04405\n",
      "P@1: 0.06871\n",
      "P@5: 0.04286\n",
      "P@10: 0.04008\n"
     ]
    }
   ],
   "source": [
    "print (\"Generating predictions...\")\n",
    "yamane_predictions = predict_cluster_hypernyms(data.test_query, data.tokenizer, clusters)\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores((data.test_query, data.test_hyper), yamane_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blackfly': [u'animal',\n",
       "  u'insect',\n",
       "  u'integrating',\n",
       "  u'information_systems',\n",
       "  u'information_management',\n",
       "  u'implementation',\n",
       "  u'agricultural_pest',\n",
       "  u'systems',\n",
       "  u'data_management',\n",
       "  u'information_system',\n",
       "  u'nonliving',\n",
       "  u'waterbody',\n",
       "  u'technical',\n",
       "  u'integrated',\n",
       "  u'enable']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_cluster_hypernyms(['blackfly'], data.tokenizer, clusters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KNN classifier on clustering data jointly learnt by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "# prepare knn dataset based on learnt clusters\n",
    "train_seq = np.array(data.tokenizer.texts_to_sequences(data.train_query))\n",
    "\n",
    "X_knn = {}\n",
    "for idx, c in enumerate(clusters):\n",
    "    cluster_ids = np.where(sample_clusters == idx)\n",
    "    # we can reduce duplicate terms to unique terms    \n",
    "    uniq_terms = np.unique(train_seq[cluster_ids])\n",
    "    #print (uniq_terms)    \n",
    "    X_knn[idx] = data.embedding_matrix[uniq_terms]  \n",
    "\n",
    "X_features = X_knn[0]\n",
    "y = np.zeros(X_knn[0].shape[0], dtype='int16')\n",
    "\n",
    "for k in range(1,len(clusters)):\n",
    "    X_features = np.vstack((X_features, X_knn[k]))\n",
    "    y = np.hstack((y, np.array([k] * X_knn[k].shape[0])))\n",
    "    \n",
    "neigh.fit(X_features, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "('Done', 100)\n",
      "('Done', 200)\n",
      "('Done', 300)\n",
      "('Done', 400)\n",
      "('Done', 500)\n",
      "('Done', 600)\n",
      "('Done', 700)\n",
      "('Done', 800)\n",
      "('Done', 900)\n",
      "('Done', 1000)\n",
      "('Done', 1100)\n",
      "('Done', 1200)\n",
      "('Done', 1300)\n",
      "('Done', 1400)\n",
      "CRIM evaluation:\n",
      "MRR: 0.09942\n",
      "MAP: 0.04663\n",
      "P@1: 0.07071\n",
      "P@5: 0.04554\n",
      "P@10: 0.04292\n"
     ]
    }
   ],
   "source": [
    "print (\"Generating predictions...\")\n",
    "yamane_predictions = predict_cluster_hypernyms(data.test_query, data.tokenizer, clusters, neigh)\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores((data.test_query, data.test_hyper), yamane_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#crim_get_top_hypernyms('turonian', None, clusters[15].model, data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 36,\n",
       "         1: 3088,\n",
       "         2: 1416,\n",
       "         3: 1953,\n",
       "         4: 137,\n",
       "         5: 119,\n",
       "         6: 245,\n",
       "         7: 468,\n",
       "         8: 989,\n",
       "         9: 275,\n",
       "         10: 396,\n",
       "         11: 402,\n",
       "         12: 240,\n",
       "         13: 102,\n",
       "         14: 191,\n",
       "         15: 53,\n",
       "         16: 35,\n",
       "         17: 92,\n",
       "         18: 86,\n",
       "         19: 110,\n",
       "         20: 554,\n",
       "         21: 335,\n",
       "         22: 126,\n",
       "         23: 158,\n",
       "         24: 173})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#yamane_predictions['dashi']\n",
    "np.mean(clusters[10].model.get_layer(name='Phi0').get_weights()[0])\n",
    "Counter(sample_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yamane Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training started...\n",
    "* ('m: ', 10, 'lambda: ', 0.15, 'max epoch per cluster: ', 15, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_normal')\n",
    "('Epoch:', 1, 'Cluster #:', 25, 'Loss:', 185.25700701117515, 'Test Loss:', 462.11729974992573)\n",
    "('Epoch:', 2, 'Cluster #:', 25, 'Loss:', 94.78926725581289, 'Test Loss:', 257.9614339789539)\n",
    "('Epoch:', 3, 'Cluster #:', 25, 'Loss:', 68.510555833322, 'Test Loss:', 168.7855049063693)\n",
    "('Epoch:', 4, 'Cluster #:', 25, 'Loss:', 56.637282415106895, 'Test Loss:', 131.58040585272218)\n",
    "('Epoch:', 5, 'Cluster #:', 25, 'Loss:', 48.74302359417314, 'Test Loss:', 114.5913256756349)\n",
    "('Epoch:', 6, 'Cluster #:', 25, 'Loss:', 43.784197641848586, 'Test Loss:', 103.29953979119527)\n",
    "('Epoch:', 7, 'Cluster #:', 25, 'Loss:', 39.676348549440156, 'Test Loss:', 94.9497262534533)\n",
    "('Epoch:', 8, 'Cluster #:', 25, 'Loss:', 36.70157310644514, 'Test Loss:', 88.89629495775293)\n",
    "('Epoch:', 9, 'Cluster #:', 25, 'Loss:', 33.68777084685018, 'Test Loss:', 85.53309237194794)\n",
    "('Epoch:', 10, 'Cluster #:', 25, 'Loss:', 31.62626134102262, 'Test Loss:', 80.67354718668425)\n",
    "('Epoch:', 11, 'Cluster #:', 25, 'Loss:', 30.100850980100514, 'Test Loss:', 83.60007793995133)\n",
    "('Epoch:', 12, 'Cluster #:', 25, 'Loss:', 27.972241665369946, 'Test Loss:', 78.48380810162551)\n",
    "('Epoch:', 13, 'Cluster #:', 25, 'Loss:', 26.79718875462626, 'Test Loss:', 78.7881210642461)\n",
    "('Epoch:', 14, 'Cluster #:', 25, 'Loss:', 25.295018676088365, 'Test Loss:', 75.88227921965843)\n",
    "('Epoch:', 15, 'Cluster #:', 25, 'Loss:', 24.44286949604233, 'Test Loss:', 71.73972092286799)\n",
    "2018-12-10 02:39:45.787354\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.02308\n",
    "P@1: 0.01201\n",
    "P@5: 0.01234\n",
    "P@10: 0.01186\n",
    "\n",
    "---------------------------------------------------------------\n",
    "* ('m: ', 10, 'lambda: ', 0.1, 'max epoch per cluster: ', 20, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'sigmoid_constraint': 'ForceToOne', dropout=0.3, learning_rate=0.001)\n",
    "\n",
    "\n",
    "('Sample clusters size: ', 11779)\n",
    "('Epoch:', 1, 'Cluster #:', 30, 'Loss:', 164.64302213018138, 'Test Loss:', 16.821501479359963)\n",
    "('Epoch:', 2, 'Cluster #:', 30, 'Loss:', 85.95771886678412, 'Test Loss:', 9.479251270126163)\n",
    "('Epoch:', 3, 'Cluster #:', 30, 'Loss:', 60.89030767480532, 'Test Loss:', 7.113059009361313)\n",
    "('Epoch:', 4, 'Cluster #:', 30, 'Loss:', 49.35616979487046, 'Test Loss:', 5.911477896573342)\n",
    "('Epoch:', 5, 'Cluster #:', 30, 'Loss:', 42.839287023517926, 'Test Loss:', 5.237530594305887)\n",
    "('Epoch:', 6, 'Cluster #:', 30, 'Loss:', 37.95000084499964, 'Test Loss:', 4.801821103322982)\n",
    "('Epoch:', 7, 'Cluster #:', 30, 'Loss:', 34.69708706878203, 'Test Loss:', 4.445396709869783)\n",
    "('Epoch:', 8, 'Cluster #:', 30, 'Loss:', 31.344791742943926, 'Test Loss:', 4.20229308469837)\n",
    "('Epoch:', 9, 'Cluster #:', 30, 'Loss:', 29.270582812105324, 'Test Loss:', 4.001659726132235)\n",
    "('Epoch:', 10, 'Cluster #:', 30, 'Loss:', 27.312983008645823, 'Test Loss:', 3.8331675905460845)\n",
    "('Epoch:', 11, 'Cluster #:', 30, 'Loss:', 25.86424312723296, 'Test Loss:', 3.7695775713167223)\n",
    "('Epoch:', 12, 'Cluster #:', 30, 'Loss:', 24.26373833860683, 'Test Loss:', 3.643787452736217)\n",
    "('Epoch:', 13, 'Cluster #:', 30, 'Loss:', 23.194776870778636, 'Test Loss:', 3.5394367815238903)\n",
    "('Epoch:', 14, 'Cluster #:', 30, 'Loss:', 22.077418524692256, 'Test Loss:', 3.4249304910639884)\n",
    "('Epoch:', 15, 'Cluster #:', 30, 'Loss:', 20.919567144932383, 'Test Loss:', 3.3781916145887294)\n",
    "('Epoch:', 16, 'Cluster #:', 30, 'Loss:', 20.137791508837896, 'Test Loss:', 3.305138163670991)\n",
    "('Epoch:', 17, 'Cluster #:', 30, 'Loss:', 19.10677393638859, 'Test Loss:', 3.261422005968933)\n",
    "('Epoch:', 18, 'Cluster #:', 30, 'Loss:', 18.686860465103926, 'Test Loss:', 3.1841310813365644)\n",
    "('Epoch:', 19, 'Cluster #:', 30, 'Loss:', 17.963673188673297, 'Test Loss:', 3.117555515795205)\n",
    "('Epoch:', 20, 'Cluster #:', 30, 'Loss:', 16.913953626791166, 'Test Loss:', 3.1116348788942028)\n",
    "2018-12-10 12:28:42.204443\n",
    "\n",
    "* Cluster distribution\n",
    "Counter({0: 68,\n",
    "         1: 516,\n",
    "         2: 56,\n",
    "         3: 1415,\n",
    "         4: 345,\n",
    "         5: 30,\n",
    "         6: 452,\n",
    "         7: 38,\n",
    "         8: 45,\n",
    "         9: 693,\n",
    "         10: 2442,\n",
    "         11: 18,\n",
    "         12: 908,\n",
    "         13: 28,\n",
    "         14: 141,\n",
    "         15: 1306,\n",
    "         16: 84,\n",
    "         17: 315,\n",
    "         18: 412,\n",
    "         19: 394,\n",
    "         20: 364,\n",
    "         21: 60,\n",
    "         22: 189,\n",
    "         23: 105,\n",
    "         24: 24,\n",
    "         25: 127,\n",
    "         26: 731,\n",
    "         27: 119,\n",
    "         28: 34,\n",
    "         29: 320})\n",
    "         \n",
    "CRIM evaluation:\n",
    "MRR: 0.02787\n",
    "P@1: 0.01935\n",
    "P@5: 0.01447\n",
    "P@10: 0.01298\n",
    "\n",
    "\n",
    "* After fitting a KNN model on the cluster results and using it to fine-tune which clusters to compute prediction of each query term, the results improved three-fold.\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.06088\n",
    "P@1: 0.0447\n",
    "P@5: 0.02825\n",
    "P@10: 0.02616\n",
    "\n",
    "* However, Yamane is, in general, disappointing with respect to SharedTask challenge.  \n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "Training started...\n",
    "('m: ', 5, 'lambda: ', 0.05, 'max epoch per cluster: ', 10, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'sigmoid_kernel_constraint: ', 'None', 'dropout: ', 0.2, 'learning_rate: ', 0.001, 'cluster_max: ', 15)\n",
    "('Sample clusters size: ', 11779)\n",
    "('Epoch:', 1, 'Cluster #:', 3, 'Loss:', 1109.3663462693028, 'Test Loss:', 50.47739016701659)\n",
    "('Epoch:', 2, 'Cluster #:', 4, 'Loss:', 443.7235447903154, 'Test Loss:', 17.8407350068523)\n",
    "('Epoch:', 3, 'Cluster #:', 4, 'Loss:', 337.1281598309441, 'Test Loss:', 20.06709446922047)\n",
    "('Epoch:', 4, 'Cluster #:', 4, 'Loss:', 267.10441667238143, 'Test Loss:', 19.677748865275674)\n",
    "('Epoch:', 5, 'Cluster #:', 4, 'Loss:', 229.65082203910208, 'Test Loss:', 17.97698300000083)\n",
    "('Epoch:', 6, 'Cluster #:', 4, 'Loss:', 193.899041174633, 'Test Loss:', 16.751907205700498)\n",
    "('Epoch:', 7, 'Cluster #:', 4, 'Loss:', 174.12937078346036, 'Test Loss:', 16.246821108061788)\n",
    "('Epoch:', 8, 'Cluster #:', 5, 'Loss:', 143.5690400129045, 'Test Loss:', 8.259589462610172)\n",
    "('Epoch:', 9, 'Cluster #:', 5, 'Loss:', 122.85453648527569, 'Test Loss:', 10.32649805369518)\n",
    "('Epoch:', 10, 'Cluster #:', 5, 'Loss:', 120.9294774463863, 'Test Loss:', 11.019405025631883)\n",
    "('Epoch:', 11, 'Cluster #:', 5, 'Loss:', 117.86586058200308, 'Test Loss:', 11.315035963446595)\n",
    "('Epoch:', 12, 'Cluster #:', 5, 'Loss:', 117.66658123196073, 'Test Loss:', 11.45983252564142)\n",
    "('Epoch:', 13, 'Cluster #:', 5, 'Loss:', 117.53339115441747, 'Test Loss:', 11.49364635983179)\n",
    "('Epoch:', 14, 'Cluster #:', 5, 'Loss:', 117.17556676329085, 'Test Loss:', 11.589454222113588)\n",
    "('Epoch:', 15, 'Cluster #:', 5, 'Loss:', 116.93114152015157, 'Test Loss:', 11.61919019261072)\n",
    "('Epoch:', 16, 'Cluster #:', 5, 'Loss:', 116.64158589304395, 'Test Loss:', 11.655326009184815)\n",
    "('Epoch:', 17, 'Cluster #:', 5, 'Loss:', 116.35099397958227, 'Test Loss:', 11.681808996588686)\n",
    "2018-12-10 21:24:08.962568\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.05966\n",
    "P@1: 0.03669\n",
    "P@5: 0.02672\n",
    "P@10: 0.02536\n",
    "\n",
    "* After predicting query term clusters\n",
    "CRIM evaluation:\n",
    "MRR: 0.06088\n",
    "P@1: 0.0447\n",
    "P@5: 0.02825\n",
    "P@10: 0.02616\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "('m: ', 5, 'lambda: ', 0.1, 'max epoch per cluster: ', 10, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'sigmoid_kernel_constraint: ', 'ForceToOne', 'dropout: ', 0.3, 'learning_rate: ', 0.001, 'cluster_max: ', 15)\n",
    "('Sample clusters size: ', 11779)\n",
    "('Epoch:', 1, 'Cluster #:', 9, 'Loss:', 482.0829993935509, 'Test Loss:', 27.62408774221937)\n",
    "('Epoch:', 2, 'Cluster #:', 10, 'Loss:', 281.1839863856323, 'Test Loss:', 17.055058593617286)\n",
    "('Epoch:', 3, 'Cluster #:', 11, 'Loss:', 208.62401276072805, 'Test Loss:', 12.390350534596523)\n",
    "('Epoch:', 4, 'Cluster #:', 12, 'Loss:', 166.41537183626983, 'Test Loss:', 9.618624108000708)\n",
    "('Epoch:', 5, 'Cluster #:', 13, 'Loss:', 134.7260290642522, 'Test Loss:', 7.7731040777703475)\n",
    "('Epoch:', 6, 'Cluster #:', 13, 'Loss:', 119.45636204084552, 'Test Loss:', 7.063226612456044)\n",
    "('Epoch:', 7, 'Cluster #:', 13, 'Loss:', 108.30129835629151, 'Test Loss:', 6.616330906035168)\n",
    "('Epoch:', 8, 'Cluster #:', 13, 'Loss:', 101.58308503461907, 'Test Loss:', 6.293532006980128)\n",
    "('Epoch:', 9, 'Cluster #:', 13, 'Loss:', 94.7640274246464, 'Test Loss:', 5.976787171151955)\n",
    "('Epoch:', 10, 'Cluster #:', 13, 'Loss:', 88.485667872232, 'Test Loss:', 5.796518988622176)\n",
    "('Epoch:', 11, 'Cluster #:', 13, 'Loss:', 88.32206717526479, 'Test Loss:', 5.7801972194946964)\n",
    "('Epoch:', 12, 'Cluster #:', 13, 'Loss:', 88.03503130540048, 'Test Loss:', 5.762659813637024)\n",
    "('Epoch:', 13, 'Cluster #:', 13, 'Loss:', 87.9104809983742, 'Test Loss:', 5.754546612789104)\n",
    "('Epoch:', 14, 'Cluster #:', 13, 'Loss:', 87.80618179810531, 'Test Loss:', 5.747131267707188)\n",
    "2018-12-11 00:15:28.930610\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.05927\n",
    "P@1: 0.03602\n",
    "P@5: 0.03072\n",
    "P@10: 0.02855\n",
    "\n",
    "* After KNN\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.06201\n",
    "P@1: 0.03736\n",
    "P@5: 0.03205\n",
    "P@10: 0.02976\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "('Sample clusters size: ', 11779)\n",
    "('m: ', 1, 'lambda: ', 0.13, 'max epoch per cluster: ', 10, 'Negative sampling: ', 'random', 'Phi Init: ', 'random_identity', 'sigmoid_kernel_constraint: ', 'ForceToOne', 'dropout: ', 0.3, 'learning_rate: ', 0.001, 'cluster_max: ', 25)\n",
    "('Epoch:', 1, 'Cluster #:', 2, 'Loss:', 2540.3022823217325, 'Test Loss:', 53.87453193264082)\n",
    "('Epoch:', 2, 'Cluster #:', 2, 'Loss:', 1667.1898109320173, 'Test Loss:', 40.18764992605429)\n",
    "('Epoch:', 3, 'Cluster #:', 2, 'Loss:', 1361.9321219512858, 'Test Loss:', 34.321806932479376)\n",
    "('Epoch:', 4, 'Cluster #:', 2, 'Loss:', 1178.6240238926068, 'Test Loss:', 30.980050023383228)\n",
    "('Epoch:', 5, 'Cluster #:', 2, 'Loss:', 1044.863646138525, 'Test Loss:', 27.789724176647724)\n",
    "('Epoch:', 6, 'Cluster #:', 3, 'Loss:', 643.6681745337323, 'Test Loss:', 16.18063761241986)\n",
    "('Epoch:', 7, 'Cluster #:', 3, 'Loss:', 585.461576843336, 'Test Loss:', 14.75499623647435)\n",
    "('Epoch:', 8, 'Cluster #:', 3, 'Loss:', 543.6171705845585, 'Test Loss:', 13.808908825924542)\n",
    "('Epoch:', 9, 'Cluster #:', 3, 'Loss:', 507.70913083471424, 'Test Loss:', 12.98376774797604)\n",
    "('Epoch:', 10, 'Cluster #:', 3, 'Loss:', 484.66481586770334, 'Test Loss:', 12.76265995549602)\n",
    "('Epoch:', 11, 'Cluster #:', 3, 'Loss:', 483.03113291140517, 'Test Loss:', 12.732977458312538)\n",
    "('Epoch:', 12, 'Cluster #:', 3, 'Loss:', 481.7263510373843, 'Test Loss:', 12.668654509856728)\n",
    "('Epoch:', 13, 'Cluster #:', 3, 'Loss:', 481.51302522893866, 'Test Loss:', 12.680976091299877)\n",
    "('Epoch:', 14, 'Cluster #:', 3, 'Loss:', 480.39035817460336, 'Test Loss:', 12.628896979962215)\n",
    "('Epoch:', 15, 'Cluster #:', 3, 'Loss:', 481.034190520407, 'Test Loss:', 12.582702009354458)\n",
    "2018-12-11 01:20:21.632953\n",
    "\n",
    "CRIM evaluation:\n",
    "MRR: 0.06717\n",
    "MAP: 0.03322\n",
    "P@1: 0.0427\n",
    "P@5: 0.03244\n",
    "P@10: 0.03148"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3042"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab['dog'].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_these_terms = np.asarray(data.tokenizer.texts_to_sequences(data.train_query))\n",
    "test_these_hypers = np.asarray(data.tokenizer.texts_to_sequences(data.train_hyper))\n",
    "\n",
    "indices = np.arange(len(test_these_terms))                               \n",
    "#np.random.seed(32)\n",
    "np.random.shuffle(indices)\n",
    "print indices[:32]\n",
    "\n",
    "term, hyper, label = extend_batch_with_negatives(test_these_terms[indices[:32]], test_these_hypers[indices[:32]], data.random_words, data.tokenizer, 5)\n",
    "print len(term)\n",
    "[(data.tokenizer.index_word[i[0]], data.tokenizer.index_word[j[0]], l) for i, j, l in zip(term, hyper, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.vocab]\n",
    "len(set(data.valid_hyper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "[l for l in crim_model.layers if type(l) == Dense][0].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sort(data.tokenizer.texts_to_sequences(data.vocab)).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9350891]\n",
      "[0.01795901]\n",
      "[0.11550742]\n",
      "[0.00809616]\n",
      "[0.11760344]\n",
      "[0.05177224]\n",
      "[0.09667916]\n",
      "[0.11339272]\n",
      "[0.05452686]\n",
      "[0.0500305]\n",
      "[0.08509561]\n",
      "[0.20673454]\n",
      "[0.15739459]\n",
      "[0.02486995]\n",
      "[0.06349776]\n",
      "[0.11436869]\n",
      "[0.07228256]\n",
      "[0.4835079]\n",
      "[0.03059227]\n",
      "[0.11007205]\n",
      "[0.07297403]\n",
      "[0.09328794]\n",
      "[0.08511002]\n",
      "[0.19539028]\n",
      "[0.0796214]\n"
     ]
    }
   ],
   "source": [
    "#for idx, m in enumerate(clusters):\n",
    " #   print idx, m.loss, m.test_loss, m.model.get_layer(name='Prediction').get_weights()[0]\n",
    "\n",
    "i = data.tokenizer.word_index['rod_laver']\n",
    "j = data.tokenizer.word_index['person']\n",
    "for c in clusters:\n",
    "    print c.model.predict([[i], [j]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5585"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del clusters\n",
    "\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
