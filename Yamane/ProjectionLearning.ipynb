{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Learning Models refactored for Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = 'GoogleNews-vectors-negative300.txt'\n",
    "model = KeyedVectors.load_word2vec_format(w2v, binary=False)\n",
    "# pre-compute L2 norms of vectors\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_subsumptions(filename):\n",
    "    subsumptions = []\n",
    "\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            subsumptions.append((row[0], row[1]))\n",
    "\n",
    "    return subsumptions\n",
    "\n",
    "def read_synonyms(filename):\n",
    "    synonyms = defaultdict(lambda: list())\n",
    "\n",
    "    with codecs.open(filename,encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            for word in row[1].split(','):\n",
    "                synonyms[row[0]].append(word)\n",
    "    \n",
    "    synonyms.default_factory = None\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subs = read_subsumptions('subsumptions-train.txt.orig')\n",
    "test_subs = read_subsumptions('subsumptions-test.txt.orig')\n",
    "valid_subs = read_subsumptions('subsumptions-validation.txt.orig')\n",
    "\n",
    "synonyms = read_synonyms('synonyms.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct pre-trained word embeddings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate training tuples for which no embedding exists\n",
    "from collections import Counter\n",
    "\n",
    "def get_terms_having_vectors(dataset):    \n",
    "    query, hyper = \\\n",
    "    zip(*[(q,h) for q, h in dataset \n",
    "          if q in model and h in model])\n",
    "    \n",
    "    return list(query), list(hyper)\n",
    "    \n",
    "train_query, train_hyper = get_terms_having_vectors(train_subs)\n",
    "test_query, test_hyper = get_terms_having_vectors(test_subs)\n",
    "\n",
    "assert len(train_query) == len(train_hyper)\n",
    "assert len(test_query) == len(test_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove OOV from synonym list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in list(synonyms.items()):\n",
    "    if k not in model:\n",
    "        synonyms.pop(k)\n",
    "    else:\n",
    "        for word in v:\n",
    "            if word not in model:\n",
    "                v.remove(word)\n",
    "    \n",
    "# flatten list of synonyms    \n",
    "syns = [word for v in synonyms.values() for word in v]    \n",
    "\n",
    "# confirm that all words in synonym vocab have embeddings representation\n",
    "assert len(list(filter(lambda x: x in model, syns)))==len(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class Data which encapsulates all the bits and pieces we require for training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, train_query, train_hyper, test_query, test_hyper, synonyms, embeddings):\n",
    "        # construct vocab made up from term and hypernyms \n",
    "        # we will choose negative samples from this vocab after exhausting\n",
    "        # the synonyms\n",
    "        self.neg_vocab = set(train_hyper + test_hyper)\n",
    "        \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.synonyms = synonyms\n",
    "        \n",
    "        # calculate size of term and hypernym dataset (train + test)\n",
    "        n_hyponyms = len(set(train_query + test_query + syns))\n",
    "        # hypernyms will be introduced in the model as either training,\n",
    "        # gold positives, test gold positives (when evaluation) or\n",
    "        # negative synonyms.\n",
    "        n_hypernyms = len(set(train_hyper + test_hyper))\n",
    "\n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['animal'].shape[0]\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words = n_hyponyms + n_hypernyms + 1, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~)')\n",
    "        self.tokenizer.fit_on_texts(train_query + train_hyper + test_query + test_hyper + syns)\n",
    "        \n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings[word]\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector (already normalised)\n",
    "                    #embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Data(train_query, train_hyper, test_query, test_hyper, synonyms, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first exhaust synonyms;\n",
    "# find the rest by drawing random terms from neg_vocab;\n",
    "# however, make sure that chosen words are not valid hypernyms;\n",
    "# finally, tokenise back to ids;    \n",
    "\n",
    "\n",
    "# positive_sample and terms both expect tuples where positive_sample = (query, hyper)\n",
    "# and terms = (all_query_terms, all_hyper_terms)\n",
    "def get_negative_words(positive_sample, word_hypernyms, data, sample_size=5):\n",
    "    neg_samples = []\n",
    "    # we need to make a copy of the synonym list\n",
    "    # synonmys will form part of out negative examples\n",
    "    if positive_sample[0] in data.synonyms:\n",
    "        neg_samples = list(synonyms[positive_sample[0]])        \n",
    "    \n",
    "    # there might not be enough; compound with random words\n",
    "    if len(neg_samples) >= sample_size:\n",
    "        # jumble negative sample indices        \n",
    "        neg_samples = np.random.choice(neg_samples, sample_size, replace=False)\n",
    "    else:\n",
    "        # get current sample's hypernyms\n",
    "        positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "        \n",
    "        # eliminate correct hypernyms from neg_vocab\n",
    "        word_choice = [nv for nv in data.neg_vocab if nv not in positive_hypernyms and nv not in neg_samples]        \n",
    "        # choose m - len(neg_samples)\n",
    "        neg_samples.extend(np.random.choice(word_choice, (sample_size-len(neg_samples))).tolist())\n",
    "            \n",
    "    return neg_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_negative_words(positive_sample, word_hypernyms, data, sample_size=5):\n",
    "    neg_samples = []\n",
    "    \n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    sample_space = filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys())\n",
    "    neg_samples = np.random.choice(list(sample_space), sample_size, replace=False)\n",
    "    \n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find most similar words to given hypernym which is not valid hypernym of word\n",
    "def get_similar_hypernyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    word = data.tokenizer.word_index[positive_sample[1]]\n",
    "    candidate_words = list(filter(lambda w: w != word, data.tokenizer.index_word.keys()))\n",
    "    sims = list(map(lambda c: np.dot(data.embedding_matrix[c], data.embedding_matrix[word]), candidate_words))\n",
    "\n",
    "    # get 30 most similar words to hypernyms\n",
    "    most_sim_idx = np.argsort(sims)[::-1][:30]    \n",
    "    similar_hypernyms = [data.tokenizer.index_word[candidate_words[idx]] for idx in most_sim_idx]\n",
    "    \n",
    "    # make sure that similar words are not actual hypernyms    \n",
    "    positive_hypernym = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    return list(filter(lambda x: x not in positive_hypernym, similar_hypernyms))[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def get_similar_hyponyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])        \n",
    "\n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = list(filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys()))\n",
    "    \n",
    "    hypo_sims = list(map(lambda c: np.dot(        \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[0]]]), candidate_words))\n",
    "    \n",
    "    most_sim_idx = np.argsort(hypo_sims)[::-1][:sample_size]\n",
    "    return list(map(lambda i: candidate_words[i], most_sim_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def mix_sim_hyper_random(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # init neg_samples\n",
    "    neg_samples = []\n",
    "    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])    \n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = list(filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys()))\n",
    "    \n",
    "    \n",
    "    # find similarity of all candidate words w.r.t. current hypernym\n",
    "    hyper_sims = list(map(lambda c: np.dot(\n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[1]]]), candidate_words))\n",
    "\n",
    "    # get most similar word to hypernym which is not hypernym\n",
    "    most_sim_idx = np.argsort(hyper_sims)[::-1][0]    \n",
    "    # append most similar hypernym to negative samples\n",
    "    neg_samples.append(candidate_words[most_sim_idx])\n",
    "        \n",
    "    if len(neg_samples) < sample_size:\n",
    "        neg_samples.extend(get_negative_words(positive_sample, word_hypernyms, data, sample_size=sample_size-1))\n",
    "    \n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of tuples where every element follows (word, negative_word)\n",
    "def get_negative_tuples(terms, data, negative_words_lambda, sample_size):\n",
    "    # convert terms to dictionary\n",
    "    input_query, input_hyper = terms\n",
    "    unq_input_query = sorted(list(set(input_query)))\n",
    "    \n",
    "    word_hypernyms = {}\n",
    "    for w in unq_input_query:        \n",
    "        word_hypernyms[w] = [h for q, h in zip(input_query, input_hyper) if q == w]\n",
    "        \n",
    "            \n",
    "    negative_tuples = []    \n",
    "    for words in zip(*terms):\n",
    "        negatives = negative_words_lambda(words, word_hypernyms, data, sample_size)\n",
    "        negative_tuples.extend(\n",
    "                [(words, n) for n in negatives]\n",
    "        )    \n",
    "    return negative_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that returns negative samples alongside set of positive samples\n",
    "# we need to pass:\n",
    "# the batch hyponym terms, batch of hypernym terms, negative_tuples, tokenizer \n",
    "# to create sequences\n",
    "def extend_batch_with_negatives(batch_X_term, batch_X_hyper, negative_tuples,                              \n",
    "                                tokenizer):\n",
    "    # initialise negative tuples container\n",
    "    positive_words = [(tokenizer.index_word[term_id], tokenizer.index_word[hyper_id]) \\\n",
    "                          for term_id, hyper_id in zip(batch_X_term.flatten(), batch_X_hyper.flatten())]\n",
    "    \n",
    "    # tokenize -ve samples\n",
    "    neg_terms, neg_hyper = zip(*[(qh[0], h) for qh, h in negative_tuples if qh in positive_words])\n",
    "    \n",
    "    neg_terms_seq = tokenizer.texts_to_sequences(neg_terms)\n",
    "    neg_hyper_seq = tokenizer.texts_to_sequences(neg_hyper)\n",
    "\n",
    "    # before increasing size of our batch, let's set the actual y values\n",
    "    # the first n terms are true (1s), and the rest are the -ve samples (0)\n",
    "    batch_y_label = np.concatenate((\n",
    "            np.ones(batch_X_term.shape[0]),\n",
    "            np.zeros(len(neg_terms_seq))\n",
    "    ))\n",
    "    # finally, stack -ve sequences at the bottom of +ves to \n",
    "    # create our final training batch\n",
    "    # at most, batch size will be 192 samples            \n",
    "\n",
    "    batch_X_term = np.vstack((batch_X_term, np.array(neg_terms_seq)))\n",
    "    batch_X_hyper = np.vstack((batch_X_hyper, np.array(neg_hyper_seq)))\n",
    "    \n",
    "    return batch_X_term, batch_X_hyper, batch_y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_hypernyms_to_one_line(data):\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    one_line = {}\n",
    "    for w in ordered_queries:\n",
    "        word_hypernyms = [h for q, h in zip(data.test_query, data.test_hyper) if q == w]\n",
    "        one_line[w] = word_hypernyms\n",
    "    return one_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from task_scorer.py provided with shared task resources\n",
    "def mean_reciprocal_rank(r):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    r = np.asarray(r).nonzero()[0]\n",
    "    return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "def precision_at_k(r, k, n):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return (np.mean(r)*k)/min(k,n)\n",
    "    # Modified from the first version. Now the gold elements are taken into account\n",
    "\n",
    "# used by Ustalov from https://github.com/nlpub/hyperstar/blob/master/evaluate.py\n",
    "def compute_ats(data, measures):\n",
    "    return [sum(measures[j].values()) / len(data.test_query) for j in range(len(measures))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "def get_evaluation_scores(data, predictions):\n",
    "    all_scores = []    \n",
    "    scores_names = ['MRR', 'P@1', 'P@5', 'P@10']\n",
    "    for query, gold_hyps in convert_hypernyms_to_one_line(data).items():\n",
    "\n",
    "        avg_pat1 = []\n",
    "        avg_pat2 = []\n",
    "        avg_pat3 = []\n",
    "\n",
    "        pred_hyps = predictions[query]\n",
    "        gold_hyps_n = len(gold_hyps)    \n",
    "        r = [0 for i in range(15)]\n",
    "\n",
    "        for j in range(len(pred_hyps)):\n",
    "            if j < gold_hyps_n:\n",
    "                pred_hyp = pred_hyps[j]\n",
    "                if pred_hyp in gold_hyps:\n",
    "                    r[j] = 1\n",
    "\n",
    "        avg_pat1.append(precision_at_k(r,1,gold_hyps_n))\n",
    "        avg_pat2.append(precision_at_k(r,5,gold_hyps_n))\n",
    "        avg_pat3.append(precision_at_k(r,10,gold_hyps_n))    \n",
    "\n",
    "        mrr_score_numb = mean_reciprocal_rank(r)\n",
    "        avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "        avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "        avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "        score_results = [mrr_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "        all_scores.append(score_results)\n",
    "    return scores_names, all_scores\n",
    "\n",
    "def get_ustalov_evaluation_scores(data, predictions):\n",
    "    measures = [{} for _ in range(10)]\n",
    "\n",
    "    for i, (t,h) in enumerate(zip(data.test_query, data.test_hyper)):\n",
    "        actual = predictions[t]\n",
    "        for j in range(0, len(measures)):\n",
    "            measures[j][(t, h)] = 1. if h in actual[:j + 1] else 0.\n",
    "\n",
    "    ats = compute_ats(data, measures) \n",
    "    return ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Projection Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Phi layer initialiser\n",
    "def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "    rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05)\n",
    "    #identity = K.eye(shape[-1], dtype='float32')        \n",
    "    rident = tf.eye(shape[-1]) * rnorm\n",
    "    return rident\n",
    "\n",
    "def random_normal(shape, dtype=\"float32\", partition_info=None): \n",
    "    return K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.05) \n",
    "\n",
    "def get_CRIM_model(phi_k=1, train_embeddings=False,\\\n",
    "                   embeddings_dim=300, vocab_size=1000,\\\n",
    "                   embeddings_matrix=None,\n",
    "                   phi_init = None,\n",
    "                   phi_activity_regularisation = None,\n",
    "                   sigmoid_kernel_regularisation = None,\n",
    "                   sigmoid_bias_regularisation = None,\n",
    "                   sigmoid_kernel_constraint = None,\n",
    "                   do_dropout = False\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size + 1, embeddings_dim, embeddings_constraint = UnitNorm(axis=1), \n",
    "                                name='TermEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = embedding_layer(hypo_input)    \n",
    "    hyper_embedding = embedding_layer(hyper_input)\n",
    "    \n",
    "    # Add Dropout to avoid overfit\n",
    "    if do_dropout:\n",
    "        hypo_embedding = Dropout(0.25)(hypo_embedding)\n",
    "        hyper_embedding = Dropout(0.25)(hyper_embedding)\n",
    "    \n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init,                               \n",
    "                               name='Phi%d' % (i))(hypo_embedding))\n",
    "\n",
    "    #phi1 = Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                #kernel_initializer=random_identity, name='Phi1')(hypo_embedding)\n",
    "\n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "    \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=Ones,\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer=Zeros,                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       )(phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "        \n",
    "    # inject pre-trained embedding weights into Embedding layer\n",
    "    model.get_layer(name='TermEmbedding').set_weights([embeddings_matrix])\n",
    "    model.get_layer(name='TermEmbedding').trainable = train_embeddings    \n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training algorithm incorporates mini-batch stochastic descent and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,       # the model which parameters will be learnt\n",
    "          epochs,      # number of epochs to run          \n",
    "          batch_size,  # size of mini-batch\n",
    "          m,           # number of negative samples\n",
    "          data,        # data required for training                              \n",
    "          neg_strategy\n",
    "         ):\n",
    "\n",
    "    # create negative tuples\n",
    "    #negative_tuples = get_negative_tuples(data.train_query + data.test_query,\n",
    "     #                                     data.train_hyper + data.test_hyper, data.neg_vocab, m)\n",
    "    \n",
    "    print (\"Generating negative tuples...\")\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_strategy, m)\n",
    "    print (\"Negative tuples...ok\")\n",
    "    \n",
    "    # create sequences\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "                \n",
    "    samples = np.arange(len(term_train_seq))\n",
    "    validation_samples = np.arange(len(term_test_seq))\n",
    "    \n",
    "    # train algorithm\n",
    "    for epoch in range(epochs):\n",
    "        # reset loss\n",
    "        loss = 0.\n",
    "        test_loss = 0.\n",
    "                        \n",
    "        np.random.shuffle(samples)        \n",
    "\n",
    "        shuffled_X_term, shuffled_X_hyper =\\\n",
    "            np.array(term_train_seq, dtype='int32')[samples],\\\n",
    "            np.array(hyper_train_seq, dtype='int32')[samples]\n",
    "\n",
    "        for b in range(0, len(samples), batch_size):\n",
    "            # product mini-batch, consisting of 32 +ve samples\n",
    "            batch_X_term = shuffled_X_term[b:b + batch_size] \n",
    "            batch_X_hyper = shuffled_X_hyper[b:b + batch_size]\n",
    "\n",
    "            # complement +ve samples with negatives\n",
    "            batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "            extend_batch_with_negatives(batch_X_term, batch_X_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "            \n",
    "            # shuffle validation set indices\n",
    "            np.random.shuffle(validation_samples)\n",
    "            # pick batch of shuffled test instances with size equal to training batch\n",
    "            batch_X_test_term, batch_X_test_hyper =\\\n",
    "                np.array(term_test_seq, dtype='int32')[validation_samples[:batch_size]],\\\n",
    "                np.array(hyper_test_seq, dtype='int32')[validation_samples[:batch_size]]\n",
    "            \n",
    "            # distort test batch with some negatives to check how algorithm fares with\n",
    "            # negatives\n",
    "            batch_X_test_term, batch_X_test_hyper, batch_y_test_label =\\\n",
    "            extend_batch_with_negatives(batch_X_test_term, batch_X_test_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "\n",
    "            # train on batch\n",
    "            loss += model.train_on_batch([batch_X_term, batch_X_hyper], \n",
    "                                          batch_y_label)[0]\n",
    "            \n",
    "            test_loss += model.test_on_batch([batch_X_test_term, batch_X_test_hyper], \n",
    "                                              batch_y_test_label)[0]                \n",
    "            \n",
    "        print('Epoch:', epoch+1, 'Loss:', loss, 'Test Loss:', test_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "#rand_norm_m0_sd001 = RandomNormal(mean = 0.0, stddev=0.01, seed=42)\n",
    "#rand_norm = RandomNormal(mean = 0.0, stddev=1., seed=42)\n",
    "\n",
    "# negative sampling options\n",
    "neg_sampling_options = {'synonym':get_negative_words, \n",
    "                        'mix_hyper_synonym': mix_sim_hyper_random,\n",
    "                        'similar_hyponym': get_similar_hyponyms,\n",
    "                        'random': get_random_negative_words\n",
    "                       }\n",
    "\n",
    "# phi random init options\n",
    "phi_init_options = {'random_identity': random_identity, 'random_normal': random_normal}\n",
    "\n",
    "# implement mini-batch stochastic training\n",
    "epochs = 15\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# number of negative samples\n",
    "m = 10\n",
    "phi_k = 1\n",
    "train_embeddings = False\n",
    "negative_option = 'mix_hyper_synonym'\n",
    "phi_init_option = 'random_identity'\n",
    "do_dropout = False\n",
    "np.random.seed(42)\n",
    "\n",
    "# create model\n",
    "crim_model = get_CRIM_model(phi_k = phi_k, train_embeddings = train_embeddings,\n",
    "                            embeddings_dim = data.embeddings_dim, vocab_size = len(data.tokenizer.word_counts),\n",
    "                            embeddings_matrix = data.embedding_matrix,\n",
    "                            phi_init = phi_init_options[phi_init_option],                            \n",
    "                            sigmoid_kernel_regularisation = l2(0.001),\n",
    "                            sigmoid_bias_regularisation = l2(0.001),\n",
    "                            sigmoid_kernel_constraint = None,#ForceToOne(),\n",
    "                            do_dropout = do_dropout\n",
    "                           )\n",
    "\n",
    "print (\"Training started...\")\n",
    "print ('Epochs: ', epochs, 'Batch size: ', batch_size, 'm: ', m, 'pki_k: ', phi_k, 'train_embeddings: ', train_embeddings,\n",
    "      'Negative sampling: ', negative_option, 'Phi Init: ', phi_init_option, 'Dropout: ', do_dropout)\n",
    "\n",
    "train(crim_model, epochs, batch_size, m, data, neg_sampling_options[negative_option])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation  code\n",
    "\n",
    "Main observations:<br>\n",
    "1. Tendency is for the model to overfit if we make the model larger than 1 projection matrix;\n",
    "1. Negative samples are important for the model to learn which words are not hypernyms;\n",
    "1. Although the model does seem to learn the correct words that are related to hypernymy to the query terms, it does not stop it from predicting with high confidence that similar but completely unrelated words are also hypernyms;\n",
    "    1. This is really apparent for animals where the model is not able to distinguish between vertebrate and invertebrate; mammal; animal; and so forth;\n",
    "    1. It's possible that we did not have enough examples to distinguish the various types of animals from each other;\n",
    "    1. Also, more targeted negative samples could have helped but these would have to be hand-created;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test whether two words are related by hypernymy\n",
    "i = data.tokenizer.word_index['pool']\n",
    "j = data.tokenizer.word_index['group']\n",
    "crim_model.predict([[i], [j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find candidate hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# alternative hypernym generator by applying Phi weights to hyponym and see which \n",
    "# words are closest to this vector\n",
    "def alt_get_hypernym(word, model, data, embeddings, top, bias = 0):\n",
    "    q_idx = data.tokenizer.word_index[word]    \n",
    "    \n",
    "    q = embeddings[q_idx]    \n",
    "        \n",
    "    try:\n",
    "        _phi = model.get_layer(name='Phi0').get_weights()[0]\n",
    "    except ValueError:\n",
    "        _phi = model.get_layer(name='Phi').get_weights()[0]\n",
    "             \n",
    "    _proj = np.dot(q, _phi)\n",
    "    #_proj /= np.linalg.norm(_proj)\n",
    "    \n",
    "    #sim = cosine_similarity(embeddings[1:], _proj.reshape(1,-1)).flatten() \n",
    "    sim = np.array(list(map(lambda v: np.dot(v, _proj), embeddings[1:]))) + bias\n",
    "    \n",
    "    return list(map(lambda i: (data.tokenizer.index_word[i+1], sim[i]), np.argsort(sim)[::-1][:top]))\n",
    "\n",
    "# compare product of hypoynm word embedding and Phi to all vectors in embeddings. Returns gibberish\n",
    "def ustalov_get_hypernyms(word, _model, data, embeddings, top):\n",
    "    q_idx = data.tokenizer.word_index[word]        \n",
    "    q = embeddings[q_idx]       \n",
    "    \n",
    "    _phi = _model.get_layer(name='Phi0').get_weights()[0]\n",
    "        \n",
    "    Y_hat = np.dot(q, _phi)\n",
    "    Y_hat /= np.linalg.norm(Y_hat)    \n",
    "    \n",
    "    return model.similar_by_vector(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function which generates top 15 predictions for each hyponym query term\n",
    "# and returns results as dictionary\n",
    "def predict_crim_hypernyms(data, model):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    results = {}\n",
    "        \n",
    "    embeddings = crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0]                     \n",
    "    for idx, word in enumerate(ordered_queries):        \n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "        #predicted_hypers = crim_get_top_hypernyms(word, hyper_candidates, model, data, 15)\n",
    "        predicted_hypers = alt_get_hypernym(word, model, data, embeddings, 15)\n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "crim_predictions = predict_crim_hypernyms(data, crim_model)\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores(data, crim_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "print (\"\")\n",
    "print (\"Ustalov-style evaluation:\")\n",
    "ats = get_ustalov_evaluation_scores(data, crim_predictions)\n",
    "ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "print (ats_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yamane et al. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attempt custom constraint to keep weight fixed at 1.\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_new_cluster_model(embedding_layer, \n",
    "                          phi_dim = 300,\n",
    "                          phi_init = None,\n",
    "                          phi_activity_regularisation = None,                          \n",
    "                          sigmoid_bias_regularisation = None,\n",
    "                          sigmoid_kernel_constraint = None,\n",
    "                          do_dropout = False):\n",
    "    \n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')    \n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    hypo_embedding, hyper_embedding = embedding_layer([hypo_input, hyper_input])\n",
    "    \n",
    "    if do_dropout:\n",
    "        hypo_embedding = Dropout(0.25)(hypo_embedding)\n",
    "        hyper_embedding = Dropout(0.25)(hyper_embedding)\n",
    "                \n",
    "    phi = Dense(phi_dim, activation=None, use_bias=False,                 \n",
    "                kernel_initializer=phi_init,                \n",
    "                name='Phi0')(hypo_embedding)\n",
    "    \n",
    "    # flatten phi and hyper_embedding tensors\n",
    "    phi = Flatten()(phi)\n",
    "    hyper_embedding = Flatten()(hyper_embedding)\n",
    "    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])    \n",
    "    \n",
    "    predictions = Dense(1, activation = \"sigmoid\", \n",
    "                        bias_initializer = Zeros,\n",
    "                        kernel_initializer = Ones,\n",
    "                        #kernel_constraint = sigmoid_kernel_constraint,                        \n",
    "                        kernel_constraint = \"UnitNorm\",                        \n",
    "                        bias_regularizer=sigmoid_bias_regularisation, \n",
    "                        name='Prediction')(phi_hyper)\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "    \n",
    "    # compile using binary_crossentropy loss\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We don't need a unique embedding layer for every sub-model.  \n",
    "\n",
    "# Instead, we can create a separate model for the embeddings and set the weights \n",
    "# according to the pre-trained embeddings\n",
    "\n",
    "def get_embeddings_model(dim, embedding_matrix):\n",
    "    hypo_input = Input(shape=(1,))\n",
    "    hyper_input = Input(shape=(1,))\n",
    "\n",
    "    word_embedding = Embedding(embedding_matrix.shape[0], dim, name='WE')\n",
    "\n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    embedding_model = Model(inputs=[hypo_input, hyper_input], outputs=[hypo_embedding, hyper_embedding])\n",
    "\n",
    "    # inject pre-trained embeddings into this mini, resusable model/layer\n",
    "    embedding_model.get_layer(name='WE').set_weights([embedding_matrix])\n",
    "    embedding_model.get_layer(name='WE').trainable = False\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YamaneCluster:\n",
    "    def __init__(self, embedding_layer, phi_dim, phi_init, sigmoid_kernel_constraint):\n",
    "        \n",
    "        self.model = get_new_cluster_model(embedding_layer = embedding_layer, \n",
    "                                           phi_dim = phi_dim, \n",
    "                                           phi_init = phi_init, \n",
    "                                           sigmoid_kernel_constraint = sigmoid_kernel_constraint)\n",
    "        self.epoch_count = 0\n",
    "        self.loss = 0.\n",
    "        self.test_loss = 0.\n",
    "    \n",
    "    def increment_epoch(self):\n",
    "        self.epoch_count += 1\n",
    "        \n",
    "    def update_loss(self, new_loss):\n",
    "        self.loss += new_loss\n",
    "        \n",
    "    def update_test_loss(self, new_loss):\n",
    "        self.test_loss += new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yamane_train(\n",
    "    epochs,      # number of epochs to run\n",
    "    m,           # number of negative samples\n",
    "    data,        # class instance containing all the data required for training/testing        \n",
    "    embedding_layer,\n",
    "    threshold    = 0.15,     # threshold; similarity below this score will trigger new cluster\n",
    "    negative_option = 'random', # inject lambda responsible for determining negative sample choice\n",
    "    phi_init_option = None,\n",
    "    sigmoid_kernel_constraint = None):    \n",
    "  \n",
    "    \n",
    "    neg_sampling_options = {'synonym':get_negative_words, \n",
    "                            'mix_hyper_synonym': mix_sim_hyper_random,\n",
    "                            'similar_hyponym': get_similar_hyponyms,\n",
    "                            'random': get_random_negative_words\n",
    "                           }\n",
    "    \n",
    "    phi_init_options = {'random_identity': random_identity, 'random_normal': random_normal}\n",
    "        \n",
    "    print (\"Generating negative tuples...\")\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_sampling_options[negative_option], m)\n",
    "    print (\"Negative tuples...ok\")\n",
    "    \n",
    "    # create sequences\n",
    "    # we have two sets of inputs: one for training query and hypernym terms;\n",
    "    #                             another for the validation query/hyper terms;\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "    \n",
    "    # convert all to arrays\n",
    "    term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq =\\\n",
    "    [np.array(x, dtype='int32') for x in [term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq]]\n",
    "            \n",
    "    # this list stores which cluster each training sequence pertains to\n",
    "    sample_clusters = np.zeros(len(term_train_seq), dtype='int32')\n",
    "    \n",
    "    print (\"m: \", m, \"lambda: \", threshold, \"max epoch per cluster: \", epochs, \n",
    "           \"Negative sampling: \", negative_option, 'Phi Init: ', phi_init_option)\n",
    "    \n",
    "    \n",
    "    print (\"Sample clusters size: \", len(sample_clusters))\n",
    "    # list containing 1 model per cluster\n",
    "    clusters = []\n",
    "    # add default model to our list of models\n",
    "    # we share the embedding layer loaded with the pre-trained weights\n",
    "    # append tuple where 1st element is the cluster and 2nd element is the \n",
    "    # number of epochs that cluster is trained\n",
    "    \n",
    "    clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim,\n",
    "                                  phi_init = phi_init_options[phi_init_option], sigmoid_kernel_constraint = sigmoid_kernel_constraint))\n",
    "    \n",
    "    # get training set indices\n",
    "    indices = np.arange(len(term_train_seq))  \n",
    "    \n",
    "    # get test set indices\n",
    "    test_indices = np.arange(len(term_test_seq))\n",
    "            \n",
    "    # initialise each training sample to cluster 0\n",
    "    sample_clusters[indices] = 0        \n",
    "    \n",
    "    # seed random generator\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # indicator of \"current\" sample cluster index\n",
    "    z_i = 0\n",
    "    \n",
    "    # train algorithm\n",
    "    #for epoch in range(epochs):\n",
    "    epoch = 0    \n",
    "    \n",
    "    while np.min([c.epoch_count for c in clusters]) < epochs:\n",
    "        # reset loss for each cluster                        \n",
    "        for c in clusters:\n",
    "            if c.epoch_count < epochs:                \n",
    "                c.loss = 0.\n",
    "            c.test_loss = 0.                \n",
    "        \n",
    "        # shuffle indices every epoch\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # train algorithm by stochastic gradient descent, one sample at a time\n",
    "        for idx, i in enumerate(indices):                        \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print (\"Processed \", idx+1, \"samples...\")\n",
    "            \n",
    "            # calculate similarity on all clusters\n",
    "            sim = list(map(lambda x: x.model.predict([term_train_seq[i], hyper_train_seq[i]]), clusters))\n",
    "            max_sim = np.argmax(sim)\n",
    "            #print \"Term:\", tokenizer.index_word[term_train_seq[i][0]], 'Hyper:', tokenizer.index_word[hyper_train_seq[i][0]], \"Max Similarity cluster:\", max_sim, \"(sim = %0.8f)\" % (sim[max_sim])\n",
    "            # limit cluster creation to a max of 25.\n",
    "            if ((sim[max_sim] < threshold) and (len(clusters) < 25)): \n",
    "                # add new cluster to list of clusters\n",
    "                clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim,\n",
    "                                phi_init = phi_init_options[phi_init_option], sigmoid_kernel_constraint = sigmoid_kernel_constraint))\n",
    "                \n",
    "                # assign current cluster index to latest model\n",
    "                z_i = len(clusters) - 1\n",
    "                sample_clusters[i] = z_i\n",
    "            else:            \n",
    "                z_i = max_sim\n",
    "                sample_clusters[i] = z_i                \n",
    "                        \n",
    "            # if current cluster reached/exceeded epoch count, skip current sample (i.e don't update cluster)\n",
    "            if clusters[z_i].epoch_count < epochs:                                            \n",
    "                # extend samples in cluster with negative samples\n",
    "                batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "                    extend_batch_with_negatives(term_train_seq[i], \n",
    "                                                hyper_train_seq[i],\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "\n",
    "                # update parameters of cluster \n",
    "                clusters[z_i].update_loss(\n",
    "                    clusters[z_i].model.train_on_batch([batch_X_term, batch_X_hyper], batch_y_label)[0]\n",
    "                )\n",
    "            \n",
    "            # measure test loss \n",
    "            # every 32 samples (and updates are processed), we will test performance on validation set\n",
    "            # of 32 randomly chosen samples. We will record test loss of every cluster and report on \n",
    "            # lowest loss\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                np.random.shuffle(test_indices)\n",
    "                batch_query, batch_hyper = term_test_seq[test_indices[:32]], hyper_test_seq[test_indices[:32]]\n",
    "                batch_query, batch_hyper, test_y_label =\\\n",
    "                    extend_batch_with_negatives(batch_query, \n",
    "                                                batch_hyper,\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "                #batch_label = [1.] * batch_query.shape[0]\n",
    "                for q, h, l in zip(batch_query, batch_hyper, test_y_label):                                    \n",
    "                    test_losses = list(map(lambda c: c.model.test_on_batch([q, h], [l])[0], clusters))\n",
    "                    best_cluster = np.argmin(test_losses)\n",
    "                    clusters[best_cluster].update_test_loss(\n",
    "                        test_losses[best_cluster]\n",
    "                    )                    \n",
    "                                                                                                                      \n",
    "        # increase epoch count for clusters\n",
    "        for cluster in clusters:            \n",
    "            cluster.epoch_count += 1\n",
    "                \n",
    "        print('Epoch:', max([c.epoch_count for c in clusters]), 'Cluster #:', len(clusters) ,\n",
    "              'Loss:', np.mean([c.loss for c in clusters]),\n",
    "              'Test Loss:', np.mean([c.test_loss for c in clusters]))\n",
    "    return clusters, sample_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "# initialise embedding later which will be shared among all clusters\n",
    "embedding_layer = get_embeddings_model(dim=data.embeddings_dim, embedding_matrix=data.embedding_matrix)\n",
    "epochs = 15\n",
    "m = 10\n",
    "\n",
    "print (\"Training started...\")\n",
    "clusters, sample_clusters =\\\n",
    "    yamane_train(epochs, m, \n",
    "                 data,\n",
    "                 embedding_layer,\n",
    "                 threshold = 0.2,\n",
    "                 negative_option = 'mix_hyper_synonym',\n",
    "                 phi_init_option = 'random_normal',\n",
    "                 sigmoid_kernel_constraint = ForceToOne())\n",
    "\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KNN classifier on clustering data jointly learnt by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "# prepare knn dataset based on learnt clusters\n",
    "train_seq = np.array(data.tokenizer.texts_to_sequences(train_query))\n",
    "\n",
    "X_knn = {}\n",
    "for idx, c in enumerate(clusters):\n",
    "    cluster_ids = np.where(sample_clusters == idx)\n",
    "    # we can reduce duplicate terms to unique terms    \n",
    "    uniq_terms = np.unique(train_seq[cluster_ids])\n",
    "    #print (uniq_terms)    \n",
    "    X_knn[idx] = data.embedding_matrix[uniq_terms]  \n",
    "\n",
    "X_features = X_knn[0]\n",
    "y = np.zeros(X_knn[0].shape[0], dtype='int16')\n",
    "\n",
    "for k in range(1,len(clusters)):\n",
    "    X_features = np.vstack((X_features, X_knn[k]))\n",
    "    y = np.hstack((y, np.array([k] * X_knn[k].shape[0])))\n",
    "    \n",
    "neigh.fit(X_features, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yamane Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check likelihood of terms semantically related by hypernmy over all clusters\n",
    "i = data.tokenizer.word_index['baby']\n",
    "j = data.tokenizer.word_index['child']\n",
    "list(map(lambda c: c.model.predict([[i], [j]]),  clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yamane Clusters\n",
    "\n",
    "Analysing the hypernym element of the training pairs consistuting a cluster, a pattern emerges. The training algorithm has realised a semantic split and has clustered terms around particular hypernyms. \n",
    "Give a model trained on the following parameters:\n",
    "* m:  10 lambda:  0.15 max epoch per cluster:  15 Negative sampling:  random Phi Init:  random_normal\n",
    "\n",
    ", the most populated clusters where in descending order: \n",
    "* Counter({4: 743, 0: 674, 7: 311, 1: 279, 5: 236, 13: 166, 11: 149, 22: 139, 23: 138, 6: 135, 24: 118, 21: 117, 18: 116, 19: 108, 17: 106, 3: 105, 20: 97, 14: 94, 16: 90, 15: 89, 9: 80, 10: 78, 12: 72, 8: 68, 2: 66})\n",
    "\n",
    "For instance, cluster 4, the most populated clusters, features animals (282), mammals (119) and birds (50).  Cluster 0 (second most populated), features chordates (203), vertebrates(201), placentals (108) and invertebrates (56). <br>\n",
    "\n",
    "It is likely that the model is learning the optimal projections to transform any given hyponym to the hypernym which features frequently in a cluster.  For instance if we multiply the embedding of\"barley\" with projection matrix learnt from the samples in cluster 0, the resultant vector would be similar to \"vertebrate\" even though barley is clearly not an animal.  \n",
    "\n",
    "When we compute the product of the projection matric in every cluster and compare the result with every word in the vocab, the top 15 most similar words are:\n",
    "\n",
    "[('plant', 9.292432),<br>\n",
    " ('food', 5.7546735),<br>\n",
    " ('herb', 5.4948673),<br>\n",
    " ('vertebrate', 5.256508),<br>\n",
    " ('chordate', 4.754751),<br>\n",
    " ('cereal', 4.3708754),<br>\n",
    " ('placental', 4.155033),<br>\n",
    " ('grass', 4.0776873),<br>\n",
    " ('factory', 3.7920942),<br>\n",
    " ('beverage', 3.3642926),<br>\n",
    " ('shrub', 3.3227572),<br>\n",
    " ('animal', 3.1966455),<br>\n",
    " ('produce', 2.8567104),<br>\n",
    " ('ruminant', 2.2009413),<br>\n",
    " ('angiosperm', 2.1797898)]<br>\n",
    "\n",
    "Turns out that 5 (out of 6)  of the hypernyms in the gold test data were correctly generated.  However note that chordate, vertebrate and placental features in the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check out words in a particular cluster\n",
    "hypers_in_cluster = list(map(lambda x: data.train_hyper[x], np.where(sample_clusters == 15)[0]))\n",
    "\n",
    "freq_hyper_cluster = Counter(hypers_in_cluster)\n",
    "sorted(freq_hyper_cluster.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return some basic cluster data\n",
    "from collections import Counter\n",
    "# show distribution of samples over the trained clusters\n",
    "print (Counter(sample_clusters))\n",
    "print (\"-\"*30)\n",
    "print (\"Train and test loss per cluster\")\n",
    "\n",
    "for idx, c in enumerate(clusters):\n",
    "    print (idx, c.epoch_count, c.loss, c.test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Yamane candidate hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alt_yamane_get_top(word, hyper_candidates, clusters, data, top, bias_list):\n",
    "    yam_results = []\n",
    "    for idx, c in enumerate(clusters):\n",
    "        yam_results.extend(\\\n",
    "            alt_get_hypernym(word, c.model, data, embedding_layer.get_layer(name=\"WE\").get_weights()[0], 10, bias=yamane_bias[idx])\n",
    "                          )\n",
    "    return sorted(yam_results, key= lambda x:x[1], reverse=True)[:top]\n",
    "\n",
    "\n",
    "def predict_yamane_hypernyms(data, clusters):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    \n",
    "    # store biases in list\n",
    "    yamane_bias = list(map(lambda c: c.model.get_layer('Prediction').get_weights()[1][0], clusters))\n",
    "    \n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    \n",
    "    results = {}\n",
    "    for idx, word in enumerate(ordered_queries):\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "        \n",
    "        # find clusters best suited to this word\n",
    "        word_id = data.tokenizer.word_index[word]\n",
    "        \n",
    "        cluster_probs = neigh.predict_proba(data.embedding_matrix[word_id].reshape(1,-1))\n",
    "        cluster_idx = np.where(cluster_probs > 0.)[1]\n",
    "        #print (cluster_idx)\n",
    "        specific_clusters = map(lambda c: clusters[c], cluster_idx)\n",
    "        specific_bias = map(lambda c: yamane_bias[c], cluster_idx)\n",
    "\n",
    "        predicted_hypers = alt_yamane_get_top(word, hyper_candidates, specific_clusters, data, 15, specific_bias )\n",
    "                \n",
    "        #predicted_hypers = yamane_get_top_hypernym(word, hyper_candidates, clusters, data, 15)\n",
    "        #predicted_hypers = alt_yamane_get_top(word, hyper_candidates, clusters, data, 15, yamane_bias)        \n",
    "        \n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute Yamane model performance results\n",
    "\n",
    "import math\n",
    "\n",
    "yamane_predictions = predict_yamane_hypernyms(data, clusters)\n",
    "\n",
    "print (\"Yamane evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores(data, yamane_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "print (\"\")\n",
    "print (\"Ustalov-style evaluation:\")\n",
    "ats = get_ustalov_evaluation_scores(data, yamane_predictions)\n",
    "ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "print (ats_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_negative_tuples((data.test_query, data.test_hyper), data, get_random_negative_words, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspect CRIM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have a look at the prediction layer weights\n",
    "print (crim_model.get_layer(name='Prediction').get_weights())\n",
    "#projs = ['Phi0', 'Phi1', 'Phi2', 'Phi3', 'Phi4']\n",
    "projs = ['Phi0']\n",
    "for p in projs:\n",
    "    print (np.mean(crim_model.get_layer(name=p).get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find hypernyms by running every vocab word against each query term and finding prediction.\n",
    "# Rather slow and inelegant.  Found better (faster) way to evaluate model by applying Phi learnt projection matrix\n",
    "# to the query term and then look for most similar words in vocab using cosine simalarity.\n",
    "\n",
    "# If single projection matrix is learnt, we can easily ignore LR parameters (scalar multiple) as well as bias\n",
    "# (applied equally to all candidate hypernyms )\n",
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "\n",
    "def crim_get_top_hypernyms(query, hyper_candidates, model, data, top):\n",
    "    query_index = data.tokenizer.word_index[query]    \n",
    "    valid_candidates = list(filter(lambda w: w != [query_index], hyper_candidates))\n",
    "    \n",
    "    candidate_sim = list(map(lambda x: model.predict([[query_index], x]).flatten()[0], valid_candidates))       \n",
    "    top_idx = np.argsort(candidate_sim)[::-1][:top]    \n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "    \n",
    "    return [(data.tokenizer.index_word[t], candidate_sim[top_idx[i]]) for i, t in enumerate(top_hyper)]\n",
    "\n",
    "\n",
    "crim_get_top_hypernyms('sofa', hyper_candidates, crim_model, data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test get hypernym routine\n",
    "alt_get_hypernym('deaf',crim_model, data, crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exceedingly slow, but technically correct method to find most likely hypernyms for all test terms\n",
    "def yamane_get_top_hypernym(query, hyper_candidates, clusters, data, top):    \n",
    "    query_index = data.tokenizer.word_index[query]\n",
    "    # remove actual query from candidates    \n",
    "    valid_candidates = list(filter(lambda x: x[0]!=query_index, hyper_candidates))\n",
    "    hyper_probs = []\n",
    "    for idx, hyper in enumerate(valid_candidates):    \n",
    "        if (idx+1) % 500 == 0:\n",
    "            print (\"Done\", idx+1)\n",
    "        candidate_sim = list(map(lambda x: x.model.predict([[query_index], hyper]).flatten()[0], clusters))\n",
    "        hyper_probs.append(np.max(candidate_sim))\n",
    "    \n",
    "    top_idx = np.argsort(hyper_probs)[::-1][:top]\n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "            \n",
    "    return [(data.tokenizer.index_word[t], hyper_probs[top_idx[i]]) for i, t in enumerate(top_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yamane_predictions = predict_yamane_hypernyms(data, clusters)\n",
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "yamane_bias = list(map(lambda c: c.model.get_layer('Prediction').get_weights()[1][0], clusters))\n",
    "alt_yamane_get_top('deaf', hyper_candidates, clusters, data, 15, yamane_bias )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
