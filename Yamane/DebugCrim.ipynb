{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape, Dropout, Lambda, Subtract\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.1  , -0.5  ,  0.4  ,  0.8  , -0.7  ],\n",
       "       [ 0.3  ,  0.2  ,  0.1  , -0.23 ,  0.1  ],\n",
       "       [-0.01 , -0.053,  0.08 , -0.001,  0.   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll pretent we're using embedding layers for our words\n",
    "\n",
    "# just two \"words\" in our vocab (input hypo, input hyper)\n",
    "input_hypo_seq = [[1]]\n",
    "input_hyper_seq = [[2]]\n",
    "\n",
    "embedding_matrix = np.zeros((4,5))\n",
    "input_a = np.array([[0.1, -0.5, 0.4, 0.8, -0.7]])\n",
    "input_b = np.array([[0.3, 0.2, 0.1, -0.23, 0.1]])\n",
    "input_c = np.array([[-0.01, -0.053, 0.08, -0.001, 0]])\n",
    "embedding_matrix[1] = input_a\n",
    "embedding_matrix[2] = input_b\n",
    "embedding_matrix[3] = input_c\n",
    "\n",
    "embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.sqrt(np.sum(embedding_matrix[1] ** 2))\n",
    "\n",
    "np.linalg.norm(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lambda():\n",
    "\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    #hypo_embedding = Flatten()(hypo_embedding)\n",
    "    sub = Subtract()([hyper_embedding, hypo_embedding])\n",
    "    sub = Lambda(lambda x : K.square(x))(sub)\n",
    "    sub = Flatten()(sub)\n",
    "\n",
    "    model_test = Model(inputs=[hypo_input, hyper_input], outputs=sub)\n",
    "    \n",
    "    model_test.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model_test.get_layer(name='WordEmbedding').trainable = False\n",
    "            \n",
    "    #model_test.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = test_lambda()\n",
    "print model_test.predict([[1], [2]])\n",
    "\n",
    "print (embedding_matrix[2] - embedding_matrix[1]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    # this one is custom and is based on the CRIM paper. \n",
    "    # we initialise on random normal noise applied to an identity matrix\n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "        return identity \n",
    "\n",
    "    def random_identity_2(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "        normal = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05) \n",
    "\n",
    "\n",
    "        return normal + identity\n",
    "        \n",
    "    \n",
    "    phi0 = Dense(5, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity, name='Phi0')(hypo_embedding)\n",
    "\n",
    "    #phi0 = Dropout(0.3, name='d1') (phi0,  training=True)\n",
    "    \n",
    "    phi1 = Dense(5, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity_2, name='Phi1')(hypo_embedding)\n",
    "\n",
    "\n",
    "    #phi1 = Dropout(0.3, name='d2') (phi1, training=True)\n",
    "        \n",
    "    phi = concatenate([phi0, phi1], axis=1, name='Phi')    \n",
    "    #phi = Dropout(0.1, name='drop')(phi, training=True)\n",
    "    \n",
    "    phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi)\n",
    "    \n",
    "    #phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    #phi_hyper = Flatten()(phi_hyper)\n",
    "    #phi_hyper = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi_hyper)\n",
    "    \n",
    "\n",
    "    #predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "    #                    kernel_initializer='random_normal',\n",
    "    #                    bias_initializer='random_normal'\n",
    "    #                   )(phi_hyper)\n",
    "\n",
    "    model_test = Model(inputs=[hypo_input, hyper_input], outputs=phi)\n",
    "    \n",
    "    model_test.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model_test.get_layer(name='WordEmbedding').trainable = False\n",
    "    \n",
    "    #inter_model = Model(inputs=model_test.input, outputs=model_test.get_layer(name='drop').output)\n",
    "    \n",
    "    #model_test.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print embedding_matrix\n",
    "\n",
    "model_test = build_model()\n",
    "\n",
    "print model_test.predict([[1], [2]])\n",
    "\n",
    "\n",
    "phi_0_W = model_test.get_layer(name='Phi0').get_weights()[0]\n",
    "phi_1_W = model_test.get_layer(name='Phi1').get_weights()[0]\n",
    "\n",
    "#print (\"Printing phi weights\")\n",
    "#print phi_0_W\n",
    "#print phi_1_W\n",
    "\n",
    "print (\"Printing hypo projections\")\n",
    "proj1 = np.dot(embedding_matrix[1], phi_0_W)\n",
    "proj2 = np.dot(embedding_matrix[1], phi_1_W)\n",
    "print proj1\n",
    "print proj2\n",
    "\n",
    "print (\"Printing mean hypo\")\n",
    "concat_W = np.vstack((proj1, proj2))\n",
    "print np.mean(concat_W, axis=0)\n",
    "#print (embedding_matrix[2] - embedding_matrix[1]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_loss(hypo_tensor, phi_weights):\n",
    "    def inner_product(y_true, y_pred):        \n",
    "        hypo_tensor_s = K.squeeze(hypo_tensor, axis=1)                                        \n",
    "        phi_weights_norm = K.l2_normalize(phi_weights, axis=1)\n",
    "        simil = y_true * 0.01 * K.mean(K.dot(phi_weights_norm, K.transpose(hypo_tensor_s)) ** 2)\n",
    "        #simil = K.dot(phi_weights_norm, K.transpose(hypo_tensor_s)) ** 2\n",
    "        return K.binary_crossentropy(y_true, y_pred) + simil                        \n",
    "        \n",
    "    return inner_product\n",
    "\n",
    "\n",
    "def custom_loss2(hypo_phi_tensor):\n",
    "    def inner_product(y_true, y_pred):                \n",
    "        simil = 0.1 * K.mean(hypo_phi_tensor ** 2, axis=-1)        \n",
    "        return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1) + simil                        \n",
    "                      \n",
    "                              \n",
    "    return inner_product\n",
    "\n",
    "def build_variant():\n",
    "\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    # this one is custom and is based on the CRIM paper. \n",
    "    # we initialise on random normal noise applied to an identity matrix\n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "        return identity \n",
    "\n",
    "    def random_identity_2(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "        normal = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05) \n",
    "\n",
    "\n",
    "        return normal + identity\n",
    "            \n",
    "    phi = Dense(5, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity, name='Phi0')(hypo_embedding)\n",
    "\n",
    "    phi = Flatten(name='FlattenPhi')(phi)\n",
    "    hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)\n",
    "            \n",
    "    #phi_hyper = Flatten()(phi_hyper)\n",
    "    phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct1')([phi, hyper_embedding])    \n",
    "    phi_hypo = Dot(axes=-1, normalize=False, name='DotProduct2')([phi, hypo_embedding])\n",
    "\n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        kernel_initializer='random_normal',\n",
    "                        bias_initializer='random_normal'\n",
    "                       )(phi_hyper)\n",
    "\n",
    "    model_test = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "        \n",
    "    #regul_loss = custom_loss(hypo_embedding, phi)\n",
    "    regul_loss = custom_loss2(phi_hypo)\n",
    "    \n",
    "    model_test.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model_test.get_layer(name='WordEmbedding').trainable = False\n",
    "    \n",
    "    #inter_model = Model(inputs=model_test.input, outputs=model_test.get_layer(name='drop').output)\n",
    "    \n",
    "    model_test.compile(optimizer='rmsprop', loss=regul_loss, metrics=['accuracy'])\n",
    "    \n",
    "    return model_test\n",
    "\n",
    "def build_classifier(feature_extractor):\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    s_vector = feature_extractor([hypo_input, hyper_input])\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        kernel_initializer='random_normal',\n",
    "                        bias_initializer='random_normal') (s_vector)\n",
    "    \n",
    "    model_test = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "    model_test.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = build_variant()\n",
    "model_test.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(hypo_tensor, phi_weights):\n",
    "    def inner_product(y_true, y_pred):        \n",
    "        hypo_tensor_s = K.squeeze(hypo_tensor, axis=1)                                        \n",
    "        phi_weights_norm = K.l2_normalize(phi_weights, axis=1)\n",
    "        simil = y_true * 0.01 * K.mean(K.dot(phi_weights_norm, K.transpose(hypo_tensor_s)) ** 2)\n",
    "        #simil = K.dot(phi_weights_norm, K.transpose(hypo_tensor_s)) ** 2\n",
    "        return K.binary_crossentropy(y_true, y_pred) + simil                        \n",
    "        \n",
    "    return inner_product\n",
    "\n",
    "\n",
    "def custom_loss2(random_sim):\n",
    "    def mse(y_true, y_pred):                \n",
    "        #K.print_tensor(y_true, message='Hello:')\n",
    "        #K.print_tensor(y_pred, message='Hello:')                        \n",
    "        return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)) + K.square(random_sim)\n",
    "                                                    \n",
    "    return mse\n",
    "\n",
    "def build_square_error():\n",
    "\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')\n",
    "    neg_input = Input(shape=(1,), name='Negative')\n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "        \n",
    "    hypo_embedding = word_embedding(hypo_input)   \n",
    "    neg_embedding = word_embedding(neg_input)\n",
    "\n",
    "    # this one is custom and is based on the CRIM paper. \n",
    "    # we initialise on random normal noise applied to an identity matrix\n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "        return identity \n",
    "\n",
    "    def random_identity_2(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "        normal = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05) \n",
    "\n",
    "\n",
    "        return normal + identity\n",
    "            \n",
    "    phi = Dense(5, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity, name='Phi0')(hypo_embedding)\n",
    "\n",
    "    phi = Flatten(name='FlattenPhi')(phi)\n",
    "    neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)\n",
    "    \n",
    "    # calculate similarity of projection with negative\n",
    "    random_sim = Dot(axes=-1, normalize=False, name='DotProductRand')([phi, neg_embedding])\n",
    "    mse = custom_loss2(random_sim)\n",
    "    \n",
    "    model_test = Model(inputs=[hypo_input, neg_input], outputs=phi)\n",
    "        \n",
    "    #regul_loss = custom_loss(hypo_embedding, phi)\n",
    "    \n",
    "    \n",
    "    model_test.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model_test.get_layer(name='WordEmbedding').trainable = False\n",
    "            \n",
    "    model_test.compile(optimizer='rmsprop', loss=mse, metrics=['accuracy'])\n",
    "    \n",
    "    return model_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     0.     0.     0.     0.   ]\n",
      " [ 0.1   -0.5    0.4    0.8   -0.7  ]\n",
      " [ 0.3    0.2    0.1   -0.23   0.1  ]\n",
      " [-0.01  -0.053  0.08  -0.001  0.   ]]\n",
      "[1.0397483, 0.0]\n",
      "1.0397483683564293\n"
     ]
    }
   ],
   "source": [
    "model_test = build_square_error()\n",
    "#model_test.summary()\n",
    "print embedding_matrix\n",
    "\n",
    "#print model_test.predict([[1], [2]])\n",
    "\n",
    "#print model_test.predict([[[1], [2]], [[3],[1]]])\n",
    "print model_test.test_on_batch([[[1], [2]], [[3],[1]]], embedding_matrix[2:4].reshape(2,-1))\n",
    "\n",
    "#print model_test.test_on_batch([[1, 2]], embedding_matrix[2:4].reshape(2,-1))\n",
    "\n",
    "a = np.sqrt(np.sum((embedding_matrix[1] - embedding_matrix[2] ) ** 2))\n",
    "b = np.sqrt(np.sum((embedding_matrix[2] - embedding_matrix[3] ) ** 2))\n",
    "\n",
    "#(a+b)/2\n",
    "#print b\n",
    "print (a + b + (np.dot(embedding_matrix[1], embedding_matrix[3]) ** 2) +\\\n",
    "              (np.dot(embedding_matrix[2], embedding_matrix[1]) ** 2))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_loss2(random_sim):\n",
    "    def err_diff(y_true, y_pred):                \n",
    "        #K.print_tensor(y_true, message='Hello:')\n",
    "        #K.print_tensor(y_pred, message='Hello:')                        \n",
    "        return (y_pred - y_true) + K.square(random_sim)\n",
    "                                                    \n",
    "    return err_diff\n",
    "\n",
    "def build_square_error2():\n",
    "\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')    \n",
    "    neg_input = Input(shape=(1,), name='Negative')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "        \n",
    "    hypo_embedding = word_embedding(hypo_input)   \n",
    "    neg_embedding = word_embedding(neg_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    # this one is custom and is based on the CRIM paper. \n",
    "    # we initialise on random normal noise applied to an identity matrix\n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "        return identity \n",
    "\n",
    "    def random_identity_2(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "        normal = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05) \n",
    "\n",
    "\n",
    "        return normal + identity\n",
    "            \n",
    "    phi = Dense(5, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity, name='Phi0')(hypo_embedding)\n",
    "\n",
    "    phi = Flatten(name='FlattenPhi')(phi)\n",
    "    neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)\n",
    "    hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)\n",
    "    \n",
    "    # calculate similarity of projection with negative\n",
    "    random_sim = Dot(axes=-1, normalize=False, name='DotProductRand')([phi, neg_embedding])\n",
    "    vector_diff = Subtract()([phi, hyper_embedding])\n",
    "    mse = Lambda(lambda x: K.sqrt(K.sum(K.square(x), axis=-1)))(vector_diff)\n",
    "    \n",
    "    diff = custom_loss2(random_sim)\n",
    "    \n",
    "    model_test = Model(inputs=[hypo_input, neg_input, hyper_input], outputs=mse)\n",
    "        \n",
    "    #regul_loss = custom_loss(hypo_embedding, phi)\n",
    "    \n",
    "    \n",
    "    model_test.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model_test.get_layer(name='WordEmbedding').trainable = False\n",
    "            \n",
    "    model_test.compile(optimizer='rmsprop', loss=diff, metrics=['accuracy'])\n",
    "    \n",
    "    return model_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hyponym (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "WordEmbedding (Embedding)    (None, 1, 5)              20        \n",
      "_________________________________________________________________\n",
      "Phi0 (Dense)                 (None, 1, 5)              25        \n",
      "_________________________________________________________________\n",
      "FlattenPhi (Flatten)         (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 45\n",
      "Trainable params: 25\n",
      "Non-trainable params: 20\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0397483, 0.0]\n",
      "[1.0397483, array([1., 1.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#model_test2 = build_square_error2()\n",
    "#model_test2.summary()\n",
    "\n",
    "#print model_test2.predict([[1], [3], [2]])\n",
    "print model_test.test_on_batch([[[1], [2]], [[3],[1]]], embedding_matrix[2:4].reshape(2,-1))\n",
    "\n",
    "print model_test2.test_on_batch([[[1], [2]], [[3], [1]], [[2], [3]]], [0., 0.])\n",
    "\n",
    "#a = np.sqrt(np.sum((embedding_matrix[1] - embedding_matrix[2] ) ** 2))\n",
    "#b = np.sqrt(np.sum((embedding_matrix[2] - embedding_matrix[3] ) ** 2))\n",
    "#print a + (np.dot(embedding_matrix[1], embedding_matrix[3]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.get_layer(name='Prediction').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "\n",
    "    word_pair = Input(shape=(2,), name='WordPair')\n",
    "    word_embedding = Embedding(4, 5, name='WordEmbedding')\n",
    "    \n",
    "    wp_embedding = word_embedding(word_pair)\n",
    "    #flat = Flatten()(wp_embedding)\n",
    "    conv1 = Conv1D(2, 1, activation='relu', name='Conv1_1')(wp_embedding)\n",
    "    conv1_2 = Conv1D(2, 2, activation='relu', name='Conv1_2')(conv1)\n",
    "    max_pool = GlobalMaxPooling1D()(conv1_2)\n",
    "    dense = Dense(32, activation='relu', name='FullyConnected')(max_pool)\n",
    "    \n",
    "    model = Model(inputs = word_pair, outputs = dense)\n",
    "    model.get_layer(name='WordEmbedding').set_weights([embedding_matrix])\n",
    "    model.get_layer(name='WordEmbedding').trainable = False\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "model_test = build_cnn()\n",
    "model_test.summary()\n",
    "\n",
    "word_input = np.array([1, 2])\n",
    "word_input = word_input.reshape(1,-1)\n",
    "\n",
    "print word_input\n",
    "model_test.predict(word_input)\n",
    "\n",
    "#print embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = build_variant()\n",
    "feature_extractor.summary()\n",
    "\n",
    "print embedding_matrix\n",
    "\n",
    "# print weights\n",
    "print feature_extractor.get_layer(name='Phi0').get_weights()[0]\n",
    "\n",
    "classifier1 = build_classifier(feature_extractor)\n",
    "classifier2 = build_classifier(feature_extractor)\n",
    "\n",
    "print \"Training first classifier\"\n",
    "metrics = classifier1.train_on_batch([[1], [2]], [1])\n",
    "print metrics\n",
    "print \"Print phi weights\"\n",
    "# print updated weights\n",
    "print feature_extractor.get_layer(name='Phi0').get_weights()[0]\n",
    "\n",
    "print \"Training second classifier\"\n",
    "metrics = classifier2.train_on_batch([[1], [3]], [0])\n",
    "print metrics\n",
    "print \"Print phi weights\"\n",
    "print feature_extractor.get_layer(name='Phi0').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "embedding_matrix =  normalize(embedding_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.linalg.norm(embedding_matrix, axis=1).reshape(-1, 1)\n",
    "#np.dot(embedding_matrix[1], embedding_matrix[2])\n",
    "\n",
    "#embedding_matrix[1:] /= np.linalg.norm(embedding_matrix, axis=1).reshape(-1, 1)[1:]\n",
    "\n",
    "#np.dot(embedding_matrix[1], embedding_matrix[2])\n",
    "print np.dot(embedding_matrix[1], embedding_matrix[1:].T)\n",
    "print np.dot(embedding_matrix[1:], embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = build_model()\n",
    "model_test.summary()\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model_test, to_file='model_test.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.get_layer(name='dropout_2').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_test = build_model()\n",
    "model_test.fit([input_hypo_seq, input_hyper_seq], [1], epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model_test.predict([input_hypo_seq, input_hyper_seq])\n",
    "\n",
    "model_test.get_layer(name='Prediction').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate forward pass when predicting\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "features = model_test.get_layer(name='Prediction').get_weights()[0]\n",
    "bias = model_test.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "print \"Weights:\"\n",
    "print features[0], features[1], bias\n",
    "print \"-\"*30\n",
    "proj1 = model_test.get_layer(name='Phi0').get_weights()[0]\n",
    "proj2 = model_test.get_layer(name='Phi1').get_weights()[0]\n",
    "print \"Learnt projections:\"\n",
    "print proj1\n",
    "print proj2\n",
    "print \"-\"*30\n",
    "P1 = np.dot(input_a, proj1)\n",
    "P2 = np.dot(input_a, proj2)\n",
    "\n",
    "P = np.concatenate((P1,P2), axis = 0)\n",
    "s = np.dot(P, input_b.T)\n",
    "print s \n",
    "pred = np.sum(s.flatten() * features.flatten()) + bias\n",
    "\n",
    "sigmoid(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Yamane POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_model(dim, embedding_matrix):\n",
    "    hypo_input = Input(shape=(1,))\n",
    "    hyper_input = Input(shape=(1,))\n",
    "\n",
    "    word_embedding = Embedding(embedding_matrix.shape[0], dim, name='WE')\n",
    "\n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    embedding_model = Model(inputs=[hypo_input, hyper_input], outputs=[hypo_embedding, hyper_embedding])\n",
    "\n",
    "    # inject pre-trained embeddings into this mini, resusable model/layer\n",
    "    embedding_model.get_layer(name='WE').set_weights([embedding_matrix])\n",
    "    embedding_model.get_layer(name='WE').trainable = False\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypo_input = Input(shape=(1,))\n",
    "hyper_input = Input(shape=(1,))\n",
    "\n",
    "# create model that reuse weights of another model\n",
    "# our model_test is not an actual model but a pre-trained\n",
    "# embeddings layer\n",
    "# this allows us to reuse the same embedding weights for every \n",
    "# cluster we need to create model.\n",
    "# less resources are therefore consumed to build and train the \n",
    "# Yamane model\n",
    "\n",
    "# employ model as a \"layer\"\n",
    "embedding_layer = get_embeddings_model(3, embedding_matrix)\n",
    "e1, e2 = embedding_layer([hypo_input, hyper_input])\n",
    "hypo_flat = Flatten()(e1)\n",
    "hyper_flat = Flatten()(e2)\n",
    "\n",
    "model2 = Model([hypo_input, hyper_input], outputs=[hypo_flat, hyper_flat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model2.predict([input_hypo_seq, input_hyper_seq])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(10)\n",
    "# hyponym cluster #1 simulation\n",
    "#X1 = np.random.normal(-0.005,0.001,size=(4,2)) \n",
    "X1 = np.random.normal(5,0.5,size=(40,2)) \n",
    "# hypernym for cluster #1\n",
    "Y1 = np.array([[5., 3.]])\n",
    "\n",
    "# hyponym cluster #2 simulation\n",
    "X2 = np.random.normal(0,0.5,size=(40,2)) \n",
    "#X2[:,1] +=  0.01\n",
    "Y2 = np.array([[2.5, 0.010]])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X1[:,0], X1[:,1],c='red')\n",
    "plt.scatter(Y1[:,0], Y1[:,1],c='red')\n",
    "plt.scatter(X2[:,0], X2[:,1],c='blue')\n",
    "plt.scatter(Y2[:,0], Y2[:,1],c='blue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create embedding matrix from synthesised samples\n",
    "X = np.vstack((X1, X2))\n",
    "Y = np.vstack((Y1, Y2))\n",
    "\n",
    "hyponym_size = X.shape[0]\n",
    "hypernym_size = Y.shape[0]\n",
    "dim = 2\n",
    "\n",
    "vocab_size = hyponym_size + hypernym_size\n",
    "embedding_matrix = np.zeros((vocab_size + 1, dim))\n",
    "\n",
    "\n",
    "embedding_matrix[1:len(X)+1,:] = X\n",
    "embedding_matrix[len(X)+1:,:] = Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for vector in embedding_matrix:\n",
    "    vector /= np.linalg.norm(vector)\n",
    "\n",
    "embedding_matrix[0] = [0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create sequence input data\n",
    "X_hyponym = X\n",
    "Y_hypernym = np.vstack((np.tile(Y[0,:], (40,1)), np.tile(Y[1,:], (40,1))))\n",
    "\n",
    "X_hyponym_seq = []\n",
    "Y_hypernym_seq = []\n",
    "for i in range(X_hyponym.shape[0]):\n",
    "    X_hyponym_seq.append([np.argwhere(embedding_matrix == X_hyponym[i])[0,0]])\n",
    "    Y_hypernym_seq.append([np.argwhere(embedding_matrix == Y_hypernym[i])[0,0]])\n",
    "\n",
    "# all samples are positive\n",
    "y_label = [1] * len(X_hyponym_seq) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that creates cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_new_cluster_model(embedding_layer, phi_dim):\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')    \n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    hypo_embedding, hyper_embedding = embedding_layer([hypo_input, hyper_input])\n",
    "    \n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "    \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=1.)\n",
    "\n",
    "        return identity * rnorm\n",
    "\n",
    "    phi = Dense(phi_dim, activation=None, use_bias=False, \n",
    "                kernel_initializer=\"random_normal\", name='Phi')(hypo_embedding)\n",
    "    \n",
    "    # flatten phi and hyper_embedding tensors\n",
    "    phi = Flatten()(phi)\n",
    "    hyper_embedding = Flatten()(hyper_embedding)\n",
    "    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", kernel_regularizer=l1(0.001), bias_regularizer=l1(0.001), name='Prediction')(phi_hyper)\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "    \n",
    "    # compile using binary_crossentropy loss\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get instance of embedding model which we will inject into every new \n",
    "# cluster model.  No sense into having individual embedding layers since\n",
    "# embedding layers are not trainable in this model\n",
    "embedding_layer = get_embeddings_model(dim=2, embedding_matrix=embedding_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yamane learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_hyponym_seq = np.array(X_hyponym_seq, dtype='int32')\n",
    "Y_hypernym_seq = np.array(Y_hypernym_seq, dtype='int32')\n",
    "y_label = np.array(y_label, dtype='int32')\n",
    "\n",
    "# all samples initialised to 1\n",
    "sample_clusters = np.ones(len(X_hyponym_seq), dtype='int32')\n",
    "\n",
    "# list of clusters\n",
    "clusters = []\n",
    "clusters.append(get_new_cluster_model(embedding_layer, phi_dim=2))\n",
    "# initialise all samples to cluster 1\n",
    "\n",
    "indices = np.arange(len(X_hyponym_seq))\n",
    "np.random.seed(42)\n",
    "# shuffle training set\n",
    "np.random.shuffle(indices)\n",
    "# simple structure that records cluster id for every sample\n",
    "sample_clusters[indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [0.]\n",
    "epochs = 50\n",
    "threshold = 0.4\n",
    "# indicator of \"current\" sample cluster index\n",
    "z_i = 0\n",
    "\n",
    "# iterate over samples\n",
    "for epoch in range(epochs):\n",
    "    print \"Epoch ======> %d\" % (epoch)\n",
    "    # reset losses to zero\n",
    "    losses = [0. for _ in losses]\n",
    "    # train algorithm by stochastic gradient descent, one sample at a time\n",
    "    for i in indices:\n",
    "        print \"Doing\", i, X_hyponym_seq[i], Y_hypernym_seq[i], sample_clusters[i]\n",
    "        # find number of clusters\n",
    "        sim = map(lambda x: x.predict([X_hyponym_seq[i], Y_hypernym_seq[i]]), clusters)\n",
    "        max_sim = np.argmax(sim)\n",
    "        print \"Max Similarity cluster:\", max_sim, \"(sim = %0.8f)\" % (sim[max_sim])\n",
    "        if sim[max_sim] < threshold:                        \n",
    "            # add new cluster to list of clusters\n",
    "            clusters.append(get_new_cluster_model(embedding_layer, phi_dim=2))\n",
    "            losses.append(0.)\n",
    "            # assign current cluster index to latest model\n",
    "            z_i = len(clusters) - 1\n",
    "            sample_clusters[i] = z_i\n",
    "        else:            \n",
    "            z_i = max_sim\n",
    "            sample_clusters[i] = z_i\n",
    "                \n",
    "        # get indices of elements belonging to cluster\n",
    "        z_i_indices = np.argwhere(sample_clusters == z_i).flatten()        \n",
    "        # shuffle batch indices\n",
    "        np.random.shuffle(z_i_indices)\n",
    "        # update parameters of cluster \n",
    "        losses[z_i] += clusters[z_i].train_on_batch([X_hyponym_seq[z_i_indices], Y_hypernym_seq[z_i_indices]], y_label[z_i_indices])[0]\n",
    "        print \"Loss on cluster\", z_i, losses[z_i]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print clusters[0].get_layer(name='Phi').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0].get_layer(name='Prediction').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_hypernym_seq\n",
    "#wrong_Y = np.zeros(80, dtype='int32').reshape(-1,1)\n",
    "\n",
    "#wrong_Y[:40] = Y_hypernym_seq[:40] -1\n",
    "#wrong_Y[40:] = Y_hypernym_seq[40:] +1\n",
    "\n",
    "np.round(clusters[0].predict([X_hyponym_seq, Y_hypernym_seq]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
