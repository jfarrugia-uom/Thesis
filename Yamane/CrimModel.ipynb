{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Yamane's Projection Learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part where we load the dataset; remove training pairs for which no embedding vector exists; tokenize separately and sequence to convert words into list of lists (latter list consisting of single word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_subsumptions(filename):\n",
    "    subsumptions = []\n",
    "\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            subsumptions.append((row[0], row[1]))\n",
    "\n",
    "    return subsumptions\n",
    "\n",
    "def read_synonyms(filename):\n",
    "    synonyms = defaultdict(lambda: list())\n",
    "\n",
    "    with codecs.open(filename,encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            for word in row[1].split(','):\n",
    "                synonyms[row[0]].append(word)\n",
    "    \n",
    "    synonyms.default_factory = None\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.path.exists('../Ustalov/subsumptions-train.txt.orig')\n",
    "\n",
    "train_subs = read_subsumptions('../Ustalov/subsumptions-train.txt.orig')\n",
    "test_subs = read_subsumptions('../Ustalov/subsumptions-test.txt.orig')\n",
    "valid_subs = read_subsumptions('../Ustalov/subsumptions-validation.txt.orig')\n",
    "\n",
    "synonyms = read_synonyms('../Ustalov/synonyms.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct pre-trained word embeddings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total number of word vectors is: ', 400000)\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print (\"Total number of word vectors is: \", len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate training tuples for which no embedding exists\n",
    "from collections import Counter\n",
    "\n",
    "def get_terms_having_vectors(dataset):    \n",
    "    query, hyper = \\\n",
    "    zip(*[(q,h) for q, h in dataset \n",
    "          if q in embeddings_index and h in embeddings_index])\n",
    "    \n",
    "    return list(query), list(hyper)\n",
    "    \n",
    "train_query, train_hyper = get_terms_having_vectors(train_subs)\n",
    "test_query, test_hyper = get_terms_having_vectors(test_subs)\n",
    "\n",
    "assert len(train_query) == len(train_hyper) == 4338\n",
    "assert len(test_query) == len(test_hyper) == 1533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove OOV words from synonym list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in list(synonyms.iteritems()):\n",
    "    if k not in embeddings_index:\n",
    "        synonyms.pop(k)\n",
    "    else:\n",
    "        for word in v:\n",
    "            if word not in embeddings_index:\n",
    "                v.remove(word)\n",
    "    \n",
    "# flatten list of synonyms    \n",
    "syns = [word for v in synonyms.values() for word in v]    \n",
    "\n",
    "# confirm that all words in synonym vocab have embeddings representation\n",
    "assert len(filter(lambda x: x in embeddings_index, syns))==len(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class Data which encapsulates all the bits and pieces we require for training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, train_query, train_hyper, test_query, test_hyper, synonyms, embeddings):\n",
    "        # construct vocab made up from term and hypernyms \n",
    "        # we will choose negative samples from this vocab after exhausting\n",
    "        # the synonyms\n",
    "        self.neg_vocab = set(train_hyper + test_hyper)\n",
    "        \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.synonyms = synonyms\n",
    "        \n",
    "        # calculate size of term and hypernym dataset (train + test)\n",
    "        n_hyponyms = len(set(train_query + test_query + syns))\n",
    "        # hypernyms will be introduced in the model as either training,\n",
    "        # gold positives, test gold positives (when evaluation) or\n",
    "        # negative synonyms.\n",
    "        n_hypernyms = len(set(train_hyper + test_hyper))\n",
    "\n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['a'].shape[0]\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words = n_hyponyms + n_hypernyms + 1)\n",
    "        self.tokenizer.fit_on_texts(train_query + train_hyper + test_query + test_hyper + syns)\n",
    "        \n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector \n",
    "                    embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(train_query, train_hyper, test_query, test_hyper, synonyms, embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first exhaust synonyms;\n",
    "# find the rest by drawing random terms from neg_vocab;\n",
    "# however, make sure that chosen words are not valid hypernyms;\n",
    "# finally, tokenise back to ids;    \n",
    "\n",
    "\n",
    "# positive_sample and terms both expect tuples where positive_sample = (query, hyper)\n",
    "# and terms = (all_query_terms, all_hyper_terms)\n",
    "def get_negative_words(positive_sample, word_hypernyms, data, sample_size=5):\n",
    "    neg_samples = []\n",
    "    # we need to make a copy of the synonym list\n",
    "    # synonmys will form part of out negative examples\n",
    "    if positive_sample[0] in data.synonyms:\n",
    "        neg_samples = list(synonyms[positive_sample[0]])        \n",
    "    \n",
    "    # there might not be enough; compound with random words\n",
    "    if len(neg_samples) >= sample_size:\n",
    "        # jumble negative sample indices        \n",
    "        neg_samples = np.random.choice(neg_samples, sample_size, replace=False)\n",
    "    else:\n",
    "        # get current sample's hypernyms\n",
    "        positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "        \n",
    "        # eliminate correct hypernyms from neg_vocab\n",
    "        word_choice = [nv for nv in data.neg_vocab if nv not in positive_hypernyms and nv not in neg_samples]        \n",
    "        # choose m - len(neg_samples)\n",
    "        neg_samples.extend(np.random.choice(word_choice, (sample_size-len(neg_samples))).tolist())\n",
    "            \n",
    "    return neg_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find most similar words to given hypernym which is not valid hypernym of word\n",
    "def get_similar_hypernyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    word = data.tokenizer.word_index[positive_sample[1]]\n",
    "    candidate_words = filter(lambda w: w != word, data.tokenizer.index_word.keys())\n",
    "    sims = map(lambda c: np.dot(data.embedding_matrix[c], data.embedding_matrix[word]), candidate_words)\n",
    "\n",
    "    # get 30 most similar words to hypernyms\n",
    "    most_sim_idx = np.argsort(sims)[::-1][:30]    \n",
    "    similar_hypernyms = [data.tokenizer.index_word[candidate_words[idx]]for idx in most_sim_idx]\n",
    "    \n",
    "    # make sure that similar words are not actual hypernyms    \n",
    "    positive_hypernym = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    return filter(lambda x: x not in positive_hypernym, similar_hypernyms)[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def get_similar_hyponyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])        \n",
    "\n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys())\n",
    "    \n",
    "    hypo_sims = map(lambda c: np.dot(        \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[0]]]), candidate_words)\n",
    "\n",
    "    most_sim_idx = np.argsort(hypo_sims)[::-1][:sample_size]\n",
    "    return map(lambda i: candidate_words[i], most_sim_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def mix_sim_hyper_random(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # init neg_samples\n",
    "    neg_samples = []\n",
    "    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])    \n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys())\n",
    "    \n",
    "    \n",
    "    # find similarity of all candidate words w.r.t. current hypernym\n",
    "    hyper_sims = map(lambda c: np.dot(\n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[1]]]), candidate_words)\n",
    "\n",
    "    # get most similar word to hypernym which is not hypernym\n",
    "    most_sim_idx = np.argsort(hyper_sims)[::-1][0]    \n",
    "    # append most similar hypernym to negative samples\n",
    "    neg_samples.append(candidate_words[most_sim_idx])\n",
    "        \n",
    "    if len(neg_samples) < sample_size:\n",
    "        neg_samples.extend(get_negative_words(positive_sample, word_hypernyms, data, sample_size=sample_size-1))\n",
    "    \n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of tuples where every element follows (word, negative_word)\n",
    "def get_negative_tuples(terms, data, negative_words_lambda, sample_size):\n",
    "    # convert terms to dictionary\n",
    "    input_query, input_hyper = terms\n",
    "    unq_input_query = sorted(list(set(input_query)))\n",
    "    \n",
    "    word_hypernyms = {}\n",
    "    for w in unq_input_query:        \n",
    "        word_hypernyms[w] = [h for q, h in zip(input_query, input_hyper) if q == w]\n",
    "        \n",
    "            \n",
    "    negative_tuples = []    \n",
    "    for words in zip(*terms):\n",
    "        negatives = negative_words_lambda(words, word_hypernyms, data, sample_size)\n",
    "        negative_tuples.extend(\n",
    "                [(words, n) for n in negatives]\n",
    "        )    \n",
    "    return negative_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('vision', 'experience'), 'knowledge'),\n",
       " (('vision', 'experience'), 'sight'),\n",
       " (('lime', 'citrus'), 'grapefruit'),\n",
       " (('lime', 'citrus'), 'coconut'),\n",
       " (('lime', 'plant'), 'factory'),\n",
       " (('lime', 'plant'), 'tangerine'),\n",
       " (('lime', 'tree'), 'pine'),\n",
       " (('lime', 'tree'), 'peach'),\n",
       " (('lime', 'food'), 'eat'),\n",
       " (('lime', 'food'), 'strawberry'),\n",
       " (('lime', 'produce'), 'develop'),\n",
       " (('lime', 'produce'), 'orange'),\n",
       " (('lime', 'fruit'), 'vegetable'),\n",
       " (('lime', 'fruit'), 'peach'),\n",
       " (('train', 'artefact'), 'arthropod'),\n",
       " (('train', 'artefact'), 'scooter'),\n",
       " (('train', 'conveyance'), 'unclean'),\n",
       " (('train', 'conveyance'), 'scooter'),\n",
       " (('train', 'transport'), 'traffic'),\n",
       " (('train', 'transport'), 'motorcycle'),\n",
       " (('train', 'vehicle'), 'car'),\n",
       " (('train', 'vehicle'), 'scooter'),\n",
       " (('train', 'artifact'), 'magical'),\n",
       " (('train', 'artifact'), 'ferry'),\n",
       " (('train', 'practice'), 'training'),\n",
       " (('train', 'practice'), 'helicopter'),\n",
       " (('train', 'transportation'), 'traffic'),\n",
       " (('train', 'transportation'), 'battleship'),\n",
       " (('train', 'learn'), 'understand'),\n",
       " (('train', 'learn'), 'car'),\n",
       " (('train', 'teach'), 'educate'),\n",
       " (('train', 'teach'), 'practice'),\n",
       " (('hawthorn', 'shrub'), 'evergreen'),\n",
       " (('hawthorn', 'shrub'), 'fun'),\n",
       " (('hawthorn', 'plant'), 'factory'),\n",
       " (('hawthorn', 'plant'), 'device'),\n",
       " (('rosemary', 'herb'), 'shrub'),\n",
       " (('rosemary', 'herb'), 'parsley'),\n",
       " (('rosemary', 'plant'), 'factory'),\n",
       " (('rosemary', 'plant'), 'parsley'),\n",
       " (('crab', 'arthropod'), 'vertebrate'),\n",
       " (('crab', 'arthropod'), 'trade'),\n",
       " (('crab', 'invertebrate'), 'vertebrate'),\n",
       " (('crab', 'invertebrate'), 'habitation'),\n",
       " (('crab', 'animal'), 'human'),\n",
       " (('crab', 'animal'), 'hibiscus'),\n",
       " (('crab', 'crustacean'), 'decapod'),\n",
       " (('crab', 'crustacean'), 'trace'),\n",
       " (('crab', 'shellfish'), 'seafood'),\n",
       " (('crab', 'shellfish'), 'boat'),\n",
       " (('funded', 'monetary'), 'finance'),\n",
       " (('funded', 'monetary'), 'map'),\n",
       " (('funded', 'paid'), 'pay'),\n",
       " (('funded', 'paid'), 'turtle'),\n",
       " (('funded', 'abundant'), 'mineral'),\n",
       " (('funded', 'abundant'), 'judge'),\n",
       " (('funded', 'supported'), 'support'),\n",
       " (('funded', 'supported'), 'grasshopper'),\n",
       " (('funded', 'resourced'), 'equip'),\n",
       " (('funded', 'resourced'), 'damage'),\n",
       " (('carabao', 'chordate'), 'archosaur'),\n",
       " (('carabao', 'chordate'), 'seabird'),\n",
       " (('carabao', 'vertebrate'), 'invertebrate'),\n",
       " (('carabao', 'vertebrate'), 'cook'),\n",
       " (('carabao', 'ungulate'), 'arthropod'),\n",
       " (('carabao', 'ungulate'), 'seabird'),\n",
       " (('carabao', 'ruminant'), 'bovine'),\n",
       " (('carabao', 'ruminant'), 'line'),\n",
       " (('carabao', 'placental'), 'uterine'),\n",
       " (('carabao', 'placental'), 'burger'),\n",
       " (('carabao', 'mammal'), 'reptile'),\n",
       " (('carabao', 'mammal'), 'visual'),\n",
       " (('carabao', 'animal'), 'human'),\n",
       " (('carabao', 'animal'), 'business'),\n",
       " (('skyline', 'horizon'), 'cloud'),\n",
       " (('skyline', 'horizon'), 'graphic'),\n",
       " (('skyline', 'outline'), 'detailed'),\n",
       " (('skyline', 'outline'), 'parrot'),\n",
       " (('soar', 'fly'), 'flight'),\n",
       " (('soar', 'fly'), 'hospital'),\n",
       " (('culture', 'growth'), 'increase'),\n",
       " (('culture', 'growth'), 'civilization'),\n",
       " (('monk', 'person'), 'man'),\n",
       " (('monk', 'person'), 'salmonid'),\n",
       " (('trial', 'experiment'), 'study'),\n",
       " (('trial', 'experiment'), 'try'),\n",
       " (('snowball', 'plaything'), 'wallflower'),\n",
       " (('snowball', 'plaything'), 'collect'),\n",
       " (('snowball', 'ball'), 'throw'),\n",
       " (('snowball', 'ball'), 'gourd'),\n",
       " (('snowball', 'condensation'), 'liquid'),\n",
       " (('snowball', 'condensation'), 'conflict'),\n",
       " (('snowball', 'weather'), 'rain'),\n",
       " (('snowball', 'weather'), 'unincorporated'),\n",
       " (('snowball', 'sphere'), 'circle'),\n",
       " (('snowball', 'sphere'), 'greyhound'),\n",
       " (('snowball', 'mass'), 'scale'),\n",
       " (('snowball', 'mass'), 'beast'),\n",
       " (('toilet', 'room'), 'floor'),\n",
       " (('toilet', 'room'), 'john'),\n",
       " (('problem', 'question'), 'answer'),\n",
       " (('problem', 'question'), 'catch'),\n",
       " (('direct', 'order'), 'help'),\n",
       " (('direct', 'order'), 'order'),\n",
       " (('direct', 'make'), 'take'),\n",
       " (('direct', 'make'), 'order'),\n",
       " (('pulmonary', 'respiratory'), 'gastrointestinal'),\n",
       " (('pulmonary', 'respiratory'), 'competent'),\n",
       " (('pulmonary', 'physical'), 'mental'),\n",
       " (('pulmonary', 'physical'), 'tighten'),\n",
       " (('pulmonary', 'medical'), 'hospital'),\n",
       " (('pulmonary', 'medical'), 'train'),\n",
       " (('chest', 'box'), 'inside'),\n",
       " (('chest', 'box'), 'breast'),\n",
       " (('jasmine', 'shrub'), 'evergreen'),\n",
       " (('jasmine', 'shrub'), 'normal'),\n",
       " (('jasmine', 'plant'), 'factory'),\n",
       " (('jasmine', 'plant'), 'code'),\n",
       " (('critter', 'animal'), 'human'),\n",
       " (('critter', 'animal'), 'employment'),\n",
       " (('half', 'part'), 'it'),\n",
       " (('half', 'part'), 'side'),\n",
       " (('key', 'metal'), 'iron'),\n",
       " (('key', 'metal'), 'emotion'),\n",
       " (('key', 'pitch'), 'ball'),\n",
       " (('key', 'pitch'), 'tree'),\n",
       " (('key', 'device'), 'bomb'),\n",
       " (('key', 'device'), 'emotion'),\n",
       " (('holly', 'tree'), 'pine'),\n",
       " (('holly', 'tree'), 'voluntary'),\n",
       " (('holly', 'plant'), 'factory'),\n",
       " (('holly', 'plant'), 'form'),\n",
       " (('modal', 'structural'), 'structure'),\n",
       " (('modal', 'structural'), 'apparel'),\n",
       " (('pig', 'ungulate'), 'arthropod'),\n",
       " (('pig', 'ungulate'), 'bear'),\n",
       " (('pig', 'creature'), 'mysterious'),\n",
       " (('pig', 'creature'), 'squirrel'),\n",
       " (('pig', 'chordate'), 'archosaur'),\n",
       " (('pig', 'chordate'), 'deer'),\n",
       " (('pig', 'beast'), 'ghost'),\n",
       " (('pig', 'beast'), 'donkey'),\n",
       " (('pig', 'swine'), 'virus'),\n",
       " (('pig', 'swine'), 'rabbit'),\n",
       " (('pig', 'animal'), 'human'),\n",
       " (('pig', 'animal'), 'rabbit'),\n",
       " (('pig', 'vertebrate'), 'invertebrate'),\n",
       " (('pig', 'vertebrate'), 'giraffe'),\n",
       " (('pig', 'mammal'), 'reptile'),\n",
       " (('pig', 'mammal'), 'fox'),\n",
       " (('potato', 'vine'), 'grape'),\n",
       " (('potato', 'vine'), 'lettuce'),\n",
       " (('potato', 'plant'), 'factory'),\n",
       " (('potato', 'plant'), 'cabbage'),\n",
       " (('potato', 'produce'), 'develop'),\n",
       " (('potato', 'produce'), 'garlic'),\n",
       " (('potato', 'vegetable'), 'fruit'),\n",
       " (('potato', 'vegetable'), 'broccoli'),\n",
       " (('potato', 'tuber'), 'fungus'),\n",
       " (('potato', 'tuber'), 'celery'),\n",
       " (('potato', 'food'), 'eat'),\n",
       " (('potato', 'food'), 'radish'),\n",
       " (('potato', 'veggie'), 'burger'),\n",
       " (('potato', 'veggie'), 'leek'),\n",
       " (('alleviation', 'injury'), 'injured'),\n",
       " (('alleviation', 'injury'), 'give'),\n",
       " (('alleviation', 'medication'), 'patient'),\n",
       " (('alleviation', 'medication'), 'caterpillar'),\n",
       " (('alleviation', 'pain'), 'feeling'),\n",
       " (('alleviation', 'pain'), 'story'),\n",
       " (('alleviation', 'comfort'), 'feel'),\n",
       " (('alleviation', 'comfort'), 'saurian'),\n",
       " (('alleviation', 'relief'), 'aid'),\n",
       " (('alleviation', 'relief'), 'day'),\n",
       " (('alleviation', 'eradication'), 'poppy'),\n",
       " (('alleviation', 'eradication'), 'picture'),\n",
       " (('alleviation', 'decrease'), 'increase'),\n",
       " (('alleviation', 'decrease'), 'regulate'),\n",
       " (('high', 'degree'), 'university'),\n",
       " (('high', 'degree'), 'tower'),\n",
       " (('divorce', 'separate'), 'multiple'),\n",
       " (('divorce', 'separate'), 'separation'),\n",
       " (('divorce', 'separation'), 'internal'),\n",
       " (('divorce', 'separation'), 'separate'),\n",
       " (('heat', 'change'), 'need'),\n",
       " (('heat', 'change'), 'warm'),\n",
       " (('heat', 'race'), 'finish'),\n",
       " (('heat', 'race'), 'hot'),\n",
       " (('heat', 'energy'), 'gas'),\n",
       " (('heat', 'energy'), 'hot'),\n",
       " (('cannon', 'implement'), 'plan'),\n",
       " (('cannon', 'implement'), 'pike'),\n",
       " (('cannon', 'device'), 'bomb'),\n",
       " (('cannon', 'device'), 'sword'),\n",
       " (('cannon', 'object'), 'element'),\n",
       " (('cannon', 'object'), 'rifle'),\n",
       " (('cannon', 'artifact'), 'magical'),\n",
       " (('cannon', 'artifact'), 'pistol'),\n",
       " (('cannon', 'artefact'), 'arthropod'),\n",
       " (('cannon', 'artefact'), 'musket'),\n",
       " (('cannon', 'weapon'), 'firearm'),\n",
       " (('cannon', 'weapon'), 'missile'),\n",
       " (('cannon', 'arm'), 'hand'),\n",
       " (('cannon', 'arm'), 'sword'),\n",
       " (('cannon', 'gun'), 'rifle'),\n",
       " (('cannon', 'gun'), 'revolver'),\n",
       " (('boy', 'man'), 'woman'),\n",
       " (('boy', 'man'), 'john'),\n",
       " (('boy', 'animal'), 'human'),\n",
       " (('boy', 'animal'), 'child'),\n",
       " (('boy', 'child'), 'girl'),\n",
       " (('boy', 'child'), 'child'),\n",
       " (('barberry', 'plant'), 'factory'),\n",
       " (('barberry', 'plant'), 'gentleman'),\n",
       " (('barberry', 'shrub'), 'evergreen'),\n",
       " (('barberry', 'shrub'), 'dishware'),\n",
       " (('creation', 'begin'), 'will'),\n",
       " (('creation', 'begin'), 'art'),\n",
       " (('musician', 'performer'), 'actor'),\n",
       " (('musician', 'performer'), 'pretend'),\n",
       " (('musician', 'vocation'), 'profession'),\n",
       " (('musician', 'vocation'), 'prosper'),\n",
       " (('musician', 'occupation'), 'war'),\n",
       " (('musician', 'occupation'), 'coin'),\n",
       " (('musician', 'career'), 'professional'),\n",
       " (('musician', 'career'), 'man'),\n",
       " (('scavenger', 'animal'), 'human'),\n",
       " (('scavenger', 'animal'), 'matter'),\n",
       " (('copepod', 'arthropod'), 'vertebrate'),\n",
       " (('copepod', 'arthropod'), 'condensation'),\n",
       " (('copepod', 'invertebrate'), 'vertebrate'),\n",
       " (('copepod', 'invertebrate'), 'narrow'),\n",
       " (('copepod', 'animal'), 'human'),\n",
       " (('copepod', 'animal'), 'narcissus'),\n",
       " (('copepod', 'crustacean'), 'decapod'),\n",
       " (('copepod', 'crustacean'), 'herbivore'),\n",
       " (('lobelia', 'plant'), 'factory'),\n",
       " (('lobelia', 'plant'), 'join'),\n",
       " (('lobelia', 'herb'), 'shrub'),\n",
       " (('lobelia', 'herb'), 'blanket'),\n",
       " (('aster', 'flower'), 'fruit'),\n",
       " (('aster', 'flower'), 'race'),\n",
       " (('aster', 'plant'), 'factory'),\n",
       " (('aster', 'plant'), 'affect'),\n",
       " (('aster', 'angiosperm'), 'archosaur'),\n",
       " (('aster', 'angiosperm'), 'release'),\n",
       " (('thistle', 'weed'), 'pest'),\n",
       " (('thistle', 'weed'), 'develop'),\n",
       " (('thistle', 'plant'), 'factory'),\n",
       " (('thistle', 'plant'), 'constrictor'),\n",
       " (('cat', 'chordate'), 'archosaur'),\n",
       " (('cat', 'chordate'), 'jaguar'),\n",
       " (('cat', 'carnivore'), 'herbivore'),\n",
       " (('cat', 'carnivore'), 'jaguar'),\n",
       " (('cat', 'plant'), 'factory'),\n",
       " (('cat', 'plant'), 'goat'),\n",
       " (('cat', 'placental'), 'uterine'),\n",
       " (('cat', 'placental'), 'bull'),\n",
       " (('cat', 'feline'), 'canine'),\n",
       " (('cat', 'feline'), 'coyote'),\n",
       " (('cat', 'cattail'), 'sedge'),\n",
       " (('cat', 'cattail'), 'giraffe'),\n",
       " (('cat', 'vertebrate'), 'invertebrate'),\n",
       " (('cat', 'vertebrate'), 'bull'),\n",
       " (('cat', 'mammal'), 'reptile'),\n",
       " (('cat', 'mammal'), 'rat'),\n",
       " (('cat', 'animal'), 'human'),\n",
       " (('cat', 'animal'), 'deer'),\n",
       " (('cat', 'beast'), 'ghost'),\n",
       " (('cat', 'beast'), 'rabbit'),\n",
       " (('cat', 'pet'), 'dog'),\n",
       " (('cat', 'pet'), 'donkey'),\n",
       " (('cat', 'creature'), 'mysterious'),\n",
       " (('cat', 'creature'), 'dog'),\n",
       " (('cat', 'friend'), 'boyfriend'),\n",
       " (('cat', 'friend'), 'elephant'),\n",
       " (('bath', 'place'), 'time'),\n",
       " (('bath', 'place'), 'bathtub'),\n",
       " (('price', 'value'), 'transaction'),\n",
       " (('price', 'value'), 'grasshopper'),\n",
       " (('watermelon', 'melon'), 'cantaloupe'),\n",
       " (('watermelon', 'melon'), 'cucumber'),\n",
       " (('watermelon', 'vine'), 'grape'),\n",
       " (('watermelon', 'vine'), 'cucumber'),\n",
       " (('watermelon', 'gourd'), 'flute'),\n",
       " (('watermelon', 'gourd'), 'cucumber'),\n",
       " (('watermelon', 'plant'), 'factory'),\n",
       " (('watermelon', 'plant'), 'cucumber'),\n",
       " (('laugh', 'fun'), 'stuff'),\n",
       " (('laugh', 'fun'), 'inflated'),\n",
       " (('worm', 'invertebrate'), 'vertebrate'),\n",
       " (('worm', 'invertebrate'), 'thoughtful'),\n",
       " (('worm', 'animal'), 'human'),\n",
       " (('worm', 'animal'), 'performer'),\n",
       " (('spade', 'object'), 'element'),\n",
       " (('spade', 'object'), 'tweezers'),\n",
       " (('spade', 'tool'), 'useful'),\n",
       " (('spade', 'tool'), 'fork'),\n",
       " (('spade', 'utensil'), 'cutlery'),\n",
       " (('spade', 'utensil'), 'chisel'),\n",
       " (('spade', 'artefact'), 'arthropod'),\n",
       " (('spade', 'artefact'), 'hammer'),\n",
       " (('spade', 'implement'), 'plan'),\n",
       " (('spade', 'implement'), 'shovel'),\n",
       " (('spade', 'artifact'), 'magical'),\n",
       " (('spade', 'artifact'), 'shovel'),\n",
       " (('fitness', 'magazine'), 'newspaper'),\n",
       " (('fitness', 'magazine'), 'remove'),\n",
       " (('jar', 'utensil'), 'cutlery'),\n",
       " (('jar', 'utensil'), 'plate'),\n",
       " (('jar', 'kitchenware'), 'tableware'),\n",
       " (('jar', 'kitchenware'), 'pot'),\n",
       " (('jar', 'container'), 'ship'),\n",
       " (('jar', 'container'), 'mug'),\n",
       " (('jar', 'object'), 'element'),\n",
       " (('jar', 'object'), 'pot'),\n",
       " (('jar', 'vessel'), 'ship'),\n",
       " (('jar', 'vessel'), 'box'),\n",
       " (('jar', 'artefact'), 'arthropod'),\n",
       " (('jar', 'artefact'), 'glass'),\n",
       " (('jar', 'artifact'), 'magical'),\n",
       " (('jar', 'artifact'), 'box'),\n",
       " (('kid', 'mammal'), 'reptile'),\n",
       " (('kid', 'mammal'), 'child'),\n",
       " (('kid', 'animal'), 'human'),\n",
       " (('kid', 'animal'), 'child'),\n",
       " (('kid', 'chordate'), 'archosaur'),\n",
       " (('kid', 'chordate'), 'child'),\n",
       " (('kid', 'goat'), 'sheep'),\n",
       " (('kid', 'goat'), 'child'),\n",
       " (('kid', 'placental'), 'uterine'),\n",
       " (('kid', 'placental'), 'child'),\n",
       " (('kid', 'ruminant'), 'bovine'),\n",
       " (('kid', 'ruminant'), 'child'),\n",
       " (('kid', 'vertebrate'), 'invertebrate'),\n",
       " (('kid', 'vertebrate'), 'child'),\n",
       " (('kid', 'ungulate'), 'arthropod'),\n",
       " (('kid', 'ungulate'), 'child'),\n",
       " (('kid', 'person'), 'man'),\n",
       " (('kid', 'person'), 'child'),\n",
       " (('kid', 'offspring'), 'birth'),\n",
       " (('kid', 'offspring'), 'child'),\n",
       " (('steam', 'clean'), 'dirty'),\n",
       " (('steam', 'clean'), 'fog'),\n",
       " (('steam', 'cook'), 'boil'),\n",
       " (('steam', 'cook'), 'fog'),\n",
       " (('steam', 'anger'), 'disgust'),\n",
       " (('steam', 'anger'), 'fog'),\n",
       " (('steam', 'water'), 'supply'),\n",
       " (('steam', 'water'), 'fog'),\n",
       " (('lizard', 'chordate'), 'archosaur'),\n",
       " (('lizard', 'chordate'), 'gecko'),\n",
       " (('lizard', 'vertebrate'), 'invertebrate'),\n",
       " (('lizard', 'vertebrate'), 'frog'),\n",
       " (('lizard', 'saurian'), 'bryozoan'),\n",
       " (('lizard', 'saurian'), 'gecko'),\n",
       " (('lizard', 'animal'), 'human'),\n",
       " (('lizard', 'animal'), 'salamander'),\n",
       " (('lizard', 'reptile'), 'mammal'),\n",
       " (('lizard', 'reptile'), 'gecko'),\n",
       " (('lizard', 'beast'), 'ghost'),\n",
       " (('lizard', 'beast'), 'frog'),\n",
       " (('lizard', 'carnivore'), 'herbivore'),\n",
       " (('lizard', 'carnivore'), 'turtle'),\n",
       " (('lizard', 'creature'), 'mysterious'),\n",
       " (('lizard', 'creature'), 'chameleon'),\n",
       " (('lace', 'cloth'), 'wool'),\n",
       " (('lace', 'cloth'), 'string'),\n",
       " (('lace', 'fabric'), 'wool'),\n",
       " (('lace', 'fabric'), 'fasten'),\n",
       " (('lace', 'tie'), 'match'),\n",
       " (('lace', 'tie'), 'fasten'),\n",
       " (('kangaroo', 'animal'), 'human'),\n",
       " (('kangaroo', 'animal'), 'helpful'),\n",
       " (('kangaroo', 'marsupial'), 'carnivore'),\n",
       " (('kangaroo', 'marsupial'), 'dirty'),\n",
       " (('kangaroo', 'mammal'), 'reptile'),\n",
       " (('kangaroo', 'mammal'), 'spontaneous'),\n",
       " (('kangaroo', 'vertebrate'), 'invertebrate'),\n",
       " (('kangaroo', 'vertebrate'), 'mercurial'),\n",
       " (('kangaroo', 'chordate'), 'archosaur'),\n",
       " (('kangaroo', 'chordate'), 'parrot'),\n",
       " (('confidence', 'emotion'), 'emotional'),\n",
       " (('confidence', 'emotion'), 'faith'),\n",
       " (('despotic', 'undemocratic'), 'corrupt'),\n",
       " (('despotic', 'undemocratic'), 'systematic'),\n",
       " (('despotic', 'commanding'), 'rank'),\n",
       " (('despotic', 'commanding'), 'clothes'),\n",
       " (('despotic', 'mean'), 'think'),\n",
       " (('despotic', 'mean'), 'scene'),\n",
       " (('despotic', 'cruel'), 'punishment'),\n",
       " (('despotic', 'cruel'), 'reproduce'),\n",
       " (('despotic', 'absolute'), 'respect'),\n",
       " (('despotic', 'absolute'), 'liquid'),\n",
       " (('lady', 'woman'), 'girl'),\n",
       " (('lady', 'woman'), 'woman'),\n",
       " (('coconut', 'plant'), 'factory'),\n",
       " (('coconut', 'plant'), 'lime'),\n",
       " (('coconut', 'palm'), 'beach'),\n",
       " (('coconut', 'palm'), 'pineapple'),\n",
       " (('coconut', 'tree'), 'pine'),\n",
       " (('coconut', 'tree'), 'apple'),\n",
       " (('coconut', 'fruit'), 'citrus'),\n",
       " (('coconut', 'fruit'), 'banana'),\n",
       " (('coconut', 'food'), 'eat'),\n",
       " (('coconut', 'food'), 'grapefruit'),\n",
       " (('coconut', 'produce'), 'develop'),\n",
       " (('coconut', 'produce'), 'plum'),\n",
       " (('china', 'state'), 'government'),\n",
       " (('china', 'state'), 'piece'),\n",
       " (('china', 'country'), 'nation'),\n",
       " (('china', 'country'), 'sadness'),\n",
       " (('ice', 'crystal'), 'glass'),\n",
       " (('ice', 'crystal'), 'wake'),\n",
       " (('ice', 'water'), 'supply'),\n",
       " (('ice', 'water'), 'dress'),\n",
       " (('ice', 'solid'), 'good'),\n",
       " (('ice', 'solid'), 'hollow'),\n",
       " (('ice', 'cool'), 'warm'),\n",
       " (('ice', 'cool'), 'biological'),\n",
       " (('pinon', 'plant'), 'factory'),\n",
       " (('pinon', 'plant'), 'continent'),\n",
       " (('pinon', 'tree'), 'oak'),\n",
       " (('pinon', 'tree'), 'fall'),\n",
       " (('pinon', 'pine'), 'oak'),\n",
       " (('pinon', 'pine'), 'soda'),\n",
       " (('pinon', 'conifer'), 'fir'),\n",
       " (('pinon', 'conifer'), 'success'),\n",
       " (('priest', 'clergyman'), 'preacher'),\n",
       " (('priest', 'clergyman'), 'father'),\n",
       " (('priest', 'person'), 'man'),\n",
       " (('priest', 'person'), 'father'),\n",
       " (('knife', 'implement'), 'plan'),\n",
       " (('knife', 'implement'), 'pincer'),\n",
       " (('knife', 'tableware'), 'dishware'),\n",
       " (('knife', 'tableware'), 'gun'),\n",
       " (('knife', 'utensil'), 'fridge'),\n",
       " (('knife', 'utensil'), 'revolver'),\n",
       " (('knife', 'kitchenware'), 'knitwear'),\n",
       " (('knife', 'kitchenware'), 'fork'),\n",
       " (('knife', 'artifact'), 'magical'),\n",
       " (('knife', 'artifact'), 'rifle'),\n",
       " (('knife', 'tool'), 'useful'),\n",
       " (('knife', 'tool'), 'saw'),\n",
       " (('knife', 'cutlery'), 'silverware'),\n",
       " (('knife', 'cutlery'), 'musket'),\n",
       " (('knife', 'artefact'), 'arthropod'),\n",
       " (('knife', 'artefact'), 'pliers'),\n",
       " (('knife', 'weapon'), 'gun'),\n",
       " (('knife', 'weapon'), 'grenade'),\n",
       " (('arrowroot', 'plant'), 'factory'),\n",
       " (('arrowroot', 'plant'), 'protection'),\n",
       " (('arrowroot', 'herb'), 'shrub'),\n",
       " (('arrowroot', 'herb'), 'ion'),\n",
       " (('joy', 'emotion'), 'emotional'),\n",
       " (('joy', 'emotion'), 'pleasure'),\n",
       " (('magpie', 'bird'), 'virus'),\n",
       " (('magpie', 'bird'), 'ideology'),\n",
       " (('magpie', 'chordate'), 'archosaur'),\n",
       " (('magpie', 'chordate'), 'buy'),\n",
       " (('magpie', 'passerine'), 'wildflower'),\n",
       " (('magpie', 'passerine'), 'pot'),\n",
       " (('magpie', 'vertebrate'), 'invertebrate'),\n",
       " (('magpie', 'vertebrate'), 'plover'),\n",
       " (('magpie', 'animal'), 'human'),\n",
       " (('magpie', 'animal'), 'representational'),\n",
       " (('phlox', 'herb'), 'shrub'),\n",
       " (('phlox', 'herb'), 'question'),\n",
       " (('phlox', 'plant'), 'factory'),\n",
       " (('phlox', 'plant'), 'pigeon'),\n",
       " (('fireweed', 'plant'), 'factory'),\n",
       " (('fireweed', 'plant'), 'galaxy'),\n",
       " (('fireweed', 'herb'), 'shrub'),\n",
       " (('fireweed', 'herb'), 'long'),\n",
       " (('truck', 'vehicle'), 'car'),\n",
       " (('truck', 'vehicle'), 'frigate'),\n",
       " (('truck', 'conveyance'), 'unclean'),\n",
       " (('truck', 'conveyance'), 'tanker'),\n",
       " (('truck', 'artifact'), 'magical'),\n",
       " (('truck', 'artifact'), 'scooter'),\n",
       " (('truck', 'transport'), 'transportation'),\n",
       " (('truck', 'transport'), 'bomber'),\n",
       " (('truck', 'artefact'), 'arthropod'),\n",
       " (('truck', 'artefact'), 'van'),\n",
       " (('truck', 'automobile'), 'car'),\n",
       " (('truck', 'automobile'), 'moped'),\n",
       " (('foyer', 'place'), 'time'),\n",
       " (('foyer', 'place'), 'hall'),\n",
       " (('foyer', 'room'), 'floor'),\n",
       " (('foyer', 'room'), 'lobby'),\n",
       " (('tooth', 'bone'), 'tissue'),\n",
       " (('tooth', 'bone'), 'qualify'),\n",
       " (('surprise', 'change'), 'need'),\n",
       " (('surprise', 'change'), 'process'),\n",
       " (('haddock', 'vertebrate'), 'invertebrate'),\n",
       " (('haddock', 'vertebrate'), 'plane'),\n",
       " (('haddock', 'fish'), 'salmon'),\n",
       " (('haddock', 'fish'), 'determined'),\n",
       " (('haddock', 'animal'), 'human'),\n",
       " (('haddock', 'animal'), 'bramble'),\n",
       " (('haddock', 'chordate'), 'archosaur'),\n",
       " (('haddock', 'chordate'), 'ape'),\n",
       " (('news', 'information'), 'data'),\n",
       " (('news', 'information'), 'physical'),\n",
       " (('heron', 'animal'), 'human'),\n",
       " (('heron', 'animal'), 'wood'),\n",
       " (('heron', 'vertebrate'), 'invertebrate'),\n",
       " (('heron', 'vertebrate'), 'nervous'),\n",
       " (('heron', 'chordate'), 'archosaur'),\n",
       " (('heron', 'chordate'), 'travel'),\n",
       " (('heron', 'bird'), 'virus'),\n",
       " (('heron', 'bird'), 'disease'),\n",
       " (('falcon', 'bird'), 'virus'),\n",
       " (('falcon', 'bird'), 'woodpecker'),\n",
       " (('falcon', 'hawk'), 'helicopter'),\n",
       " (('falcon', 'hawk'), 'pheasant'),\n",
       " (('falcon', 'chordate'), 'archosaur'),\n",
       " (('falcon', 'chordate'), 'dove'),\n",
       " (('falcon', 'animal'), 'human'),\n",
       " (('falcon', 'animal'), 'sparrow'),\n",
       " (('falcon', 'vertebrate'), 'invertebrate'),\n",
       " (('falcon', 'vertebrate'), 'crow'),\n",
       " (('falcon', 'raptor'), 'hornet'),\n",
       " (('falcon', 'raptor'), 'crow'),\n",
       " (('falcon', 'predator'), 'insect'),\n",
       " (('falcon', 'predator'), 'crow'),\n",
       " (('falcon', 'creature'), 'beast'),\n",
       " (('falcon', 'creature'), 'woodpecker'),\n",
       " (('actor', 'person'), 'man'),\n",
       " (('actor', 'person'), 'actress'),\n",
       " (('bend', 'turn'), 'take'),\n",
       " (('bend', 'turn'), 'curve'),\n",
       " (('raise', 'construct'), 'build'),\n",
       " (('raise', 'construct'), 'rise'),\n",
       " (('raise', 'move'), 'step'),\n",
       " (('raise', 'move'), 'increase'),\n",
       " (('she', 'band'), 'album'),\n",
       " (('she', 'band'), 'undemocratic'),\n",
       " (('sport', 'fun'), 'stuff'),\n",
       " (('sport', 'fun'), 'game'),\n",
       " (('sport', 'athlete'), 'player'),\n",
       " (('sport', 'athlete'), 'game'),\n",
       " (('sport', 'club'), 'football'),\n",
       " (('sport', 'club'), 'game'),\n",
       " (('sport', 'exercise'), 'training'),\n",
       " (('sport', 'exercise'), 'game'),\n",
       " (('sport', 'game'), 'play'),\n",
       " (('sport', 'game'), 'game'),\n",
       " (('sport', 'entertainment'), 'tv'),\n",
       " (('sport', 'entertainment'), 'game'),\n",
       " (('street', 'road'), 'highway'),\n",
       " (('street', 'road'), 'prejudice'),\n",
       " (('cock', 'chordate'), 'archosaur'),\n",
       " (('cock', 'chordate'), 'arrange'),\n",
       " (('cock', 'animal'), 'human'),\n",
       " (('cock', 'animal'), 'wood'),\n",
       " (('cock', 'vertebrate'), 'invertebrate'),\n",
       " (('cock', 'vertebrate'), 'individual'),\n",
       " (('cock', 'bird'), 'virus'),\n",
       " (('cock', 'bird'), 'clinical'),\n",
       " (('cock', 'chicken'), 'meat'),\n",
       " (('cock', 'chicken'), 'injure'),\n",
       " (('france', 'place'), 'time'),\n",
       " (('france', 'place'), 'ethnicity'),\n",
       " (('nyala', 'mammal'), 'reptile'),\n",
       " (('nyala', 'mammal'), 'brahman'),\n",
       " (('nyala', 'placental'), 'uterine'),\n",
       " (('nyala', 'placental'), 'trade'),\n",
       " (('nyala', 'vertebrate'), 'invertebrate'),\n",
       " (('nyala', 'vertebrate'), 'equip'),\n",
       " (('nyala', 'chordate'), 'archosaur'),\n",
       " (('nyala', 'chordate'), 'view'),\n",
       " (('nyala', 'ungulate'), 'arthropod'),\n",
       " (('nyala', 'ungulate'), 'medical'),\n",
       " (('nyala', 'ruminant'), 'bovine'),\n",
       " (('nyala', 'ruminant'), 'starling'),\n",
       " (('nyala', 'antelope'), 'elk'),\n",
       " (('nyala', 'antelope'), 'biological'),\n",
       " (('nyala', 'animal'), 'human'),\n",
       " (('nyala', 'animal'), 'container'),\n",
       " (('rocket', 'plant'), 'factory'),\n",
       " (('rocket', 'plant'), 'missile'),\n",
       " (('rocket', 'herb'), 'shrub'),\n",
       " (('rocket', 'herb'), 'helicopter'),\n",
       " (('rocket', 'vehicle'), 'car'),\n",
       " (('rocket', 'vehicle'), 'missile'),\n",
       " (('nationality', 'race'), 'finish'),\n",
       " (('nationality', 'race'), 'mood'),\n",
       " (('nationality', 'classification'), 'taxonomic'),\n",
       " (('nationality', 'classification'), 'simplify'),\n",
       " (('nationality', 'status'), 'permanent'),\n",
       " (('nationality', 'status'), 'modify'),\n",
       " (('nationality', 'people'), 'all'),\n",
       " (('nationality', 'people'), 'refuse'),\n",
       " (('nationality', 'country'), 'nation'),\n",
       " (('nationality', 'country'), 'ethnic'),\n",
       " (('nationality', 'ethnicity'), 'religion'),\n",
       " (('nationality', 'ethnicity'), 'artifact'),\n",
       " (('hearse', 'vehicle'), 'car'),\n",
       " (('hearse', 'vehicle'), 'pictorial'),\n",
       " (('mosquito', 'insect'), 'pest'),\n",
       " (('mosquito', 'insect'), 'grasshopper'),\n",
       " (('mosquito', 'arthropod'), 'vertebrate'),\n",
       " (('mosquito', 'arthropod'), 'hornet'),\n",
       " (('mosquito', 'invertebrate'), 'vertebrate'),\n",
       " (('mosquito', 'invertebrate'), 'grasshopper'),\n",
       " (('mosquito', 'animal'), 'human'),\n",
       " (('mosquito', 'animal'), 'ant'),\n",
       " (('philodendron', 'vine'), 'grape'),\n",
       " (('philodendron', 'vine'), 'amphibian'),\n",
       " (('philodendron', 'plant'), 'factory'),\n",
       " (('philodendron', 'plant'), 'government'),\n",
       " (('philodendron', 'liana'), 'phlox'),\n",
       " (('philodendron', 'liana'), 'muscle'),\n",
       " (('songbird', 'passerine'), 'wildflower'),\n",
       " (('songbird', 'passerine'), 'medication'),\n",
       " (('songbird', 'bird'), 'virus'),\n",
       " (('songbird', 'bird'), 'homo'),\n",
       " (('songbird', 'vertebrate'), 'invertebrate'),\n",
       " (('songbird', 'vertebrate'), 'damage'),\n",
       " (('songbird', 'chordate'), 'archosaur'),\n",
       " (('songbird', 'chordate'), 'all'),\n",
       " (('songbird', 'animal'), 'human'),\n",
       " (('songbird', 'animal'), 'poplar'),\n",
       " (('frog', 'animal'), 'human'),\n",
       " (('frog', 'animal'), 'alligator'),\n",
       " (('frog', 'amphibian'), 'reptile'),\n",
       " (('frog', 'amphibian'), 'turtle'),\n",
       " (('frog', 'chordate'), 'archosaur'),\n",
       " (('frog', 'chordate'), 'snake'),\n",
       " (('frog', 'vertebrate'), 'invertebrate'),\n",
       " (('frog', 'vertebrate'), 'toad'),\n",
       " (('frog', 'beast'), 'ghost'),\n",
       " (('frog', 'beast'), 'bullfrog'),\n",
       " (('frog', 'creature'), 'mysterious'),\n",
       " (('frog', 'creature'), 'snake'),\n",
       " (('pterosaur', 'archosaur'), 'brachiopod'),\n",
       " (('pterosaur', 'archosaur'), 'set'),\n",
       " (('pterosaur', 'reptile'), 'mammal'),\n",
       " (('pterosaur', 'reptile'), 'family'),\n",
       " (('pterosaur', 'vertebrate'), 'invertebrate'),\n",
       " (('pterosaur', 'vertebrate'), 'leadership'),\n",
       " (('pterosaur', 'chordate'), 'angiosperm'),\n",
       " (('pterosaur', 'chordate'), 'wildcat'),\n",
       " (('pterosaur', 'animal'), 'human'),\n",
       " (('pterosaur', 'animal'), 'uncontrollable'),\n",
       " (('pepper', 'plant'), 'factory'),\n",
       " (('pepper', 'plant'), 'story'),\n",
       " (('pepper', 'vine'), 'grape'),\n",
       " (('pepper', 'vine'), 'smart'),\n",
       " (('saloon', 'establishment'), 'creation'),\n",
       " (('saloon', 'establishment'), 'species'),\n",
       " (('saloon', 'room'), 'floor'),\n",
       " (('saloon', 'room'), 'stir'),\n",
       " (('saloon', 'bar'), 'restaurant'),\n",
       " (('saloon', 'bar'), 'wolf'),\n",
       " (('saloon', 'building'), 'construction'),\n",
       " (('saloon', 'building'), 'pouch'),\n",
       " (('saloon', 'business'), 'industry'),\n",
       " (('saloon', 'business'), 'ungulate'),\n",
       " (('saloon', 'tavern'), 'pub'),\n",
       " (('saloon', 'tavern'), 'possible'),\n",
       " (('contact', 'lens'), 'optical'),\n",
       " (('contact', 'lens'), 'touch'),\n",
       " (('contact', 'touch'), 'hand'),\n",
       " (('contact', 'touch'), 'touch'),\n",
       " (('snowberry', 'plant'), 'factory'),\n",
       " (('snowberry', 'plant'), 'hear'),\n",
       " (('spark', 'trace'), 'detect'),\n",
       " (('spark', 'trace'), 'begin'),\n",
       " (('screwdriver', 'implement'), 'plan'),\n",
       " (('screwdriver', 'implement'), 'shovel'),\n",
       " (('screwdriver', 'object'), 'element'),\n",
       " (('screwdriver', 'object'), 'spade'),\n",
       " (('screwdriver', 'artifact'), 'magical'),\n",
       " (('screwdriver', 'artifact'), 'chisel'),\n",
       " (('screwdriver', 'utensil'), 'cutlery'),\n",
       " (('screwdriver', 'utensil'), 'mallet'),\n",
       " (('screwdriver', 'artefact'), 'arthropod'),\n",
       " (('screwdriver', 'artefact'), 'pincers'),\n",
       " (('screwdriver', 'tool'), 'useful'),\n",
       " (('screwdriver', 'tool'), 'fork'),\n",
       " (('rock', 'music'), 'musical'),\n",
       " (('rock', 'music'), 'stone'),\n",
       " (('rock', 'hard'), 'get'),\n",
       " (('rock', 'hard'), 'stone'),\n",
       " (('guppy', 'chordate'), 'archosaur'),\n",
       " (('guppy', 'chordate'), 'goldfish'),\n",
       " (('guppy', 'vertebrate'), 'invertebrate'),\n",
       " (('guppy', 'vertebrate'), 'goldfish'),\n",
       " (('guppy', 'animal'), 'human'),\n",
       " (('guppy', 'animal'), 'goldfish'),\n",
       " (('guppy', 'fish'), 'salmon'),\n",
       " (('guppy', 'fish'), 'goldfish'),\n",
       " (('statue', 'art'), 'artist'),\n",
       " (('statue', 'art'), 'prove'),\n",
       " (('statue', 'image'), 'picture'),\n",
       " (('statue', 'image'), 'melon'),\n",
       " (('nuthatch', 'animal'), 'human'),\n",
       " (('nuthatch', 'animal'), 'decapod'),\n",
       " (('nuthatch', 'chordate'), 'archosaur'),\n",
       " (('nuthatch', 'chordate'), 'transfer'),\n",
       " (('nuthatch', 'vertebrate'), 'invertebrate'),\n",
       " (('nuthatch', 'vertebrate'), 'kill'),\n",
       " (('nuthatch', 'bird'), 'virus'),\n",
       " (('nuthatch', 'bird'), 'see'),\n",
       " (('nuthatch', 'passerine'), 'wildflower'),\n",
       " (('nuthatch', 'passerine'), 'pool'),\n",
       " (('field', 'mouse'), 'rat'),\n",
       " (('field', 'mouse'), 'pitch'),\n",
       " (('straw', 'cover'), 'covering'),\n",
       " (('straw', 'cover'), 'church'),\n",
       " (('stretcher', 'carrier'), 'aircraft'),\n",
       " (('stretcher', 'carrier'), 'unprepared'),\n",
       " (('stretcher', 'equipment'), 'gear'),\n",
       " (('stretcher', 'equipment'), 'medicine'),\n",
       " (('stretcher', 'bed'), 'bedroom'),\n",
       " (('stretcher', 'bed'), 'herbivore'),\n",
       " (('stretcher', 'litter'), 'trash'),\n",
       " (('stretcher', 'litter'), 'people'),\n",
       " (('stretcher', 'hospital'), 'medical'),\n",
       " (('stretcher', 'hospital'), 'overwhelm'),\n",
       " (('shark', 'chordate'), 'archosaur'),\n",
       " (('shark', 'chordate'), 'lodging'),\n",
       " (('shark', 'vertebrate'), 'invertebrate'),\n",
       " (('shark', 'vertebrate'), 'paranormal'),\n",
       " (('shark', 'fish'), 'salmon'),\n",
       " (('shark', 'fish'), 'choke'),\n",
       " (('shark', 'animal'), 'human'),\n",
       " (('shark', 'animal'), 'dishware'),\n",
       " (('construe', 'define'), 'function'),\n",
       " (('construe', 'define'), 'fall'),\n",
       " (('construe', 'judge'), 'court'),\n",
       " (('construe', 'judge'), 'cruel'),\n",
       " (('construe', 'understand'), 'know'),\n",
       " (('construe', 'understand'), 'puzzle'),\n",
       " (('construe', 'build'), 'construct'),\n",
       " (('construe', 'build'), 'starling'),\n",
       " (('construe', 'express'), 'train'),\n",
       " (('construe', 'express'), 'multiple'),\n",
       " (('wheel', 'machine'), 'gun'),\n",
       " (('wheel', 'machine'), 'change'),\n",
       " (('wheel', 'part'), 'it'),\n",
       " (('wheel', 'part'), 'clinical'),\n",
       " (('wheel', 'object'), 'element'),\n",
       " (('wheel', 'object'), 'hobby'),\n",
       " (('cholla', 'cactus'), 'saguaro'),\n",
       " (('cholla', 'cactus'), 'company'),\n",
       " (('cholla', 'plant'), 'factory'),\n",
       " (('cholla', 'plant'), 'bodily'),\n",
       " (('cholla', 'succulent'), 'shrub'),\n",
       " (('cholla', 'succulent'), 'escape'),\n",
       " (('feeder', 'animal'), 'human'),\n",
       " (('feeder', 'animal'), 'reduce'),\n",
       " (('barley', 'plant'), 'factory'),\n",
       " (('barley', 'plant'), 'gray'),\n",
       " (('barley', 'cereal'), 'wheat'),\n",
       " (('barley', 'cereal'), 'pain'),\n",
       " (('barley', 'grass'), 'lawn'),\n",
       " (('barley', 'grass'), 'satisfy'),\n",
       " (('barley', 'herb'), 'shrub'),\n",
       " (('barley', 'herb'), 'carriage'),\n",
       " (('barley', 'food'), 'eat'),\n",
       " (('barley', 'food'), 'crazy'),\n",
       " (('barley', 'grain'), 'wheat'),\n",
       " (('barley', 'grain'), 'terrestrial'),\n",
       " (('echinoderm', 'animal'), 'human'),\n",
       " (('echinoderm', 'animal'), 'organism'),\n",
       " (('echinoderm', 'invertebrate'), 'vertebrate'),\n",
       " (('echinoderm', 'invertebrate'), 'conveyance'),\n",
       " (('harvest', 'remove'), 'add'),\n",
       " (('harvest', 'remove'), 'produce'),\n",
       " (('credit', 'payment'), 'pay'),\n",
       " (('credit', 'payment'), 'faith'),\n",
       " (('rhea', 'bird'), 'virus'),\n",
       " (('rhea', 'bird'), 'diseased'),\n",
       " (('rhea', 'animal'), 'human'),\n",
       " (('rhea', 'animal'), 'performer'),\n",
       " (('rhea', 'ratite'), 'bryozoan'),\n",
       " (('rhea', 'ratite'), 'candy'),\n",
       " (('rhea', 'vertebrate'), 'invertebrate'),\n",
       " (('rhea', 'vertebrate'), 'run'),\n",
       " (('rhea', 'chordate'), 'archosaur'),\n",
       " (('rhea', 'chordate'), 'electric'),\n",
       " (('louse', 'animal'), 'human'),\n",
       " (('louse', 'animal'), 'attitude'),\n",
       " (('louse', 'invertebrate'), 'vertebrate'),\n",
       " (('louse', 'invertebrate'), 'content'),\n",
       " (('louse', 'insect'), 'pest'),\n",
       " (('louse', 'insect'), 'liana'),\n",
       " (('louse', 'arthropod'), 'vertebrate'),\n",
       " (('louse', 'arthropod'), 'environmental'),\n",
       " (('eye', 'hole'), 'stretch'),\n",
       " (('eye', 'hole'), 'girl'),\n",
       " (('eye', 'look'), 'you'),\n",
       " (('eye', 'look'), 'flatten'),\n",
       " (('beef', 'chordate'), 'archosaur'),\n",
       " (('beef', 'chordate'), 'athlete'),\n",
       " (('beef', 'ruminant'), 'arthropod'),\n",
       " (('beef', 'ruminant'), 'herbivore'),\n",
       " (('beef', 'ungulate'), 'arthropod'),\n",
       " (('beef', 'ungulate'), 'determine'),\n",
       " (('beef', 'vertebrate'), 'invertebrate'),\n",
       " (('beef', 'vertebrate'), 'player'),\n",
       " (('beef', 'mammal'), 'reptile'),\n",
       " (('beef', 'mammal'), 'twice'),\n",
       " (('beef', 'placental'), 'uterine'),\n",
       " (('beef', 'placental'), 'puzzle'),\n",
       " (('beef', 'cattle'), 'livestock'),\n",
       " (('beef', 'cattle'), 'nightshade'),\n",
       " (('beef', 'animal'), 'human'),\n",
       " (('beef', 'animal'), 'old'),\n",
       " (('beef', 'bovine'), 'cow'),\n",
       " (('beef', 'bovine'), 'stimulate'),\n",
       " (('beef', 'meet'), 'meeting'),\n",
       " (('beef', 'meet'), 'necessary'),\n",
       " (('beef', 'food'), 'eat'),\n",
       " (('beef', 'food'), 'print'),\n",
       " (('exercise', 'event'), 'world'),\n",
       " (('exercise', 'event'), 'practice'),\n",
       " (('exercise', 'work'), 'well'),\n",
       " (('exercise', 'work'), 'practice'),\n",
       " (('exercise', 'train'), 'bus'),\n",
       " (('exercise', 'train'), 'practice'),\n",
       " (('gear', 'wheel'), 'steering'),\n",
       " (('gear', 'wheel'), 'baggage'),\n",
       " (('gear', 'device'), 'bomb'),\n",
       " (('gear', 'device'), 'baggage'),\n",
       " (('link', 'instruction'), 'learning'),\n",
       " (('link', 'instruction'), 'joint'),\n",
       " (('tramcar', 'wagon'), 'sedan'),\n",
       " (('tramcar', 'wagon'), 'feel'),\n",
       " (('tramcar', 'vehicle'), 'car'),\n",
       " (('tramcar', 'vehicle'), 'commodity'),\n",
       " (('dolphin', 'whale'), 'shark'),\n",
       " (('dolphin', 'whale'), 'rabbit'),\n",
       " (('dolphin', 'chordate'), 'archosaur'),\n",
       " (('dolphin', 'chordate'), 'lion'),\n",
       " (('dolphin', 'placental'), 'uterine'),\n",
       " (('dolphin', 'placental'), 'cow'),\n",
       " (('dolphin', 'vertebrate'), 'invertebrate'),\n",
       " (('dolphin', 'vertebrate'), 'gorilla'),\n",
       " (('dolphin', 'mammal'), 'reptile'),\n",
       " (('dolphin', 'mammal'), 'goat'),\n",
       " (('dolphin', 'animal'), 'human'),\n",
       " (('dolphin', 'animal'), 'donkey'),\n",
       " (('dolphin', 'cetacean'), 'songbird'),\n",
       " (('dolphin', 'cetacean'), 'bear'),\n",
       " (('dolphin', 'creature'), 'beast'),\n",
       " (('dolphin', 'creature'), 'cow'),\n",
       " (('octopus', 'mollusk'), 'gastropod'),\n",
       " (('octopus', 'mollusk'), 'conclude'),\n",
       " (('octopus', 'cephalopod'), 'trilobite'),\n",
       " (('octopus', 'cephalopod'), 'beast'),\n",
       " (('octopus', 'animal'), 'human'),\n",
       " (('octopus', 'animal'), 'staff'),\n",
       " (('octopus', 'invertebrate'), 'vertebrate'),\n",
       " (('octopus', 'invertebrate'), 'get'),\n",
       " (('computer', 'machine'), 'gun'),\n",
       " (('computer', 'machine'), 'stereo'),\n",
       " (('computer', 'furniture'), 'clothing'),\n",
       " (('computer', 'furniture'), 'radio'),\n",
       " (('computer', 'useful'), 'helpful'),\n",
       " (('computer', 'useful'), 'stereo'),\n",
       " (('computer', 'tool'), 'use'),\n",
       " (('computer', 'tool'), 'machine'),\n",
       " (('verbena', 'flower'), 'fruit'),\n",
       " (('verbena', 'flower'), 'cyclical'),\n",
       " (('verbena', 'plant'), 'factory'),\n",
       " (('verbena', 'plant'), 'normal'),\n",
       " (('verbena', 'angiosperm'), 'archosaur'),\n",
       " (('verbena', 'angiosperm'), 'cancel'),\n",
       " (('feel', 'think'), 'know'),\n",
       " (('feel', 'think'), 'handle'),\n",
       " (('feel', 'experience'), 'knowledge'),\n",
       " (('feel', 'experience'), 'atmosphere'),\n",
       " (('feel', 'touch'), 'hand'),\n",
       " (('feel', 'touch'), 'atmosphere'),\n",
       " (('hate', 'dislike'), 'liking'),\n",
       " (('hate', 'dislike'), 'worm'),\n",
       " (('centipede', 'invertebrate'), 'vertebrate'),\n",
       " (('centipede', 'invertebrate'), 'say'),\n",
       " (('centipede', 'animal'), 'human'),\n",
       " (('centipede', 'animal'), 'change'),\n",
       " (('centipede', 'arthropod'), 'vertebrate'),\n",
       " (('centipede', 'arthropod'), 'alcohol'),\n",
       " (('decorate', 'change'), 'need'),\n",
       " (('decorate', 'change'), 'old'),\n",
       " (('peace', 'order'), 'help'),\n",
       " (('peace', 'order'), 'order'),\n",
       " (('touch', 'affect'), 'affected'),\n",
       " (('touch', 'affect'), 'move'),\n",
       " (('touch', 'act'), 'law'),\n",
       " (('touch', 'act'), 'move'),\n",
       " (('muskrat', 'chordate'), 'archosaur'),\n",
       " (('muskrat', 'chordate'), 'border'),\n",
       " (('muskrat', 'placental'), 'uterine'),\n",
       " (('muskrat', 'placental'), 'integrity'),\n",
       " (('muskrat', 'mammal'), 'reptile'),\n",
       " (('muskrat', 'mammal'), 'technique'),\n",
       " (('muskrat', 'vertebrate'), 'invertebrate'),\n",
       " (('muskrat', 'vertebrate'), 'determined'),\n",
       " (('muskrat', 'rodent'), 'rat'),\n",
       " (('muskrat', 'rodent'), 'undemocratic'),\n",
       " (('muskrat', 'animal'), 'human'),\n",
       " (('muskrat', 'animal'), 'character'),\n",
       " (('hazelnut', 'tree'), 'pine'),\n",
       " (('hazelnut', 'tree'), 'standardized'),\n",
       " (('hazelnut', 'plant'), 'factory'),\n",
       " (('hazelnut', 'plant'), 'image'),\n",
       " (('zucchini', 'plant'), 'factory'),\n",
       " (('zucchini', 'plant'), 'monotreme'),\n",
       " (('zucchini', 'marrow'), 'bone'),\n",
       " (('zucchini', 'marrow'), 'government'),\n",
       " (('zucchini', 'vine'), 'grape'),\n",
       " (('zucchini', 'vine'), 'hobby'),\n",
       " (('zucchini', 'squash'), 'eggplant'),\n",
       " (('zucchini', 'squash'), 'support'),\n",
       " (('mule', 'ungulate'), 'arthropod'),\n",
       " (('mule', 'ungulate'), 'donkey'),\n",
       " (('mule', 'animal'), 'human'),\n",
       " (('mule', 'animal'), 'horse'),\n",
       " (('mule', 'chordate'), 'archosaur'),\n",
       " (('mule', 'chordate'), 'horse'),\n",
       " (('mule', 'placental'), 'uterine'),\n",
       " (('mule', 'placental'), 'horse'),\n",
       " (('mule', 'equine'), 'feline'),\n",
       " (('mule', 'equine'), 'donkey'),\n",
       " (('mule', 'mammal'), 'reptile'),\n",
       " (('mule', 'mammal'), 'horse'),\n",
       " (('mule', 'vertebrate'), 'invertebrate'),\n",
       " (('mule', 'vertebrate'), 'horse'),\n",
       " (('dill', 'plant'), 'factory'),\n",
       " (('dill', 'plant'), 'time'),\n",
       " (('dill', 'herb'), 'shrub'),\n",
       " (('dill', 'herb'), 'dislike'),\n",
       " (('pelican', 'vertebrate'), 'invertebrate'),\n",
       " (('pelican', 'vertebrate'), 'narcissus'),\n",
       " (('pelican', 'seabird'), 'shorebird'),\n",
       " (('pelican', 'seabird'), 'swallow'),\n",
       " (('pelican', 'chordate'), 'archosaur'),\n",
       " (('pelican', 'chordate'), 'strategy'),\n",
       " (('pelican', 'animal'), 'human'),\n",
       " (('pelican', 'animal'), 'escape'),\n",
       " (('pelican', 'bird'), 'virus'),\n",
       " (('pelican', 'bird'), 'familiarize'),\n",
       " (('ewe', 'sheep'), 'cattle'),\n",
       " (('ewe', 'sheep'), 'feline'),\n",
       " (('ewe', 'ungulate'), 'arthropod'),\n",
       " (('ewe', 'ungulate'), 'baggage'),\n",
       " (('ewe', 'vertebrate'), 'invertebrate'),\n",
       " (('ewe', 'vertebrate'), 'stationary'),\n",
       " (('ewe', 'mammal'), 'reptile'),\n",
       " (('ewe', 'mammal'), 'train'),\n",
       " (('ewe', 'chordate'), 'archosaur'),\n",
       " (('ewe', 'chordate'), 'world'),\n",
       " (('ewe', 'animal'), 'human'),\n",
       " (('ewe', 'animal'), 'relationship'),\n",
       " (('ewe', 'ruminant'), 'bovine'),\n",
       " (('ewe', 'ruminant'), 'question'),\n",
       " (('ewe', 'placental'), 'uterine'),\n",
       " (('ewe', 'placental'), 'till'),\n",
       " (('ptarmigan', 'game'), 'play'),\n",
       " (('ptarmigan', 'game'), 'university'),\n",
       " (('ptarmigan', 'animal'), 'human'),\n",
       " (('ptarmigan', 'animal'), 'negation'),\n",
       " (('ptarmigan', 'grouse'), 'pheasant'),\n",
       " (('ptarmigan', 'grouse'), 'medicine'),\n",
       " (('cod', 'chordate'), 'archosaur'),\n",
       " (('cod', 'chordate'), 'carp'),\n",
       " (('cod', 'vertebrate'), 'invertebrate'),\n",
       " (('cod', 'vertebrate'), 'goldfish'),\n",
       " (('cod', 'animal'), 'human'),\n",
       " (('cod', 'animal'), 'tuna'),\n",
       " (('cod', 'fish'), 'salmon'),\n",
       " (('cod', 'fish'), 'mackerel'),\n",
       " (('cod', 'seafood'), 'shellfish'),\n",
       " (('cod', 'seafood'), 'carp'),\n",
       " (('cod', 'creature'), 'beast'),\n",
       " (('cod', 'creature'), 'dolphin'),\n",
       " (('cod', 'food'), 'eat'),\n",
       " (('cod', 'food'), 'tuna'),\n",
       " (('fact', 'information'), 'data'),\n",
       " (('fact', 'information'), 'mouse'),\n",
       " (('fact', 'album'), 'song'),\n",
       " (('fact', 'album'), 'armor'),\n",
       " (('alligator', 'chordate'), 'archosaur'),\n",
       " (('alligator', 'chordate'), 'lizard'),\n",
       " (('alligator', 'animal'), 'human'),\n",
       " (('alligator', 'animal'), 'crocodile'),\n",
       " (('alligator', 'vertebrate'), 'invertebrate'),\n",
       " (('alligator', 'vertebrate'), 'crocodile'),\n",
       " (('alligator', 'reptile'), 'mammal'),\n",
       " (('alligator', 'reptile'), 'turtle'),\n",
       " (('alligator', 'predator'), 'insect'),\n",
       " (('alligator', 'predator'), 'frog'),\n",
       " (('alligator', 'carnivore'), 'herbivore'),\n",
       " (('alligator', 'carnivore'), 'toad'),\n",
       " (('alligator', 'creature'), 'mysterious'),\n",
       " (('alligator', 'creature'), 'lizard'),\n",
       " ...]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_negative_tuples((data.test_query, data.test_hyper), data, mix_sim_hyper_random, 2)\n",
    "#get_negative_words(('zebra', 'mammal'), (data.train_query, data.train_hyper), data.synonyms, data.neg_vocab, 2)\n",
    "\n",
    "#get_negative_words(('mackerel', 'fish'), (data.train_query, data.train_hyper), data)\n",
    "#get_negative_words(('lime', 'citrus'), (data.train_query + data.test_query, data.train_hyper + data.test_hyper), data, 5)\n",
    "#get_similar_hypernyms(('mackerel', 'fish'), (data.train_query, data.train_hyper), data)\n",
    "#get_negative_tuples((['cat'], ['animal']), data, get_similar_hyponyms, 5)\n",
    "\n",
    "#get_similar_hyponyms(('cat', 'animal'), (data.test_query, data.test_hyper), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(*[(xy[0], x_) for xy, x_ in neg_tuples if xy == ('zebra', 'placental')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that returns negative samples alongside set of positive samples\n",
    "# we need to pass:\n",
    "# the batch hyponym terms, batch of hypernym terms, negative_tuples, tokenizer \n",
    "# to create sequences\n",
    "def extend_batch_with_negatives(batch_X_term, batch_X_hyper, negative_tuples,                              \n",
    "                                tokenizer):\n",
    "    # initialise negative tuples container\n",
    "    positive_words = [(tokenizer.index_word[term_id], tokenizer.index_word[hyper_id]) \\\n",
    "                          for term_id, hyper_id in zip(batch_X_term.flatten(), batch_X_hyper.flatten())]\n",
    "    \n",
    "    # tokenize -ve samples\n",
    "    neg_terms, neg_hyper = zip(*[(qh[0], h) for qh, h in negative_tuples if qh in positive_words])\n",
    "    \n",
    "    neg_terms_seq = tokenizer.texts_to_sequences(neg_terms)\n",
    "    neg_hyper_seq = tokenizer.texts_to_sequences(neg_hyper)\n",
    "\n",
    "    # before increasing size of our batch, let's set the actual y values\n",
    "    # the first n terms are true (1s), and the rest are the -ve samples (0)\n",
    "    batch_y_label = np.concatenate((\n",
    "            np.ones(batch_X_term.shape[0]),\n",
    "            np.zeros(len(neg_terms_seq))\n",
    "    ))\n",
    "    # finally, stack -ve sequences at the bottom of +ves to \n",
    "    # create our final training batch\n",
    "    # at most, batch size will be 192 samples            \n",
    "\n",
    "    batch_X_term = np.vstack((batch_X_term, np.array(neg_terms_seq)))\n",
    "    batch_X_hyper = np.vstack((batch_X_hyper, np.array(neg_hyper_seq)))\n",
    "    \n",
    "    return batch_X_term, batch_X_hyper, batch_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating negative Tuples\n",
      "swordfish chordate 1.0\n",
      "swordfish tuna 0.0\n"
     ]
    }
   ],
   "source": [
    "# non-essential code; just testing the negative_extender\n",
    "\n",
    "_terms = np.array(data.tokenizer.texts_to_sequences(train_query[4:5]))\n",
    "_hypers = np.array(data.tokenizer.texts_to_sequences(train_hyper[4:5]))\n",
    "\n",
    "print \"Generating negative Tuples\"\n",
    "neg_tuples = get_negative_tuples((data.train_query + data.test_query, \n",
    "                                  data.train_hyper + data.test_hyper), data, get_negative_words, 1)\n",
    "\n",
    "\n",
    "_terms, _hypers, _lab = extend_batch_with_negatives(_terms, _hypers, neg_tuples, data.tokenizer)\n",
    "_hypers = data.tokenizer.sequences_to_texts(_hypers)\n",
    "for idx, _t in enumerate(data.tokenizer.sequences_to_texts(_terms)):\n",
    "    print _t, _hypers[idx], _lab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example on how to use negative_tuples to get 2 negative samples\n",
    "zip(*[(q,h)for q, h in negative_tuples if q in ['cat', 'poplar','black']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total number of words in training/test set\n",
    "print data.tokenizer.num_words\n",
    "# print number of unique words\n",
    "print len(data.tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_CRIM_model(phi_k=1, train_embeddings=False,\\\n",
    "                   embeddings_dim=300, vocab_size=1000,\\\n",
    "                   embeddings_matrix=None,\n",
    "                   phi_init = None,\n",
    "                   phi_activity_regularisation = None,\n",
    "                   sigmoid_kernel_regularisation = None,\n",
    "                   sigmoid_bias_regularisation = None,\n",
    "                   sigmoid_kernel_constraint = None\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size + 1, embeddings_dim, embeddings_constraint = UnitNorm(axis=1), \n",
    "                                name='TermEmbedding')\n",
    "    hypo_embedding = embedding_layer(hypo_input)    \n",
    "    hyper_embedding = embedding_layer(hyper_input)\n",
    "    \n",
    "    # Add Dropout to avoid overfit\n",
    "    #hypo_embedding = Dropout(0.5)(hypo_embedding)\n",
    "    #hyper_embedding = Dropout(0.5)(hyper_embedding)\n",
    "\n",
    "    # we will set the weights before compilation and training\n",
    "    # here I have two varieties:\n",
    "    # one is a standard random normal initialiser, mean=0, std 0.01\n",
    "    rand_norm_init = RandomNormal(mean = 0.0, stddev=0.01, seed=42)\n",
    "    # this one is custom and is based on the CRIM paper. \n",
    "    # we initialise on random normal noise applied to an identity matrix\n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), \n",
    "                                 mean=0., stddev=0.01)\n",
    "\n",
    "        return identity * rnorm\n",
    "\n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init, \n",
    "                               name='Phi%d' % (i))(hypo_embedding))\n",
    "\n",
    "    #phi1 = Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                #kernel_initializer=random_identity, name='Phi1')(hypo_embedding)\n",
    "\n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "    \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=Ones,\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer=Zeros,                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       )(phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "\n",
    "    # inject pre-trained embedding weights into Embedding layer\n",
    "    model.get_layer(name='TermEmbedding').set_weights([embeddings_matrix])\n",
    "    model.get_layer(name='TermEmbedding').trainable = train_embeddings    \n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement mini-batch stochastic training with negative sampling\n",
    "\n",
    "At every epoch, we will randomly shuffle the training set and split it in 32 distinct batches\n",
    "For every one of the positive samples, we will generate m negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training algorithm incorporates mini-batch stochastic descent and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,       # the model which parameters will be learnt\n",
    "          epochs,      # number of epochs to run          \n",
    "          batch_size,  # size of mini-batch\n",
    "          m,           # number of negative samples\n",
    "          data,        # data required for training                              \n",
    "          neg_strategy\n",
    "         ):\n",
    "\n",
    "    # create negative tuples\n",
    "    #negative_tuples = get_negative_tuples(data.train_query + data.test_query,\n",
    "     #                                     data.train_hyper + data.test_hyper, data.neg_vocab, m)\n",
    "    \n",
    "    print \"Generating negative tuples...\"\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_strategy, m)\n",
    "    print \"Negative tuples...ok\"\n",
    "    \n",
    "    # create sequences\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "                \n",
    "    samples = np.arange(len(term_train_seq))\n",
    "    validation_samples = np.arange(len(term_test_seq))\n",
    "    \n",
    "    # train algorithm\n",
    "    for epoch in range(epochs):\n",
    "        # reset loss\n",
    "        loss = 0.\n",
    "        test_loss = 0.\n",
    "                        \n",
    "        np.random.shuffle(samples)\n",
    "\n",
    "        shuffled_X_term, shuffled_X_hyper =\\\n",
    "            np.array(term_train_seq, dtype='int32')[samples],\\\n",
    "            np.array(hyper_train_seq, dtype='int32')[samples]\n",
    "\n",
    "        for b in range(0, len(samples), batch_size):\n",
    "            # product mini-batch, consisting of 32 +ve samples\n",
    "            batch_X_term = shuffled_X_term[b:b + batch_size] \n",
    "            batch_X_hyper = shuffled_X_hyper[b:b + batch_size]\n",
    "\n",
    "            # complement +ve samples with negatives\n",
    "            batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "            extend_batch_with_negatives(batch_X_term, batch_X_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "            \n",
    "            # shuffle validation set indices\n",
    "            np.random.shuffle(validation_samples)\n",
    "            # pick batch of shuffled test instances with size equal to training batch\n",
    "            batch_X_test_term, batch_X_test_hyper =\\\n",
    "                np.array(term_test_seq, dtype='int32')[validation_samples[:batch_size]],\\\n",
    "                np.array(hyper_test_seq, dtype='int32')[validation_samples[:batch_size]]\n",
    "            \n",
    "            # distort test batch with some negatives to check how algorithm fares with\n",
    "            # negatives\n",
    "            batch_X_test_term, batch_X_test_hyper, batch_y_test_label =\\\n",
    "            extend_batch_with_negatives(batch_X_test_term, batch_X_test_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "\n",
    "            # train on batch\n",
    "            loss += model.train_on_batch([batch_X_term, batch_X_hyper], \n",
    "                                          batch_y_label)[0]\n",
    "            \n",
    "            test_loss += model.test_on_batch([batch_X_test_term, batch_X_test_hyper], \n",
    "                                              batch_y_test_label)[0]                \n",
    "            \n",
    "        print('Epoch:', epoch+1, 'Loss:', loss, 'Test Loss:', test_loss)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harness code to pass in the various parameters to the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "('Epochs: ', 8, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', True, 'Negative sampling: ', 'mix_hyper_synonym', 'Phi Init: ', 'random_normal')\n",
      "Generating negative tuples...\n",
      "Negative tuples...ok\n",
      "('Epoch:', 1, 'Loss:', 57.355670511722565, 'Test Loss:', 59.72325420379639)\n",
      "('Epoch:', 2, 'Loss:', 27.334548115730286, 'Test Loss:', 36.40574422478676)\n",
      "('Epoch:', 3, 'Loss:', 22.817064203321934, 'Test Loss:', 33.09851683676243)\n",
      "('Epoch:', 4, 'Loss:', 20.19028501957655, 'Test Loss:', 31.51719556748867)\n",
      "('Epoch:', 5, 'Loss:', 18.312923535704613, 'Test Loss:', 30.627734020352364)\n",
      "('Epoch:', 6, 'Loss:', 16.621946439146996, 'Test Loss:', 29.96564643085003)\n",
      "('Epoch:', 7, 'Loss:', 15.119618479162455, 'Test Loss:', 29.58227378129959)\n",
      "('Epoch:', 8, 'Loss:', 13.738165482878685, 'Test Loss:', 31.057048320770264)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "# Phi layer initialiser\n",
    "def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "    identity = K.eye(shape[-1], dtype='float32')\n",
    "\n",
    "    rnorm = K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.01)\n",
    "\n",
    "    return identity * rnorm\n",
    "\n",
    "def random_normal(shape, dtype=\"float32\", partition_info=None): \n",
    "    return K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.05)\n",
    "\n",
    "#rand_norm_m0_sd001 = RandomNormal(mean = 0.0, stddev=0.01, seed=42)\n",
    "#rand_norm = RandomNormal(mean = 0.0, stddev=1., seed=42)\n",
    "\n",
    "# negative sampling options\n",
    "neg_sampling_options = {'synonym':get_negative_words, \n",
    "                        'mix_hyper_synonym': mix_sim_hyper_random,\n",
    "                        'similar_hyponym': get_similar_hyponyms\n",
    "                       }\n",
    "\n",
    "# phi random init options\n",
    "phi_init_options = {'random_identity': random_identity, 'random_normal': random_normal}\n",
    "\n",
    "# implement mini-batch stochastic training\n",
    "epochs = 8\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# number of negative samples\n",
    "m = 10\n",
    "phi_k = 1\n",
    "train_embeddings = True\n",
    "negative_option = 'mix_hyper_synonym'\n",
    "phi_init_option = 'random_normal'\n",
    "np.random.seed(42)\n",
    "\n",
    "# create model\n",
    "crim_model = get_CRIM_model(phi_k = phi_k, train_embeddings = train_embeddings,\n",
    "                            embeddings_dim = data.embeddings_dim, vocab_size = 2931,\n",
    "                            embeddings_matrix = data.embedding_matrix,\n",
    "                            phi_init = phi_init_options[phi_init_option],                            \n",
    "                            sigmoid_kernel_regularisation = None,\n",
    "                            sigmoid_bias_regularisation = None,\n",
    "                            sigmoid_kernel_constraint = ForceToOne()\n",
    "                           )\n",
    "\n",
    "print \"Training started...\"\n",
    "print ('Epochs: ', epochs, 'Batch size: ', batch_size, 'm: ', m, 'pki_k: ', phi_k, 'train_embeddings: ', train_embeddings,\n",
    "      'Negative sampling: ', negative_option, 'Phi Init: ', phi_init_option)\n",
    "\n",
    "train(crim_model, epochs, batch_size, m, data, neg_sampling_options[negative_option])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.]], dtype=float32), array([-0.59664816], dtype=float32)]"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a look at the prediction layer weights\n",
    "crim_model.get_layer(name='Prediction').get_weights()\n",
    "#model.get_layer(name='Phi0').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010779559\n"
     ]
    }
   ],
   "source": [
    "# get phi mean value\n",
    "projs = ['Phi0']#, 'Phi1', 'Phi2']\n",
    "for p in projs:\n",
    "    print np.mean(crim_model.get_layer(name=p).get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation  code\n",
    "\n",
    "Main observations:<br>\n",
    "1. Tendency is for the model to overfit if we make the model larger than 1 projection matrix;\n",
    "1. Negative samples are important for the model to learn which words are not hypernyms;\n",
    "1. Although the model does seem to learn the correct words that are related to hypernymy to the query terms, it does not stop it from predicting with high confidence that similar but completely unrelated words are also hypernyms;\n",
    "    1. This is really apparent for animals where the model is not able to distinguish between vertebrate and invertebrate; mammal; animal; and so forth;\n",
    "    1. It's possible that we did not have enough examples to distinguish the various types of animals from each other;\n",
    "    1. Also, more targeted negative samples could have helped but these would have to be hand-created;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_crim(data, model, neg_strategy):\n",
    "    # initialise metrics\n",
    "    train_accuracy = 0.\n",
    "    basic_test_accuracy = 0.\n",
    "    basic_test_f1 = 0.\n",
    "    negative_test_accuracy = 0.\n",
    "    negative_test_f1 = 0.\n",
    "    \n",
    "    # tokenise training data\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    y_train_labels = [1.] * len(term_train_seq)\n",
    "    print \"Evaluating training set...\" \n",
    "        \n",
    "    _, train_accuracy = np.round(model.evaluate([term_train_seq, hyper_train_seq], y_train_labels), 5)\n",
    "\n",
    "    # evaluate on test set but create negative examples at a ratio of 1:1\n",
    "    # tokenize testing data\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "\n",
    "    print \"Evaluating given test dataset size (\", len(term_test_seq), ') items.'\n",
    "    # convert to arrays\n",
    "    term_test_seq = np.array(term_test_seq, dtype='int32')\n",
    "    hyper_test_seq = np.array(hyper_test_seq, dtype='int32')\n",
    "    y_test_labels = [1.] * len(term_test_seq)\n",
    "    \n",
    "    _, basic_test_accuracy = np.round(model.evaluate([term_test_seq, hyper_test_seq], y_test_labels), 5)\n",
    "    \n",
    "    predictions = model.predict([term_test_seq, hyper_test_seq])\n",
    "    binary_predictions = map(lambda p: 1. if p >= 0.5 else 0., predictions)\n",
    "    basic_test_f1 = np.round(f1_score(y_test_labels, binary_predictions), 5)\n",
    "\n",
    "\n",
    "    # generate 2 negative tuple for every unique entry in the test_set\n",
    "    if neg_strategy:\n",
    "        print \"Augmenting basic test samples with negatives...\"\n",
    "        negative_tuples = get_negative_tuples((data.test_query, data.test_hyper), \n",
    "                                               data, neg_strategy, 2)\n",
    "\n",
    "        term_test_seq, hyper_test_seq, y_test_labels =\\\n",
    "            extend_batch_with_negatives(term_test_seq, hyper_test_seq,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer)                  \n",
    "    \n",
    "        print \"Evaluating extended test dataset size (\", term_test_seq.shape[0], ') items.'\n",
    "        _, negative_test_accuracy = np.round(model.evaluate([term_test_seq, hyper_test_seq], y_test_labels), 5)\n",
    "        \n",
    "        predictions = model.predict([term_test_seq, hyper_test_seq])\n",
    "        binary_predictions = map(lambda p: 1. if p >= 0.5 else 0., predictions)\n",
    "        negative_test_f1 = np.round(f1_score(y_test_labels, binary_predictions), 5)\n",
    "    \n",
    "    \n",
    "    return train_accuracy, basic_test_accuracy, basic_test_f1, negative_test_accuracy, negative_test_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating training set...\n",
      "4338/4338 [==============================] - 1s 137us/step\n",
      "Evaluating given test dataset size ( 1533 ) items.\n",
      "1533/1533 [==============================] - 0s 118us/step\n",
      "Augmenting basic test samples with negatives...\n",
      "Evaluating extended test dataset size ( 4599 ) items.\n",
      "4599/4599 [==============================] - 1s 122us/step\n",
      "Training accuracy: 0.71784 ; Test accuracy: 0.61709 ; Test F1: 0.76321 ; Negative Test accuracy: 0.84475 ; Negative Test F1: 0.72602\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, basic_test_accuracy, basic_test_f1, negative_test_accuracy, negative_test_f1 =\\\n",
    "    evaluate_crim(data, crim_model, get_negative_words)\n",
    "    \n",
    "print \"Training accuracy:\", train_accuracy, \"; Test accuracy:\", basic_test_accuracy, \"; Test F1:\", basic_test_f1, \"; Negative Test accuracy:\",negative_test_accuracy, \"; Negative Test F1:\", negative_test_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,hyper in zip(test_query, test_hyper):\n",
    "    print word, hyper, crim_model.predict( [[data.tokenizer.word_index[word]], [data.tokenizer.word_index[hyper]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment results\n",
    "('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1)<br>\n",
    "Training accuracy: 0.7416; Testing accuracy: 0.72380; Test f1: 0.733013\n",
    "\n",
    "('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 2)<br>\n",
    "Training accuracy: 0.81766; Testing accuracy: 0.74435; Test f1: 0.75947\n",
    "\n",
    "('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 5)<br>\n",
    "Training accuracy: 0.88428; Testing accuracy: 0.75709; Test f1: 0.7779\n",
    "\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 5)<br>\n",
    "-- lesser readings like with phi_k = 2\n",
    "\n",
    "### Try with a smaller number of negative samples and projection matrices\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1)<br>\n",
    "Training accuracy: 0.79599; Testing accuracy: 0.75339; Test f1: 0.77307 <br>\n",
    "-- fairly reasonable results.  \n",
    "\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 1, 'pki_k: ', 1)<br>\n",
    "Training accuracy: 0.98317; Testing accuracy: 0.78216; Test f1: 0.84217 <br>\n",
    "-- smallest model gives best results\n",
    "\n",
    "('Epochs: ', 10, 'Batch size: ', 32, 'm: ', 1, 'pki_k: ', 5)<br>\n",
    "Training accuracy: 0.98986; Testing accuracy: 0.77476; Test f1: 0.83681 <br>\n",
    "-- increasing the complexity of the model, increases overfit but doesn't really improve the quality of\n",
    "   the predictions\n",
    "   \n",
    "### Attempt to fine-tune embeddings\n",
    "Increases overfit.  When qualitatively studying hypernyms returned by model, I find quite a lot of overconfident matches.  Ex: matching \"cold\" (From training data):<br>\n",
    "[('animal', 1.0), ('vertebrate', 1.0), ('vehicle', 1.0), ('artifact', 1.0), ('tool', 1.0), ('chordate', 1.0), ('creature', 1.0), ('bird', 1.0), ('mammal', 1.0), ('artefact', 0.9999999)]\n",
    "\n",
    "Training accuracy: 0.98778; Testing accuracy: 0.72421; Test f1: 0.79844 \n",
    "\n",
    "### Removal of sigmoid layer regularisation; phi layer initialised on random_normal distribution (mean=0, sd=0.01)\n",
    "('Epochs: ', 20, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ',False)\n",
    "Training accuracy: 0.92001; Testing accuracy: 0.85236; Test f1: 0.74861 \n",
    "\n",
    "### Cast CRIM as Yamane but with no soft-clustering, learning single projection matrix\n",
    "('Epochs: ', 20, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'synonym', 'Phi Init: ', 'random_normal') (std 0.05)\n",
    "\n",
    "Training accuracy: 0.66897 ; Test accuracy: 0.56621 ; Test F1: 0.72303 ; Negative Test accuracy: 0.84497 ; Negative Test F1: 0.70886<br>\n",
    "\n",
    "* Increasing projections does improve the evaluation metrics\n",
    "\n",
    "('Epochs: ', 20, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 3, 'train_embeddings: ', False, 'Negative sampling: ', 'synonym', 'Phi Init: ', 'random_normal') (std 0.01)\n",
    "\n",
    "Training accuracy: 0.92808 ; Test accuracy: 0.64188 ; Test F1: 0.78188 ; Negative Test accuracy: 0.85236 ; Negative Test F1: 0.74348\n",
    "\n",
    "* Increasing projections and loosening constraint that keeps LR weights at 1, the projection matrices still develop similarly.  Classification evaluation results improve:\n",
    "\n",
    "Training accuracy: 0.9811 ; Test accuracy: 0.70189 ; Test F1: 0.82484 ; Negative Test accuracy: 0.84518 ; Negative Test F1: 0.7514\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This bit generates hypernyms\n",
    "Requires a list of candidate hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 0.96224177),\n",
       " ('human', 0.9570506),\n",
       " ('beast', 0.9486063),\n",
       " ('vertebrate', 0.948362),\n",
       " ('animal', 0.94165653),\n",
       " ('chordate', 0.9388889),\n",
       " ('object', 0.9273808),\n",
       " ('food', 0.92394197),\n",
       " ('hobby', 0.92351276),\n",
       " ('game', 0.91993695),\n",
       " ('insect', 0.9094172),\n",
       " ('mammal', 0.90893006),\n",
       " ('arthropod', 0.9065798),\n",
       " ('shape', 0.89415187),\n",
       " ('place', 0.8843403)]"
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "\n",
    "def crim_get_top_hypernyms(query, hyper_candidates, model, data, top):\n",
    "    query_index = data.tokenizer.word_index[query]    \n",
    "    valid_candidates = filter(lambda w: w != [query_index], hyper_candidates)    \n",
    "    \n",
    "    candidate_sim = map(lambda x: model.predict([[query_index], x]).flatten()[0], valid_candidates)        \n",
    "    top_idx = np.argsort(candidate_sim)[::-1][:top]\n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "    return [(data.tokenizer.index_word[t], candidate_sim[top_idx[i]]) for i, t in enumerate(top_hyper)]\n",
    "\n",
    "\n",
    "crim_get_top_hypernyms('cat', hyper_candidates, crim_model, data, 15)\n",
    "#print \"-\" * 30\n",
    "#print crim_get_top_hypernyms('apartment', hyper_candidates, crim_model, data, 10)\n",
    "#print \"-\" * 30\n",
    "#print crim_get_top_hypernyms('carrot', hyper_candidates, crim_model, data, 10)\n",
    "#print \"-\" * 30\n",
    "#print crim_get_top_hypernyms('dagger', hyper_candidates, crim_model, data, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25018686]], dtype=float32)"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = data.tokenizer.word_index['cat']\n",
    "j = data.tokenizer.word_index['plant']\n",
    "crim_model.predict([[i], [j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hypernyms in the training set exhibit a sort of power distribution \n",
    "# animal is features in ~6.35% of the samples\n",
    "\n",
    "# in this universe the model will start to think that everything is an animal\n",
    "hyper_freq = Counter(data.train_hyper)\n",
    "for key, value in sorted(hyper_freq.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "    print \"%s: %s\" % (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Yamane et al. \n",
    "\n",
    "## Clusters learnt together with projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attempt custom constraint to keep weight fixed at 1.\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_new_cluster_model(embedding_layer, phi_dim):\n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')    \n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    hypo_embedding, hyper_embedding = embedding_layer([hypo_input, hyper_input])\n",
    "    \n",
    "    def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "        identity = K.eye(shape[-1], dtype='float32')\n",
    "    \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.01)\n",
    "\n",
    "        return identity * rnorm\n",
    "\n",
    "    rand_0_01 = RandomNormal(mean=0., stddev=0.01)\n",
    "    \n",
    "    phi = Dense(phi_dim, activation=None, use_bias=False, \n",
    "                kernel_initializer=random_identity,\n",
    "                #kernel_regularizer=l2(0.001),                \n",
    "                name='Phi')(hypo_embedding)\n",
    "    \n",
    "    # flatten phi and hyper_embedding tensors\n",
    "    phi = Flatten()(phi)\n",
    "    hyper_embedding = Flatten()(hyper_embedding)\n",
    "    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    force_to_one = ForceToOne()\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", \n",
    "                        bias_initializer=Zeros,\n",
    "                        kernel_initializer=Ones,\n",
    "                        kernel_constraint= force_to_one,                        \n",
    "                        bias_regularizer=l2(0.001), \n",
    "                        name='Prediction')(phi_hyper)\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "    \n",
    "    # compile using binary_crossentropy loss\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = get_new_cluster_model(embedding_layer, 300)\n",
    "#model.get_layer(name='Phi').get_weights()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't need a unique embedding layer for every sub-model.  \n",
    "\n",
    "Instead, we can create a separate model for the embeddings and set the weights according to the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_model(dim, embedding_matrix):\n",
    "    hypo_input = Input(shape=(1,))\n",
    "    hyper_input = Input(shape=(1,))\n",
    "\n",
    "    word_embedding = Embedding(embedding_matrix.shape[0], dim, name='WE')\n",
    "\n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    embedding_model = Model(inputs=[hypo_input, hyper_input], outputs=[hypo_embedding, hyper_embedding])\n",
    "\n",
    "    # inject pre-trained embeddings into this mini, resusable model/layer\n",
    "    embedding_model.get_layer(name='WE').set_weights([embedding_matrix])\n",
    "    embedding_model.get_layer(name='WE').trainable = False\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YamaneCluster:\n",
    "    def __init__(self, embedding_layer, phi_dim=300):\n",
    "        self.model = get_new_cluster_model(embedding_layer, phi_dim)\n",
    "        self.epoch_count = 0\n",
    "        self.loss = 0.\n",
    "        self.test_loss = 0.\n",
    "    \n",
    "    def increment_epoch(self):\n",
    "        self.epoch_count += 1\n",
    "        \n",
    "    def update_loss(self, new_loss):\n",
    "        self.loss += new_loss\n",
    "        \n",
    "    def update_test_loss(self, new_loss):\n",
    "        self.test_loss += new_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yamane et al. training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yamane_train(\n",
    "    epochs,      # number of epochs to run\n",
    "    m,           # number of negative samples\n",
    "    data,        # class instance containing all the data required for training/testing\n",
    "          #train_query, # input sequence of hyponyms\n",
    "          #train_hyper, # input sequence of hypernyms\n",
    "          #valid_query, # validation sequence of hyponyms\n",
    "          #valid_hyper, # validation sequence of hypernyms       \n",
    "          #tokenizer,        # tokenizer used to generate sequences\n",
    "    embedding_layer,\n",
    "    threshold    = 0.15,     # threshold; similarity below this score will trigger new cluster\n",
    "    neg_strategy = get_negative_words):    # inject lambda responsible for determining negative sample choice\n",
    " \n",
    "\n",
    "    # create negative tuples\n",
    "    #negative_tuples = get_negative_tuples(data.train_query + data.test_query, \n",
    "    #                                     data.train_hyper + data.test_hyper, \n",
    "    #                                    data.neg_vocab, m)\n",
    "    \n",
    "    print \"Generating negative tuples...\"\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_strategy, m)\n",
    "    print \"Negative tuples...ok\"\n",
    "    \n",
    "    # create sequences\n",
    "    # we have two sets of inputs: one for training query and hypernym terms;\n",
    "    #                             another for the validation query/hyper terms;\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "    \n",
    "    # convert all to arrays\n",
    "    term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq =\\\n",
    "    [np.array(x, dtype='int32') for x in term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq]\n",
    "            \n",
    "    # this list stores which cluster each training sequence pertains to\n",
    "    sample_clusters = np.zeros(len(term_train_seq), dtype='int32')\n",
    "    \n",
    "    print (\"m: \", m, \"lambda: \", threshold, \"max epoch per cluster: \", epochs)\n",
    "    print \"Sample clusters size: \", len(sample_clusters)\n",
    "    # list containing 1 model per cluster\n",
    "    clusters = []\n",
    "    # add default model to our list of models\n",
    "    # we share the embedding layer loaded with the pre-trained weights\n",
    "    # append tuple where 1st element is the cluster and 2nd element is the \n",
    "    # number of epochs that cluster is trained\n",
    "    \n",
    "    clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim))\n",
    "    \n",
    "    # get training set indices\n",
    "    indices = np.arange(len(term_train_seq))  \n",
    "    \n",
    "    # get test set indices\n",
    "    test_indices = np.arange(len(term_test_seq))\n",
    "            \n",
    "    # initialise each training sample to cluster 0\n",
    "    sample_clusters[indices] = 0        \n",
    "    \n",
    "    # seed random generator\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # indicator of \"current\" sample cluster index\n",
    "    z_i = 0\n",
    "    \n",
    "    # train algorithm\n",
    "    #for epoch in range(epochs):\n",
    "    epoch = 0\n",
    "    test_loss = 0.    \n",
    "    \n",
    "    while np.min([c.epoch_count for c in clusters]) < epochs:\n",
    "        # reset loss for each cluster                        \n",
    "        for c in clusters:\n",
    "            if c.epoch_count < epochs:                \n",
    "                c.loss = 0.\n",
    "            c.test_loss = 0.\n",
    "        \n",
    "        test_loss = 0.\n",
    "        \n",
    "        # shuffle indices every epoch\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # train algorithm by stochastic gradient descent, one sample at a time\n",
    "        for idx, i in enumerate(indices):                        \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print \"Processed \", idx+1, \"samples...\"\n",
    "            \n",
    "            # calculate similarity on all clusters\n",
    "            sim = map(lambda x: x.model.predict([term_train_seq[i], hyper_train_seq[i]]), clusters)\n",
    "            max_sim = np.argmax(sim)\n",
    "            #print \"Term:\", tokenizer.index_word[term_train_seq[i][0]], 'Hyper:', tokenizer.index_word[hyper_train_seq[i][0]], \"Max Similarity cluster:\", max_sim, \"(sim = %0.8f)\" % (sim[max_sim])\n",
    "            # limit cluster creation to a max of 25.\n",
    "            if ((sim[max_sim] < threshold) and (len(clusters) < 25)): \n",
    "                # add new cluster to list of clusters\n",
    "                clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim))                \n",
    "                # assign current cluster index to latest model\n",
    "                z_i = len(clusters) - 1\n",
    "                sample_clusters[i] = z_i\n",
    "            else:            \n",
    "                z_i = max_sim\n",
    "                sample_clusters[i] = z_i                \n",
    "            \n",
    "            \n",
    "            # if current cluster reached/exceeded epoch count, skip current sample (i.e don't update cluster)\n",
    "            if clusters[z_i].epoch_count < epochs:                                            \n",
    "                # extend samples in cluster with negative samples\n",
    "                batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "                    extend_batch_with_negatives(term_train_seq[i], \n",
    "                                                hyper_train_seq[i],\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "\n",
    "                # update parameters of cluster \n",
    "                clusters[z_i].update_loss(\n",
    "                    clusters[z_i].model.train_on_batch([batch_X_term, batch_X_hyper], batch_y_label)[0]\n",
    "                )\n",
    "            \n",
    "            # measure test loss \n",
    "            # every 32 samples (and updates are processed), we will test performance on validation set\n",
    "            # of 32 randomly chosen samples. We will record test loss of every cluster and report on \n",
    "            # lowest loss\n",
    "            \n",
    "            if (idx + 1) % 5000 == 0:\n",
    "                np.random.shuffle(test_indices)\n",
    "                batch_query, batch_hyper = term_test_seq[test_indices[:32]], hyper_test_seq[test_indices[:32]]\n",
    "                batch_query, batch_hyper, test_y_label =\\\n",
    "                    extend_batch_with_negatives(batch_query, \n",
    "                                                batch_hyper,\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "                #batch_label = [1.] * batch_query.shape[0]\n",
    "                for q, h, l in zip(batch_query, batch_hyper, test_y_label):                                    \n",
    "                    test_losses = map(lambda c: c.model.test_on_batch([q, h], [l])[0], clusters)\n",
    "                    best_cluster = np.argmin(test_losses)\n",
    "                    clusters[best_cluster].update_test_loss(\n",
    "                        test_losses[best_cluster]\n",
    "                    )\n",
    "                    \n",
    "                                                                                                                      \n",
    "        # increase epoch count for clusters\n",
    "        for cluster in clusters:            \n",
    "            cluster.epoch_count += 1\n",
    "                \n",
    "        print('Epoch:', max([c.epoch_count for c in clusters]), 'Cluster #:', len(clusters) ,\n",
    "              'Loss:', np.mean([c.loss for c in clusters]),\n",
    "              'Test Loss:', np.mean([c.test_loss for c in clusters]))\n",
    "    return clusters, sample_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Generating negative tuples...\n",
      "Negative tuples...ok\n",
      "('m: ', 5, 'lambda: ', 0.15, 'max epoch per cluster: ', 13)\n",
      "Sample clusters size:  4338\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 1, 'Cluster #:', 5, 'Loss:', 424.06980473771694, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 2, 'Cluster #:', 8, 'Loss:', 209.78459176700562, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 3, 'Cluster #:', 9, 'Loss:', 152.27852884462723, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 4, 'Cluster #:', 12, 'Loss:', 113.76620229702287, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 5, 'Cluster #:', 14, 'Loss:', 87.48968440287614, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 6, 'Cluster #:', 15, 'Loss:', 71.93684632508084, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 7, 'Cluster #:', 16, 'Loss:', 61.276110916922335, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 8, 'Cluster #:', 18, 'Loss:', 52.13947001311721, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 9, 'Cluster #:', 18, 'Loss:', 46.92406466473929, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 10, 'Cluster #:', 18, 'Loss:', 38.54392140723454, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 11, 'Cluster #:', 18, 'Loss:', 34.45376912199168, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 12, 'Cluster #:', 18, 'Loss:', 30.979884763196523, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 13, 'Cluster #:', 18, 'Loss:', 27.966043333393625, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 14, 'Cluster #:', 18, 'Loss:', 25.848024627271418, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 15, 'Cluster #:', 18, 'Loss:', 24.327525575412437, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 16, 'Cluster #:', 18, 'Loss:', 23.016247507883236, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 17, 'Cluster #:', 18, 'Loss:', 22.168324165744707, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 18, 'Cluster #:', 18, 'Loss:', 21.594313230236164, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 19, 'Cluster #:', 18, 'Loss:', 21.1504987717037, 'Test Loss:', 0.0)\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "('Epoch:', 20, 'Cluster #:', 18, 'Loss:', 20.853390683187172, 'Test Loss:', 0.0)\n",
      "2018-11-12 10:38:00.484512\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# initialise embedding later which will be shared among all clusters\n",
    "embedding_layer = get_embeddings_model(dim=data.embeddings_dim, embedding_matrix=data.embedding_matrix)\n",
    "epochs = 13\n",
    "m = 5\n",
    "\n",
    "print \"Training started...\"\n",
    "clusters, sample_clusters =\\\n",
    "    yamane_train(epochs, m, \n",
    "                 data,\n",
    "                 embedding_layer,\n",
    "                 threshold = 0.15,\n",
    "                 neg_strategy = mix_sim_hyper_random\n",
    "                 )\n",
    "\n",
    "print datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 20, 20, 20, 20, 19, 19, 19, 18, 17, 17, 17, 16, 16, 15, 14, 13, 13]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.epoch_count for c in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running experiment to try and get the best performance out of the Yamane model.\n",
    "* By constraining the weight to 1., we stop the model from being over-zealous in predicting hypernymy;\n",
    "* Doing that increased the size of the weights in the Phi layer and the first cluster (0) was especially impacted;\n",
    "* To keep the weights of phi small, I introduced activation regularisation which intends to keep the phi.X_hypo dot product value.  However, this resulted in a large loss and inability to decrease loss even after several iterations.\n",
    "* Decreasing the threshold controls the number of clusters.  At lambda=0.2, we end up with around 20 clusters; while with a lambda of 0.15 we get eight clusters by the end of 30 epochs.\n",
    "* The number of negative samples also affects the number of clusters.  Higher -ve sample amounts return more clusters.\n",
    "\n",
    "I need to be able to measure performance better:\n",
    "* For starters, I need to include loss on unseen data in the training algorithm;\n",
    "* Secondly, I need to implement proper evaluation score.  Since this model is actually a collection of models (ensemble technique you could say), evaluation scores must be all hand-coded.\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print some stats\n",
    "* Number of samples per cluster;\n",
    "* Loss per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 996, 1: 747, 2: 303, 3: 254, 4: 251, 5: 250, 17: 190, 16: 158, 11: 137, 15: 136, 13: 133, 7: 131, 6: 128, 10: 126, 9: 115, 12: 112, 14: 97, 8: 74})\n",
      "------------------------------\n",
      "Train and test loss per cluster\n",
      "0 20 23.98701424896717\n",
      "1 20 23.89376202598214\n",
      "2 20 9.941689834464341\n",
      "3 20 17.530611298047006\n",
      "4 20 19.239212260581553\n",
      "5 19 20.823654890991747\n",
      "6 19 17.05653603747487\n",
      "7 19 16.639262665063143\n",
      "8 18 14.652702376246452\n",
      "9 17 19.35460589081049\n",
      "10 17 20.094996355473995\n",
      "11 17 22.820394083857536\n",
      "12 16 20.502011947333813\n",
      "13 16 23.752162247896194\n",
      "14 15 16.37061956524849\n",
      "15 14 23.684391610324383\n",
      "16 13 27.60558620095253\n",
      "17 13 37.411818757653236\n"
     ]
    }
   ],
   "source": [
    "# show distribution of samples over the trained clusters\n",
    "print Counter(sample_clusters)\n",
    "print \"-\"*30\n",
    "print \"Train and test loss per cluster\"\n",
    "\n",
    "for idx, c in enumerate(clusters):\n",
    "    print idx, c.epoch_count, c.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['produce',\n",
       " 'cloak',\n",
       " 'cloak',\n",
       " 'cloak',\n",
       " 'add',\n",
       " 'glove',\n",
       " 'glove',\n",
       " 'glove',\n",
       " 'glass',\n",
       " 'beet',\n",
       " 'beet',\n",
       " 'plum',\n",
       " 'plum',\n",
       " 'turnip',\n",
       " 'turnip',\n",
       " 'catfish',\n",
       " 'wool',\n",
       " 'cold',\n",
       " 'mackerel',\n",
       " 'rose',\n",
       " 'flower',\n",
       " 'wrench',\n",
       " 'wrench',\n",
       " 'wrench',\n",
       " 'shirt',\n",
       " 'shirt',\n",
       " 'shirt',\n",
       " 'couch',\n",
       " 'couch',\n",
       " 'couch',\n",
       " 'light',\n",
       " 'phrase',\n",
       " 'head',\n",
       " 'whole',\n",
       " 'whole',\n",
       " 'whole',\n",
       " 'salt',\n",
       " 'trout',\n",
       " 'battleship',\n",
       " 'battleship',\n",
       " 'candy',\n",
       " 'grave',\n",
       " 'copper',\n",
       " 'paint',\n",
       " 'paint',\n",
       " 'shovel',\n",
       " 'shovel',\n",
       " 'shovel',\n",
       " 'pistol',\n",
       " 'pistol',\n",
       " 'pistol',\n",
       " 'butter',\n",
       " 'freezer',\n",
       " 'freezer',\n",
       " 'freezer',\n",
       " 'freezer',\n",
       " 'stove',\n",
       " 'stove',\n",
       " 'stove',\n",
       " 'stove',\n",
       " 'book',\n",
       " 'fish',\n",
       " 'goat',\n",
       " 'bread',\n",
       " 'cherry',\n",
       " 'cherry',\n",
       " 'log',\n",
       " 'dance',\n",
       " 'parsley',\n",
       " 'parsley',\n",
       " 'guitar',\n",
       " 'guitar',\n",
       " 'guitar',\n",
       " 'sieve',\n",
       " 'sieve',\n",
       " 'sieve',\n",
       " 'bathtub',\n",
       " 'root',\n",
       " 'root',\n",
       " 'dress',\n",
       " 'dress',\n",
       " 'dress',\n",
       " 'saxophone',\n",
       " 'saxophone',\n",
       " 'saxophone',\n",
       " 'chip',\n",
       " 'lettuce',\n",
       " 'lettuce',\n",
       " 'castle',\n",
       " 'castle',\n",
       " 'castle',\n",
       " 'grapefruit',\n",
       " 'grapefruit',\n",
       " 'diamond',\n",
       " 'desk',\n",
       " 'desk',\n",
       " 'desk',\n",
       " 'frigate',\n",
       " 'frigate',\n",
       " 'whale',\n",
       " 'revolver',\n",
       " 'revolver',\n",
       " 'revolver',\n",
       " 'phone',\n",
       " 'phone',\n",
       " 'phone',\n",
       " 'phone',\n",
       " 'phone',\n",
       " 'tobacco',\n",
       " 'aluminum',\n",
       " 'pot',\n",
       " 'restaurant',\n",
       " 'strawberry',\n",
       " 'strawberry',\n",
       " 'ferry',\n",
       " 'ferry',\n",
       " 'groove',\n",
       " 'groove',\n",
       " 'ham',\n",
       " 'jet',\n",
       " 'jet',\n",
       " 'jet',\n",
       " 'helicopter',\n",
       " 'helicopter',\n",
       " 'helicopter',\n",
       " 'toaster',\n",
       " 'toaster',\n",
       " 'toaster',\n",
       " 'toaster',\n",
       " 'blouse',\n",
       " 'blouse',\n",
       " 'blouse',\n",
       " 'tissue',\n",
       " 'cemetery',\n",
       " 'trumpet',\n",
       " 'trumpet',\n",
       " 'trumpet',\n",
       " 'redirect',\n",
       " 'stone',\n",
       " 'stone',\n",
       " 'stone',\n",
       " 'everything',\n",
       " 'cut',\n",
       " 'type',\n",
       " 'chicken',\n",
       " 'sword',\n",
       " 'sword',\n",
       " 'sword',\n",
       " 'onion',\n",
       " 'onion',\n",
       " 'bomber',\n",
       " 'bomber',\n",
       " 'bomber',\n",
       " 'dessert',\n",
       " 'cathedral',\n",
       " 'cathedral',\n",
       " 'scooter',\n",
       " 'scooter',\n",
       " 'scooter',\n",
       " 'saw',\n",
       " 'saw',\n",
       " 'hotel',\n",
       " 'villa',\n",
       " 'villa',\n",
       " 'villa',\n",
       " 'dishwasher',\n",
       " 'dishwasher',\n",
       " 'dishwasher',\n",
       " 'dishwasher',\n",
       " 'alcohol',\n",
       " 'chair',\n",
       " 'chair',\n",
       " 'chair',\n",
       " 'tanker',\n",
       " 'tanker',\n",
       " 'bookcase',\n",
       " 'bookcase',\n",
       " 'bookcase',\n",
       " 'frame',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'corn',\n",
       " 'chocolate',\n",
       " 'combine',\n",
       " 'soap',\n",
       " 'container',\n",
       " 'apple',\n",
       " 'apple',\n",
       " 'dirt',\n",
       " 'rake',\n",
       " 'rake',\n",
       " 'rake',\n",
       " 'waste',\n",
       " 'gun',\n",
       " 'gun',\n",
       " 'gun',\n",
       " 'cauliflower',\n",
       " 'cauliflower',\n",
       " 'cow',\n",
       " 'cello',\n",
       " 'cello',\n",
       " 'cello',\n",
       " 'tuna',\n",
       " 'booby',\n",
       " 'caffeine',\n",
       " 'sweater',\n",
       " 'sweater',\n",
       " 'sweater',\n",
       " 'spear',\n",
       " 'spear',\n",
       " 'spear',\n",
       " 'apricot',\n",
       " 'apricot',\n",
       " 'egg',\n",
       " 'car',\n",
       " 'car',\n",
       " 'car',\n",
       " 'hatchet',\n",
       " 'hatchet',\n",
       " 'hatchet',\n",
       " 'bus',\n",
       " 'bus',\n",
       " 'bus',\n",
       " 'bottle',\n",
       " 'bottle',\n",
       " 'bottle',\n",
       " 'bottle',\n",
       " 'peach',\n",
       " 'peach',\n",
       " 'spoon',\n",
       " 'spoon',\n",
       " 'spoon',\n",
       " 'spoon',\n",
       " 'abuse',\n",
       " 'abuse',\n",
       " 'orange',\n",
       " 'mug',\n",
       " 'mug',\n",
       " 'mug',\n",
       " 'mug',\n",
       " 'mug',\n",
       " 'photographic',\n",
       " 'photographic',\n",
       " 'stapler',\n",
       " 'musket',\n",
       " 'musket',\n",
       " 'musket',\n",
       " 'jaguar',\n",
       " 'salad',\n",
       " 'jacket',\n",
       " 'jacket',\n",
       " 'jacket',\n",
       " 'celery',\n",
       " 'celery',\n",
       " 'steak',\n",
       " 'rice',\n",
       " 'wine',\n",
       " 'pineapple',\n",
       " 'pineapple',\n",
       " 'grape',\n",
       " 'grape',\n",
       " 'oven',\n",
       " 'oven',\n",
       " 'oven',\n",
       " 'oven',\n",
       " 'stereo',\n",
       " 'stereo',\n",
       " 'stereo',\n",
       " 'stereo',\n",
       " 'fridge',\n",
       " 'fridge',\n",
       " 'fridge',\n",
       " 'fridge',\n",
       " 'meerkat',\n",
       " 'corkscrew',\n",
       " 'corkscrew',\n",
       " 'corkscrew',\n",
       " 'robe',\n",
       " 'robe',\n",
       " 'robe',\n",
       " 'flask',\n",
       " 'banana',\n",
       " 'banana',\n",
       " 'nose',\n",
       " 'nose',\n",
       " 'columbine',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'turtle',\n",
       " 'piano',\n",
       " 'piano',\n",
       " 'pear',\n",
       " 'pear',\n",
       " 'flute',\n",
       " 'flute',\n",
       " 'flute',\n",
       " 'box',\n",
       " 'box',\n",
       " 'box',\n",
       " 'wardrobe',\n",
       " 'wardrobe',\n",
       " 'wardrobe',\n",
       " 'paper']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list terms in particular cluster\n",
    "[train_query[i] for i in np.argwhere(sample_clusters == 2).flatten()]\n",
    "\n",
    "\n",
    "# find in which clusters a particular query term ended\n",
    "#word_id = [idx for idx, term in enumerate(train_query) if term=='freedom']\n",
    "#sample_clusters[word_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "\n",
    "def yamane_get_top_hypernym(query, hyper_candidates, clusters, data, top):    \n",
    "    query_index = data.tokenizer.word_index[query]\n",
    "    # remove actual query from candidates    \n",
    "    valid_candidates = filter(lambda x: x[0]!=query_index, hyper_candidates)\n",
    "    hyper_probs = []\n",
    "    for hyper in valid_candidates:                        \n",
    "        candidate_sim = map(lambda x: x.model.predict([[query_index], hyper]).flatten()[0], clusters)\n",
    "        hyper_probs.append(np.max(candidate_sim))\n",
    "    \n",
    "    top_idx = np.argsort(hyper_probs)[::-1][:top]\n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "            \n",
    "    return [(data.tokenizer.index_word[t], hyper_probs[top_idx[i]]) for i, t in enumerate(top_hyper)]\n",
    "\n",
    "#print yamane_get_top_hypernym('budgerigar', hyper_candidates, term_tokenizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artefact',\n",
       " 'artifact',\n",
       " 'craft',\n",
       " 'watercraft',\n",
       " 'vehicle',\n",
       " 'habitation',\n",
       " 'conveyance',\n",
       " 'document',\n",
       " 'commerce',\n",
       " 'object',\n",
       " 'sport',\n",
       " 'normative',\n",
       " 'vessel',\n",
       " 'teff',\n",
       " 'kenaf']"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'boat'\n",
    "lizard_hyper = yamane_get_top_hypernym(word, hyper_candidates, clusters, data, 15)\n",
    "[h for h, p in lizard_hyper]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation logic for Yamane model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set Evaluation Outcome\n",
    "\n",
    "* m = 5; epochs = 30; lambda = 0.15; activation_regularisation = l2_0.001.; Training Accuracy = 0.9207\n",
    "* m = 5; epochs = 15; lambda = 0.15; kernel_regularisation = l2_0.001.; Training Accuracy = 0.1708\n",
    "* m = 5; epochs = 15; lambda = 0.15; kernel_regularisation = NA; Training Accuracy = 0.9506\n",
    "\n",
    "Changed algorithm slightly so that each cluster gets trained on the same number of epochs.<br>\n",
    "This improved training accuracy but had not discernible effect on test performance. Model was subject\n",
    "to higher variance and overfit.\n",
    "\n",
    "* m = 5; epochs = 15; lambda = 0.15; no regularisation in phi; training accuracy = 0.9787920700783771\n",
    "* m = 3; epochs = 30; lambda = 0.15; no regularisation on phi; training accuracy = 0.983402489626556\n",
    "* m = 4; epochs = 10; lambda = 0.2; no phi regularisation; training accuracy = 0.9723374827109267\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_yamane(clusters, query_seq, hyper_seq, labels):\n",
    "        \n",
    "    test_predictions=[]\n",
    "\n",
    "    for idx, (x_term, x_hyper) in enumerate(zip(query_seq, hyper_seq)):\n",
    "        if (idx+1) % 500 == 0:\n",
    "            print \"Done\", idx+1\n",
    "        \n",
    "        hyp_prob_clusters = map(lambda x: x.model.predict((x_term, x_hyper)), clusters)\n",
    "        \n",
    "        cluster_max = np.argmax(hyp_prob_clusters)\n",
    "        test_predictions.append((np.max(hyp_prob_clusters), cluster_max))\n",
    "\n",
    "    # compute accuracy\n",
    "    accuracy = 1. - (np.sum(\n",
    "                        np.abs(\n",
    "                            np.round(np.array(zip(*test_predictions)[0]), 0) - labels\n",
    "                        )\n",
    "                    ) / query_seq.shape[0])\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4338\n",
      "Done 500\n",
      "Done 1000\n",
      "Done 1500\n",
      "Done 2000\n",
      "Done 2500\n",
      "Done 3000\n",
      "Done 3500\n",
      "Done 4000\n",
      "0.9612724757952974\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy on training set itself\n",
    "input_list = [data.train_query, data.train_hyper]\n",
    "query_seq, hyper_seq = map(lambda x: np.array(data.tokenizer.texts_to_sequences(x), dtype='int32'), input_list)\n",
    "print \"Train dataset size:\", query_seq.shape[0]\n",
    "\n",
    "train_accuracy = evaluate_yamane(clusters, query_seq, hyper_seq, [1.]*query_seq.shape[0])\n",
    "print train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 1533\n",
      "Extended test dataset size: 4599\n",
      "Done 500\n",
      "Done 1000\n",
      "Done 1500\n",
      "Done 2000\n",
      "Done 2500\n",
      "Done 3000\n",
      "Done 3500\n",
      "Done 4000\n",
      "Done 4500\n",
      "Test set accuracy: 0.8549684714068275\n"
     ]
    }
   ],
   "source": [
    "# evaluate trained yamane model\n",
    "# we need to do this from first principles since our model is really an ensemble of models\n",
    "negative_tuples = get_negative_tuples((data.train_query + data.test_query, \n",
    "                                       data.train_hyper + data.test_hyper), \n",
    "                                       data, get_negative_words, 2)\n",
    "\n",
    "input_list = [test_query, test_hyper]\n",
    "\n",
    "query_seq, hyper_seq = map(lambda x: np.array(data.tokenizer.texts_to_sequences(x), dtype='int32'), input_list)\n",
    "print \"Test dataset size:\", query_seq.shape[0]\n",
    "    \n",
    "\n",
    "query_seq, hyper_seq, y_labels =\\\n",
    "        extend_batch_with_negatives(query_seq, hyper_seq,\n",
    "                                    negative_tuples,\n",
    "                                    data.tokenizer)  \n",
    "    \n",
    "print \"Extended test dataset size:\", query_seq.shape[0]\n",
    "\n",
    "\n",
    "accuracy = evaluate_yamane(clusters, query_seq, hyper_seq, y_labels)\n",
    "\n",
    "print \"Test set accuracy:\",accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Evaluation Outcome\n",
    "\n",
    "* m = 5; epochs = 30; lambda = 0.15; activation_regularisation = l2_0.001.; Testing Accuracy = 0.5943\n",
    "* m = 5; epochs = 15; lambda = 0.15; kernel_regularisation = l2_0.001.; Testing Accuracy = rubbish\n",
    "* m = 5; epochs = 15; lambda = 0.15; kernel_regularisation = None; Testing Accuracy = 0.7764\n",
    "\n",
    "After change of training algorithm to ensure that every cluster is updated for the same number of times... \n",
    "* m = 5; epochs = 15; lambda = 0.15; no phi regularisation; Testing accuracy = 0.77517\n",
    "* m = 3; epochs = 30; lambda = 0.15; no phi regularisation; Testing accuracy = 0.78381\n",
    "* m = 3; epochs = 20; lambda = 0.15; no phi regularisation; Testing accuracy = 0.78792\n",
    "\n",
    "* m = 1; epochs = 20; lambda = 0.2; no phi regularisation; Testing accuracy = 0.696259\n",
    "* m = 4; epochs = 10; lambda = 0.2; no phi regularisation; Testing accuracy = 0.776408\n",
    "\n",
    "\n",
    "Introduced new negative strategy whereby one of the negative samples is always the most similar word\n",
    "to the given hypernym which is not one of the words hyperyms.\n",
    "\n",
    "Extended dataset size: 4599.  Negatives are synonyms of the query words; clusters learnt = 20\n",
    "* ('m: ', 5, 'lambda: ', 0.15, 'max epoch per cluster: ', 13)\n",
    "    * Training accuracy = 0.96; Testing accuracy = 0.855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate according to MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_hypernyms_to_one_line(data):\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    one_line = {}\n",
    "    for w in ordered_queries:\n",
    "        word_hypernyms = [h for q, h in zip(data.test_query, data.test_hyper) if q == w]\n",
    "        one_line[w] = word_hypernyms\n",
    "    return one_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank(r):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    r = np.asarray(r).nonzero()[0]\n",
    "    return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "def precision_at_k(r, k, n):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return (np.mean(r)*k)/min(k,n)\n",
    "    # Modified from the first version. Now the gold elements are taken into account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_yamane_hypernyms(data, clusters):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    \n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    \n",
    "    results = {}\n",
    "    for idx, word in enumerate(ordered_queries):\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print \"Done\", idx + 1\n",
    "        predicted_hypers = yamane_get_top_hypernym(word, hyper_candidates, clusters, data, 15)\n",
    "        results[word] = list(predicted_hypers[0])\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_get_hypernym(word, model, data, embeddings, top):\n",
    "    q_idx = data.tokenizer.word_index[word]    \n",
    "    \n",
    "    q = embeddings[q_idx]\n",
    "    #q = data.embedding_matrix[q_idx]\n",
    "        \n",
    "    _phi = model.get_layer(name='Phi0').get_weights()[0]\n",
    "    #\n",
    "    #_phi = model.get_layer(name='Phi0').get_weights()[0] +\\\n",
    "    #       model.get_layer(name='Phi1').get_weights()[0] +\\\n",
    "    #       model.get_layer(name='Phi2').get_weights()[0]\n",
    "    \n",
    "    _proj = np.dot(q, _phi)\n",
    "    \n",
    "    sim = cosine_similarity(embeddings, _proj.reshape(1,-1)).flatten()\n",
    "    \n",
    "    return map(lambda i: (data.tokenizer.index_word[i], sim[i]), np.argsort(sim)[::-1][:top])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_crim_hypernyms(data, model):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    results = {}\n",
    "        \n",
    "    embeddings = crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0]\n",
    "    #embeddings = data.embedding_matrix\n",
    "                            \n",
    "    for idx, word in enumerate(ordered_queries):\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print \"Done\", idx + 1\n",
    "        #predicted_hypers = crim_get_top_hypernyms(word, hyper_candidates, model, data, 15)\n",
    "        predicted_hypers = alt_get_hypernym(word, model, data, embeddings, 15)\n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 0.50561774),\n",
       " ('human', 0.4879193),\n",
       " ('beast', 0.46308428),\n",
       " ('vertebrate', 0.46242478),\n",
       " ('animal', 0.4453914),\n",
       " ('chordate', 0.43889248),\n",
       " ('object', 0.41451705),\n",
       " ('food', 0.4079268),\n",
       " ('hobby', 0.4071234),\n",
       " ('game', 0.40058747),\n",
       " ('insect', 0.38279364),\n",
       " ('mammal', 0.38201594),\n",
       " ('arthropod', 0.3783149),\n",
       " ('shape', 0.36002675),\n",
       " ('place', 0.3468836)]"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get current sample's hypernyms\n",
    "#predictions = predict_hypernyms(data, clusters)\n",
    "\n",
    "alt_get_hypernym('cat',crim_model, data, crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('.', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "#pickle.dump(predictions, open(os.path.join(dest, 'yamane_epoch13_cluster18_prediction.pkl'), 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yamane_predictions = pickle.load(open(os.path.join(dest, 'yamane_epoch13_cluster18_prediction.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "def get_evaluation_scores(data, predictions):\n",
    "    all_scores = []    \n",
    "    scores_names = ['MRR', 'P@1', 'P@5', 'P@10']\n",
    "    for query, gold_hyps in convert_hypernyms_to_one_line(data).iteritems():\n",
    "\n",
    "        avg_pat1 = []\n",
    "        avg_pat2 = []\n",
    "        avg_pat3 = []\n",
    "\n",
    "        pred_hyps = predictions[query]\n",
    "        gold_hyps_n = len(gold_hyps)    \n",
    "        r = [0 for i in range(15)]\n",
    "\n",
    "        for j in range(len(pred_hyps)):\n",
    "            if j < gold_hyps_n:\n",
    "                pred_hyp = pred_hyps[j]\n",
    "                if pred_hyp in gold_hyps:\n",
    "                    r[j] = 1\n",
    "\n",
    "        avg_pat1.append(precision_at_k(r,1,gold_hyps_n))\n",
    "        avg_pat2.append(precision_at_k(r,5,gold_hyps_n))\n",
    "        avg_pat3.append(precision_at_k(r,10,gold_hyps_n))    \n",
    "\n",
    "        mrr_score_numb = mean_reciprocal_rank(r)\n",
    "        avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "        avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "        avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "        score_results = [mrr_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "        all_scores.append(score_results)\n",
    "    return scores_names, all_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yamane models\n",
    "\n",
    "Yamane evaluation:<br>\n",
    "MRR: 0.456185185185<br>\n",
    "P@1: 0.4311111111111111<br>\n",
    "P@5: 0.32437037037037036<br>\n",
    "P@10: 0.29980687830687813<br>\n",
    "<hr>\n",
    "\n",
    "#### CRIM models\n",
    "* Simplest model evaluated: epochs: 10; m=1; phi_k = 1\n",
    "\n",
    "CRIM evaluation<br>\n",
    "MRR: 0.408481481481<br>\n",
    "P@1: 0.3711111111111111<br>\n",
    "P@5: 0.24518518518518506<br>\n",
    "P@10: 0.2410123456790123<br>\n",
    "\n",
    "* Model evaluated: epochs: 10; m = 10; phi_k = 10\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.337<br>\n",
    "P@1: 0.2777777777777778<br>\n",
    "P@5: 0.21485185185185185<br>\n",
    "P@10: 0.20493121693121696<br>\n",
    "\n",
    "* ('Epochs: ', 7, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 5, 'train_embeddings: ', False)\n",
    "    * Removed regularisation from final layer\n",
    "    \n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.33891005291<br>\n",
    "P@1: 0.2777777777777778<br>\n",
    "P@5: 0.22551851851851862<br>\n",
    "P@10: 0.2137513227513228<br>\n",
    "\n",
    "* ('Epochs: ', 20, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', False)\n",
    "    * Negative sampling based on hyponym synonyms/random words; no regularisation, random normal init, synonym random negative sampling<br>\n",
    "    \n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.402243386243<br>\n",
    "P@1: 0.3466666666666667<br>\n",
    "P@5: 0.28040740740740716<br>\n",
    "P@10: 0.26355996472663124   <br>\n",
    "\n",
    "* ('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'synonym', 'Phi Init: ', 'random_normal')\n",
    "* More negative samples, have an adverse impact on the results<br>\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.370518518519<br>\n",
    "P@1: 0.31333333333333335<br>\n",
    "P@5: 0.25762962962962943<br>\n",
    "P@10: 0.24290740740740735<br>\n",
    "\n",
    "* ('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 1, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'similar_hyponym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.335593554594<br>\n",
    "P@1: 0.3022222222222222<br>\n",
    "P@5: 0.17833333333333348<br>\n",
    "P@10: 0.16412345679012352<br>\n",
    "\n",
    "* Modified CRIM to be equal to Yamane but with no soft-clustering.  Learned just one projection matrix;\n",
    "* Evaluation results based on accuracy/F1 were quite poor but MRR, P@k scores were highest I managed using the CRIM model;\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.439148148148<br>\n",
    "P@1: 0.3888888888888889<br>\n",
    "P@5: 0.3091851851851849<br>\n",
    "P@10: 0.29187566137566123<br>\n",
    "\n",
    "* Keeping LR kernel weight fixed and increasing the number of matrices, yields projection matrices that are quite similar to each other.  To evaluate, I added all projection matrices together (3 in this case).  I got similar results but not as good.  Adding more projection matrices helped accuracy/F1 when recasting the problem as classication but did not do much with respect to MRR and p@k\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.41137037037<br>\n",
    "P@1: 0.35555555555555557<br>\n",
    "P@5: 0.2919629629629628<br>\n",
    "P@10: 0.27328395061728383<br>\n",
    "\n",
    "* Again kept LR kernel weight fixed to 1 but changed negative sampling strategy to a mix of negative hypernym and synonym. Improved likelihood of getting high-ranked positive hypernyms but quality of subsequent hypernyms degraded as can be attested by the p@5 and p@10 scores.\n",
    "\n",
    "('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'mix_hyper_synonym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.452407407407<br>\n",
    "P@1: 0.4111111111111111<br>\n",
    "P@5: 0.2835185185185183<br>\n",
    "P@10: 0.2627098765432097<br>\n",
    "\n",
    "* Increasing m to 10, actually reduced the scores.\n",
    "\n",
    "('Epochs: ', 15, 'Batch size: ', 32, 'm: ', 10, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'mix_hyper_synonym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.426833333333<br>\n",
    "P@1: 0.3888888888888889<br>\n",
    "P@5: 0.2716296296296294<br>\n",
    "P@10: 0.24958289241622564<br>\n",
    "\n",
    "* Decreasing batch size and increasing epochs more or less returns the same result as when we had a 32 sample batch size and lower epoch count\n",
    "\n",
    "('Epochs: ', 30, 'Batch size: ', 12, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'mix_hyper_synonym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.455185185185<br>\n",
    "P@1: 0.41555555555555557<br>\n",
    "P@5: 0.3007037037037034<br>\n",
    "P@10: 0.27982186948853605<br>\n",
    "\n",
    "* Increasing batch size also degrades performance\n",
    "\n",
    "('Epochs: ', 20, 'Batch size: ', 52, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', False, 'Negative sampling: ', 'mix_hyper_synonym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.437962962963<br>\n",
    "P@1: 0.39111111111111113<br>\n",
    "P@5: 0.2701851851851849<br>\n",
    "P@10: 0.25051234567901226<br>\n",
    "\n",
    "* Tuning embeddings (constrained to unit norm), has an adverse effect on results\n",
    "\n",
    "('Epochs: ', 8, 'Batch size: ', 32, 'm: ', 5, 'pki_k: ', 1, 'train_embeddings: ', True, 'Negative sampling: ', 'synonym', 'Phi Init: ', 'random_normal')\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.341307760141<br>\n",
    "P@1: 0.28888888888888886<br>\n",
    "P@5: 0.22011111111111115<br>\n",
    "P@10: 0.21259435626102294<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yamane evaluation:\n",
      "MRR: 0.456185185185\n",
      "P@1: 0.4311111111111111\n",
      "P@5: 0.32437037037037036\n",
      "P@10: 0.29980687830687813\n",
      "Done 25\n",
      "Done 50\n",
      "Done 75\n",
      "Done 100\n",
      "Done 125\n",
      "Done 150\n",
      "Done 175\n",
      "Done 200\n",
      "Done 225\n",
      "Done 250\n",
      "Done 275\n",
      "Done 300\n",
      "Done 325\n",
      "Done 350\n",
      "Done 375\n",
      "Done 400\n",
      "Done 425\n",
      "Done 450\n",
      "------------------------------\n",
      "CRIM evaluation:\n",
      "MRR: 0.417220458554\n",
      "P@1: 0.37555555555555553\n",
      "P@5: 0.2918148148148147\n",
      "P@10: 0.2761075837742504\n"
     ]
    }
   ],
   "source": [
    "print \"Yamane evaluation:\"\n",
    "score_names, all_scores = get_evaluation_scores(data, yamane_predictions)\n",
    "for k in range(len(scores_names)):\n",
    "    print scores_names[k]+': '+str(sum([score_list[k] for score_list in all_scores]) / len(all_scores))\n",
    "\n",
    "crim_predictions = predict_crim_hypernyms(data, crim_model)\n",
    "\n",
    "print \"-\"*30\n",
    "print \"CRIM evaluation:\"\n",
    "score_names, all_scores = get_evaluation_scores(data, crim_predictions)\n",
    "for k in range(len(scores_names)):\n",
    "    print scores_names[k]+': '+str(sum([score_list[k] for score_list in all_scores]) / len(all_scores))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['undemocratic',\n",
       " 'place',\n",
       " 'bowtie',\n",
       " 'shaped',\n",
       " 'invalid',\n",
       " 'naivety',\n",
       " 'roadrunner',\n",
       " 'change',\n",
       " 'depress',\n",
       " 'enrage',\n",
       " 'everything',\n",
       " 'villa',\n",
       " 'move',\n",
       " 'smooth',\n",
       " 'rendezvous']"
      ]
     },
     "execution_count": 1099,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yamane_predictions['china']\n",
    "#crim_predictions['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse weights learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[1.]], dtype=float32), array([-2.7328842], dtype=float32)]\n",
      "1 [array([[1.]], dtype=float32), array([-2.8369036], dtype=float32)]\n",
      "2 [array([[1.]], dtype=float32), array([-2.006221], dtype=float32)]\n",
      "3 [array([[1.]], dtype=float32), array([-2.5338047], dtype=float32)]\n",
      "4 [array([[1.]], dtype=float32), array([-2.1483057], dtype=float32)]\n",
      "5 [array([[1.]], dtype=float32), array([-2.0767462], dtype=float32)]\n",
      "6 [array([[1.]], dtype=float32), array([-1.7475002], dtype=float32)]\n",
      "7 [array([[1.]], dtype=float32), array([-1.6737198], dtype=float32)]\n",
      "8 [array([[1.]], dtype=float32), array([-1.3753002], dtype=float32)]\n",
      "9 [array([[1.]], dtype=float32), array([-1.7079883], dtype=float32)]\n",
      "10 [array([[1.]], dtype=float32), array([-1.6500005], dtype=float32)]\n",
      "11 [array([[1.]], dtype=float32), array([-1.7033082], dtype=float32)]\n",
      "12 [array([[1.]], dtype=float32), array([-1.6039661], dtype=float32)]\n",
      "13 [array([[1.]], dtype=float32), array([-1.6968616], dtype=float32)]\n",
      "14 [array([[1.]], dtype=float32), array([-1.6306189], dtype=float32)]\n",
      "15 [array([[1.]], dtype=float32), array([-1.6794662], dtype=float32)]\n",
      "16 [array([[1.]], dtype=float32), array([-1.7685634], dtype=float32)]\n",
      "17 [array([[1.]], dtype=float32), array([-1.8209363], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# have a look at the Prediction Layer weights\n",
    "\n",
    "for idx, c in enumerate(clusters):\n",
    "    print idx, c.model.get_layer(name='Prediction').get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clusters[3].model.get_layer(name='Phi').get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_tokenizer.texts_to_sequences(test_query)[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_X_hyper\n",
    "negatives\n",
    "\n",
    "a = [(1,2), (1,3), (1,4)]\n",
    "zip(*a)\n",
    "\n",
    "#for idx, term in enumerate(neg_terms):\n",
    "    #print term, neg_hyper[idx]\n",
    "    \n",
    "term_tokenizer.texts_to_sequences(neg_terms)\n",
    "term_tokenizer.index_word[507], term_tokenizer.index_word[130]\n",
    "\n",
    "\n",
    "\n",
    "for idx, t in enumerate(batch_X_term.flatten()):\n",
    "    print term_tokenizer.index_word[t], \\\n",
    "      hyper_tokenizer.index_word[batch_X_hyper.flatten()[idx]], \\\n",
    "       batch_y_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.tile(np.zeros(1), (2,1))\n",
    "rpt = np.repeat([[word_idx]],3,axis=0)\n",
    "np.vstack(([[1],[2],[3]], rpt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get negative samples\n",
    "# number of negative samples\n",
    "m = 5\n",
    "word_hypernyms = [y for x, y in train if x == 'jacket']\n",
    "print word_hypernyms\n",
    "#possible = [nv for nv in neg_vocab if nv not in word_hypernyms]\n",
    "#np.random.choice(possible, 5).tolist()\n",
    "#synonyms['piranha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_negative_words('sausage', train_query, train_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.dot(np.arange(3),np.arange(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "b = [3,4]\n",
    "\n",
    "a, b = [np.array(x, dtype='int32')for x in [a, b]]\n",
    "print b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 2\n",
      "(300, 300)\n",
      "Similarity between given terms: [[-0.20150737]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('change', 0.49852777),\n",
       " ('make', 0.33062166),\n",
       " ('work', 0.3202718),\n",
       " ('action', 0.28008065),\n",
       " ('place', 0.26855475),\n",
       " ('hornet', 0.26388532),\n",
       " ('cockroach', 0.25421357),\n",
       " ('pheasant', 0.24992587),\n",
       " ('falcon', 0.24652898),\n",
       " ('salmon', 0.23833041),\n",
       " ('cathedral', 0.23767158),\n",
       " ('cod', 0.23764622),\n",
       " ('herring', 0.23626004),\n",
       " ('key', 0.23345503),\n",
       " ('carp', 0.23164552)]"
      ]
     },
     "execution_count": 1098,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "q_idx = data.tokenizer.word_index['china']\n",
    "h_idx = data.tokenizer.word_index['plant']\n",
    "\n",
    "print q_idx, h_idx\n",
    "\n",
    "#print embeddings_index['swordfish'][:20]\n",
    "tuned_embeddings = crim_model.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "q, h = tuned_embeddings[q_idx], tuned_embeddings[h_idx]\n",
    "#q, h = embedding_layer.predict([[q_idx],[h_idx]])\n",
    "#q, h = [x.flatten() for x in [q, h]]\n",
    "\n",
    "_phi = crim_model.get_layer(name='Phi0').get_weights()[0]        \n",
    "\n",
    "print _phi.shape\n",
    "_proj = np.dot(q, _phi)\n",
    "\n",
    "print \"Similarity between given terms:\", cosine_similarity(h.reshape(1,-1), _proj.reshape(1,-1))\n",
    "\n",
    "sim = cosine_similarity(tuned_embeddings, _proj.reshape(1,-1)).flatten()\n",
    "#sim = cosine_similarity(data.embedding_matrix, _proj.reshape(1,-1)).flatten()\n",
    "map(lambda i: (data.tokenizer.index_word[i], sim[i]), np.argsort(sim)[::-1][:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 12\n",
      "(300, 300)\n",
      "Similarity between given terms: [[0.35952055]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('artifact', 0.7524527),\n",
       " ('beast', 0.7518158),\n",
       " ('artefact', 0.73376083),\n",
       " ('predator', 0.7144881),\n",
       " ('creature', 0.6901133),\n",
       " ('shape', 0.66437286),\n",
       " ('raptor', 0.6472047),\n",
       " ('object', 0.6456276),\n",
       " ('veggie', 0.63704467),\n",
       " ('game', 0.63308823),\n",
       " ('canine', 0.61953783),\n",
       " ('insect', 0.616869),\n",
       " ('herbivore', 0.6158552),\n",
       " ('firearm', 0.60992),\n",
       " ('transport', 0.60256714)]"
      ]
     },
     "execution_count": 1034,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "q_idx = data.tokenizer.word_index['lime']\n",
    "h_idx = data.tokenizer.word_index['invertebrate']\n",
    "\n",
    "print q_idx, h_idx\n",
    "\n",
    "#print embeddings_index['swordfish'][:20]\n",
    "        \n",
    "q, h = tuned_embeddings[q_idx], tuned_embeddings[h_idx]\n",
    "q, h = [x.flatten() for x in [q, h]]\n",
    "\n",
    "_phi = crim_model.get_layer(name='Phi0').get_weights()[0]        \n",
    "\n",
    "print _phi.shape\n",
    "_proj = np.dot(q, _phi)\n",
    "\n",
    "print \"Similarity between given terms:\", cosine_similarity(h.reshape(1,-1), _proj.reshape(1,-1))\n",
    "\n",
    "sim = cosine_similarity(tuned_embeddings, _proj.reshape(1,-1)).flatten()\n",
    "map(lambda i: (data.tokenizer.index_word[i], sim[i]), np.argsort(sim)[::-1][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.36265057]], dtype=float32)"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_idx = data.tokenizer.word_index['hart']\n",
    "h_idx = data.tokenizer.word_index['mammal']\n",
    "#embeddings_index['swordfish']\n",
    "print q_idx, h_idx\n",
    "\n",
    "crim_model.predict([[q_idx],[h_idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of tuples where every element follows (word, negative_word)\n",
    "def get_negative_tuples(terms, synonyms, negative_vocab, negative_words_lambda , sample_size):\n",
    "    negative_tuples = []\n",
    "    for word in set(terms[0]):\n",
    "        negatives = negative_words_lambda(word, query_terms, hyper_terms, negative_vocab, sample_size)\n",
    "        negative_tuples.extend(\n",
    "                [(word, n) for n in negatives]\n",
    "        )\n",
    "    return negative_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2  -0.5   0.1   2.  ]\n",
      " [ 1.5   1.3   2.1   0.  ]\n",
      " [ 0.    0.25  0.2  -0.3 ]] (3, 4)\n",
      "[ 56 231  24   2] (4,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-96.8 , 437.9 ,  60.75])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[0.2, -0.5, 0.1, 2.], [1.5, 1.3, 2.1, 0.], [0, 0.25, 0.2, -0.3]])\n",
    "print W, W.shape\n",
    "\n",
    "x = np.array([56, 231, 24, 2])\n",
    "print x, x.shape\n",
    "\n",
    " \n",
    "np.dot(x, W.T) + np.array([1.1, 3.2, -1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('piano', 0.9856831),\n",
       " ('cello', 0.98091674),\n",
       " ('flute', 0.97312176),\n",
       " ('saxophone', 0.9671867),\n",
       " ('sword', 0.9248958),\n",
       " ('bus', 0.91819954),\n",
       " ('bookcase', 0.91581094),\n",
       " ('wardrobe', 0.913466),\n",
       " ('spear', 0.9124167),\n",
       " ('scooter', 0.9069559),\n",
       " ('hatchet', 0.906389),\n",
       " ('stereo', 0.9054192),\n",
       " ('bottle', 0.8994448),\n",
       " ('onion', 0.89725924),\n",
       " ('pineapple', 0.89628285),\n",
       " ('cherry', 0.8947779),\n",
       " ('phone', 0.89401215),\n",
       " ('musket', 0.89235973),\n",
       " ('bass', 0.89191216),\n",
       " ('strawberry', 0.88773155)]"
      ]
     },
     "execution_count": 1029,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to given word\n",
    "word = data.tokenizer.word_index['guitar']\n",
    "candidate_words = filter(lambda w: w != word, data.tokenizer.index_word.keys())\n",
    "tuned_embeddings = crim_model.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "sims = map(lambda c: np.dot(tuned_embeddings[c], tuned_embeddings[word]), candidate_words)\n",
    "\n",
    "most_sim_idx = np.argsort(sims)[::-1][:20]\n",
    "#print most_sim_idx\n",
    "\n",
    "[(data.tokenizer.index_word[candidate_words[idx]], sims[idx]) for idx in most_sim_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bass', 0.7609487),\n",
       " ('piano', 0.71672153),\n",
       " ('saxophone', 0.6717821),\n",
       " ('harmonica', 0.66169775),\n",
       " ('mandolin', 0.6379906),\n",
       " ('banjo', 0.6303143),\n",
       " ('violin', 0.6158097),\n",
       " ('keyboard', 0.59280485),\n",
       " ('trumpet', 0.5794264),\n",
       " ('cello', 0.5722961),\n",
       " ('band', 0.5655428),\n",
       " ('trombone', 0.5609751),\n",
       " ('musician', 0.5495929),\n",
       " ('flute', 0.53228563),\n",
       " ('clarinet', 0.53192836),\n",
       " ('music', 0.5316814),\n",
       " ('ukulele', 0.52557594),\n",
       " ('album', 0.51117814),\n",
       " ('drum', 0.5047761),\n",
       " ('song', 0.5043962)]"
      ]
     },
     "execution_count": 1031,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to given word\n",
    "word = data.tokenizer.word_index['guitar']\n",
    "candidate_words = filter(lambda w: w != word, data.tokenizer.index_word.keys())\n",
    "sims = map(lambda c: np.dot(data.embedding_matrix[c], data.embedding_matrix[word]), candidate_words)\n",
    "\n",
    "most_sim_idx = np.argsort(sims)[::-1][:20]\n",
    "#print most_sim_idx\n",
    "\n",
    "[(data.tokenizer.index_word[candidate_words[idx]], sims[idx]) for idx in most_sim_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processor\t: 0\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 78\r\n",
      "model name\t: Intel(R) Core(TM) i7-6500U CPU @ 2.50GHz\r\n",
      "stepping\t: 3\r\n",
      "cpu MHz\t\t: 2591.996\r\n",
      "cache size\t: 4096 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 2\r\n",
      "core id\t\t: 0\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 0\r\n",
      "initial apicid\t: 0\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 22\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch pti avx2 rdseed clflushopt\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\r\n",
      "bogomips\t: 5183.99\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n",
      "processor\t: 1\r\n",
      "vendor_id\t: GenuineIntel\r\n",
      "cpu family\t: 6\r\n",
      "model\t\t: 78\r\n",
      "model name\t: Intel(R) Core(TM) i7-6500U CPU @ 2.50GHz\r\n",
      "stepping\t: 3\r\n",
      "cpu MHz\t\t: 2591.996\r\n",
      "cache size\t: 4096 KB\r\n",
      "physical id\t: 0\r\n",
      "siblings\t: 2\r\n",
      "core id\t\t: 1\r\n",
      "cpu cores\t: 2\r\n",
      "apicid\t\t: 1\r\n",
      "initial apicid\t: 1\r\n",
      "fpu\t\t: yes\r\n",
      "fpu_exception\t: yes\r\n",
      "cpuid level\t: 22\r\n",
      "wp\t\t: yes\r\n",
      "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx rdrand hypervisor lahf_lm abm 3dnowprefetch pti avx2 rdseed clflushopt\r\n",
      "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\r\n",
      "bogomips\t: 5183.99\r\n",
      "clflush size\t: 64\r\n",
      "cache_alignment\t: 64\r\n",
      "address sizes\t: 39 bits physical, 48 bits virtual\r\n",
      "power management:\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
