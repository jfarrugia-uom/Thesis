{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc, Token\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(u'The tired traveller roamed the sandy desert, seeking food.')\n",
    "#doc = nlp(u'Procter and Gamble is looking at buying UK startup for â‚¬ 1 billion')\n",
    "for token in doc:\n",
    "    print token.text, token.lemma_, token.pos_, token.dep_, token.shape_, token.is_alpha, token.is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get an explanation of a POS or dependency  tag\n",
    "spacy.explain(\"dobj\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display dependency parse tree\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ne in doc.ents:\n",
    "    print ne.text, ne.start_char, ne.end_char, ne.label_\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# play with word vectors\n",
    "tokens = nlp(u'dog terrier banana apple')\n",
    "for t in tokens:\n",
    "    print t.text, t.has_vector, t.vector_norm, t.is_oov\n",
    "\n",
    "print \n",
    "for t1 in tokens:\n",
    "    for t2 in tokens:\n",
    "        if (t1.text == t2.text):\n",
    "            continue\n",
    "        print t1.text, t2.text, t1.similarity(t2), t2.vector.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalise to unit vector\n",
    "test_norm = tokens[1].vector / tokens[1].vector_norm\n",
    "# l2 norm is now equal to 1\n",
    "print np.sum(test_norm ** 2) ** 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets use some corpora that ship with NLTK\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a sentence arbitrarily\n",
    "s = gutenberg.sents('austen-sense.txt')[1201]\n",
    "\n",
    "#gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "# let's try load the entire raw corpus into spaCy doc\n",
    "#doc = nlp(gutenberg.raw('austen-sense.txt'))\n",
    "\n",
    "#len(gutenberg.sents('austen-sense.txt')[:1000])\n",
    "sense100 = gutenberg.sents('austen-sense.txt')[:100]\n",
    "# let's join the first 1000 sentences of Sense & Sensibility in one large sentence\n",
    "sense = \" \".join([\" \".join(_s) for _s in sense100])\n",
    "\n",
    "#corpus = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print gutenberg.raw('austen-sense.txt')[:200]\n",
    "len(gutenberg.raw('austen-sense.txt')[:200])\n",
    "\n",
    "#\"lots of newlines\\nlots of newlines\\n\\n\".find(\"\\n\")\n",
    "#\"lots of newlines\\nlots of newlines\\n\\n\"[:16]\n",
    "# generator that finds new-line indices in raw corpus\n",
    "# double new-lines will be used as paragraph delimiters\n",
    "def GenTest(raw):\n",
    "    _max = len(raw)\n",
    "    n = 0;\n",
    "    while n+1 < _max:\n",
    "        if (raw[n] == \"\\n\" and raw[n+1] == \"\\n\"):\n",
    "            yield n\n",
    "        n += 1\n",
    "\n",
    "newlines = [i for i in GenTest(gutenberg.raw('austen-sense.txt'))]\n",
    "newlines = np.array(newlines)\n",
    "# length of Sense = 673022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# break a corpus into several sub-corpora\n",
    "\n",
    "interval = 50000\n",
    "corpus = []\n",
    "_max = 673022\n",
    "idx = 0\n",
    "while idx < _max:\n",
    "    prev_idx = idx\n",
    "    idx += interval    \n",
    "    if (newlines[newlines >= idx].size):\n",
    "        idx = np.min(newlines[newlines >= idx])        \n",
    "    else:\n",
    "        idx = _max\n",
    "        \n",
    "    corpus.append(gutenberg.raw('austen-sense.txt')[prev_idx:idx].replace(\"\\n\", \" \"))\n",
    "    print prev_idx, idx\n",
    "    \n",
    "#nlp(gutenberg.raw('austen-sense.txt')[0:50887].replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse the first sub-corpus\n",
    "#doc = nlp(corpus[0])\n",
    "doc = nlp(corpus[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list(doc.sents)[60]\n",
    "for chunk in doc.noun_chunks:\n",
    "    print chunk.text, chunk.label_, chunk.lemma_, chunk.root\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find root of sentences\n",
    "for i in nlp(u\"The spacecraft was safely landed by the pilot on the surface of the new planet. The dog caught the frisbee in the park.\").sents:    \n",
    "    print i.root.text, i.root.tag_, i.root.lemma_, i.root.dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print token.text, token.tag_, token.dep_\n",
    "\n",
    "print \"---------------------\"\n",
    "# get noun chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    print chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [[\"hello\", \"world\", \".\"], [\"good-bye\", \"universe\", \".\"]]\n",
    "\" \".join([\" \".join(_s) for _s in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp.tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "#doc2 = nlp(u\"The tired travelers roamed the sandy desert, seeking food.\")\n",
    "doc2 = nlp(u'This is a ship-shipping ship, shipping shipping ships.')\n",
    "for chunk in doc2.noun_chunks:\n",
    "    print \"%s | %s | %s | %s\" \\\n",
    "    %(chunk.text, chunk.root.lemma_, chunk.root.dep_, chunk.root.head.lemma_)\n",
    "\n",
    "print \"-\"*20\n",
    "for token in doc2:\n",
    "    print \"%d; %s/%s \\(%s\\) <-- %s --%s/%s\" % (token.i, token.lemma_, token.tag_, token.pos_, token.dep_, token.head.lemma_, token.head.tag_)\n",
    "\n",
    "print [word for word in doc2[3].children if word.pos_ != 'PUNCT']\n",
    "print [word for word in doc2[3].lefts if word.pos_ != 'PUNCT']\n",
    "print [word for word in doc2[3].rights if word.pos_ != 'PUNCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment with custom tokeniser, ideal to use on pre-tokenised text\n",
    "class WhitespaceTokenizer(object):\n",
    "    def __init__(self, vocab, ):\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        words = text.split(' ')\n",
    "        spaces = [True] * len(words)\n",
    "        return Doc(self.vocab, words=words, spaces = spaces)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use Whitespace tokenizer if my text has already been tokenized\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "\n",
    "doc3 = nlp(\" \".join(word_list))\n",
    "for token in doc3:\n",
    "    print token.text, token.lemma_, token.pos_, token.head, token.dep_\n",
    "\n",
    "#print \"-\"*30\n",
    "#for chunk in doc3.noun_chunks:\n",
    "#    print chunk.text, chunk.label_, chunk.lemma_, chunk.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Al al PROPN Pacino compound\n",
      "1 3 Pacino pacino PROPN starred nsubj\n",
      "2 10 and and CCONJ Pacino cc\n",
      "3 14 Robert robert PROPN Niro compound\n",
      "4 21 De de PROPN Niro compound\n",
      "5 24 Niro niro PROPN Pacino conj\n",
      "6 29 starred star VERB starred ROOT\n",
      "7 37 together together ADV starred advmod\n",
      "8 46 in in ADP starred prep\n",
      "9 49 The the DET II det\n",
      "10 53 Godfather godfather PROPN II compound\n",
      "11 63 Part part PROPN II compound\n",
      "12 68 II ii PROPN in pobj\n",
      "13 70 . . PUNCT starred punct\n",
      "14 72 De de PROPN Niro compound\n",
      "15 75 Niro niro PROPN disagreed nsubj\n",
      "16 80 and and CCONJ Niro cc\n",
      "17 84 executive executive ADJ president amod\n",
      "18 94 vice vice NOUN president compound\n",
      "19 99 president president NOUN Evans compound\n",
      "20 109 Robert robert PROPN Evans compound\n",
      "21 116 Evans evans PROPN Niro conj\n",
      "22 122 disagreed disagree VERB disagreed ROOT\n",
      "23 132 on on ADP disagreed prep\n",
      "24 135 aspects aspect NOUN on pobj\n",
      "25 143 of of ADP aspects prep\n",
      "26 146 the the DET film det\n",
      "27 150 film film NOUN of pobj\n",
      "28 154 . . PUNCT disagreed punct\n",
      "29 156 Disagreeing disagree VERB is csubj\n",
      "30 168 is be VERB is ROOT\n",
      "31 171 one one NUM thing nummod\n",
      "32 175 thing thing NOUN is attr\n",
      "33 181 but but CCONJ is cc\n",
      "34 185 apples apple NOUN are nsubj\n",
      "35 192 are be VERB is conj\n",
      "36 196 another another DET are attr\n",
      "37 203 . . PUNCT are punct\n",
      "Al Pacino and Robert De Niro starred together in The Godfather Part II . De Niro and executive vice president Robert Evans disagreed on aspects of the film . Disagreeing is one thing but apples are another .\n",
      "--------------------------------------------------\n",
      "\n",
      "Printing entities\n",
      "Al Pacino PERSON 0 2 al pacino\n",
      "Robert De Niro PERSON 3 6 robert de niro\n",
      "The Godfather GPE 9 11 the godfather\n",
      "De Niro PERSON 14 16 de niro\n",
      "Robert Evans PERSON 20 22 robert evans\n",
      "one CARDINAL 31 32 one\n",
      "--------------------------------------------------\n",
      "\n",
      "Printing noun chunks\n",
      "Al Pacino NP 0 2\n",
      "Robert De Niro NP 3 6\n",
      "The Godfather Part II NP 9 13\n",
      "De Niro NP 14 16\n",
      "executive vice president Robert Evans NP 17 22\n",
      "aspects NP 24 25\n",
      "the film NP 26 28\n",
      "one thing NP 31 33\n",
      "apples NP 34 35\n"
     ]
    }
   ],
   "source": [
    "nlp.tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "\n",
    "input_str = u\"Al Pacino and Robert De Niro starred together in The Godfather Part II. \" +\\\n",
    "             \"De Niro and executive vice president Robert Evans disagreed on aspects of the film. \"+\\\n",
    "             \"Disagreeing is one thing but apples are another.\"\n",
    "\n",
    "        \n",
    "#input_str = u\"Lynching him was not a good idea.  A lynching is always exciting.  \"+\\\n",
    " #            \"Vice president Silvio Berlusconi quipped that tendentiousness is a Concept.\"\n",
    "doc3 = nlp(input_str)\n",
    "\n",
    "\n",
    "for token in doc3:\n",
    "    print token.i, token.idx, token.text, token.lemma_, token.pos_, token.head, token.dep_\n",
    "\n",
    "# create word_list to mock pre-tockenized strings\n",
    "word_list = [token.text for token in doc3]\n",
    "print \" \".join(word_list)\n",
    "\n",
    "# let's create n-grams from the string tokens\n",
    "from nltk.util import ngrams\n",
    "trigrams = ngrams([token for token in doc3 if token.pos_ != 'PUNCT'], 3)\n",
    "#print list(trigrams)\n",
    "\n",
    "print \"-\"*50\n",
    "#for ne in doc3.ents:\n",
    "#    print ne.text, ne.start_char, ne.end_char, ne.label_\n",
    "print u\"\\nPrinting entities\"\n",
    "\n",
    "for ents in doc3.ents:    \n",
    "    print ents.text, ents.label_, ents.start, ents.end, ents.lower_\n",
    "    \n",
    "print \"-\"*50\n",
    "print u\"\\nPrinting noun chunks\"\n",
    "\n",
    "for ents in doc3.noun_chunks:    \n",
    "    print ents.text, ents.label_, ents.start, ents.end\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept3 [executive vice president]\n",
      "concept2 [vice president, Vice president]\n",
      "entity2 [De Niro, Adolf Hitler]\n",
      "entity3 [Robert De Niro, John Fitzgerald Kennedy]\n"
     ]
    }
   ],
   "source": [
    "entity_spans = []\n",
    "concept_spans = []\n",
    "spans = []\n",
    "\n",
    "def on_match(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]    \n",
    "    span = doc[start:end]    \n",
    "    if nlp.vocab.strings[match_id].startswith(\"entity\"):\n",
    "        entity_spans.append(span)\n",
    "    else:\n",
    "        concept_spans.append(span)\n",
    "    #spans.append(span)    \n",
    "\n",
    "\n",
    "def create_patterns(**patterns):\n",
    "    matcher2 = PhraseMatcher(nlp.vocab)    \n",
    "    matcher3 = PhraseMatcher(nlp.vocab)    \n",
    "    for p in patterns:                \n",
    "        nlp_patterns = [nlp(text) for text in patterns[p]]\n",
    "        print p, nlp_patterns\n",
    "        assert p.endswith(\"2\") or p.endswith(\"3\")        \n",
    "        if p.endswith(\"2\"):\n",
    "            matcher2.add(p, on_match, *nlp_patterns)\n",
    "        else:\n",
    "            matcher3.add(p, on_match, *nlp_patterns)  \n",
    "    \n",
    "    return matcher3, matcher2\n",
    "\n",
    "#Token.remove_extension('is_input_ent')\n",
    "if not (Token.has_extension('is_input_ent')):\n",
    "    Token.set_extension('is_input_ent', default=False)\n",
    "if not (Token.has_extension('is_input_concept')):\n",
    "    Token.set_extension('is_input_concept', default=False)\n",
    "\n",
    "doc3 = nlp(input_str)\n",
    "\n",
    "#for t in doc3:\n",
    " #   print t.text, t.ent_iob_\n",
    "\n",
    "# we can use a phrase matcher and create larger terminology lists\n",
    "# also we will implement a call-back event listener to take an action\n",
    "# when a match is found\n",
    "\n",
    "p1 = [u'Robert De Niro', u\"John Fitzgerald Kennedy\"]\n",
    "p2 = [u'De Niro', u'Adolf Hitler']\n",
    "p3 = [u'executive vice president']\n",
    "p4 = [u'vice president', u'Vice president']\n",
    "\n",
    "matchers = create_patterns(entity3=p1, entity2=p2, concept3=p3, concept2=p4)\n",
    "matchers = list(matchers)\n",
    "#nlp_patterns = [nlp(text) for text in p1]\n",
    "#matcher = PhraseMatcher(nlp.vocab)\n",
    "#matcher.add(\"entity2\", on_match, *nlp_patterns)\n",
    "#match = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:\n",
      "Concepts:\n",
      "--------------------------------------------------\n",
      "\n",
      "Entities:\n",
      "Concepts:\n",
      "Vice president\n",
      "--------------------------------------------------\n",
      "\n",
      "Silvio Berlusconi 17 19 PERSON\n"
     ]
    }
   ],
   "source": [
    "for m in matchers:\n",
    "    matches = m(doc3)\n",
    "    print \"Entities:\"\n",
    "    for s in entity_spans:\n",
    "        print s\n",
    "        s.merge()\n",
    "        for t in s:\n",
    "            t._.is_input_ent = True\n",
    "        \n",
    "    \n",
    "    print \"Concepts:\"    \n",
    "    for s in concept_spans:\n",
    "        print s \n",
    "        s.merge()\n",
    "        for t in s:\n",
    "            t._.is_input_concept = True\n",
    "        \n",
    "    concept_spans = []\n",
    "    entity_spans = []\n",
    "    print \"-\"*50+\"\\n\"\n",
    "\n",
    "for ne in doc3.ents:\n",
    "    print ne.text, ne.start , ne.end, ne.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_token(t, doc):\n",
    "    state = dict(count=0)\n",
    "    def _process_token(t):\n",
    "        string = None\n",
    "        if t.ent_iob_ == 'B': # if start of named entity                            \n",
    "            string = doc.ents[state['count']].text.replace(\" \", \"_\")\n",
    "            state['count'] += 1            \n",
    "        # if inside named entity or punctuation or space, ignore token\n",
    "        elif t.ent_iob_ == 'I' or t.pos_ in ['PUNCT', 'SPACE']:\n",
    "            pass\n",
    "        # if entity: retain case, create single token\n",
    "        elif t._.is_input_ent: \n",
    "            string = t.text.replace(\" \",\"_\") \n",
    "        # if concept: lower case, create single token\n",
    "        elif t._.is_input_concept:\n",
    "            string = t.text.lower().replace(\" \",\"_\") \n",
    "        # if uni-gram, grab lemma\n",
    "        else:\n",
    "            string = t.lemma_\n",
    "        \n",
    "        return string\n",
    "    return _process_token\n",
    "   \n",
    "#filter(None, map(process_token, [1,2,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'lynch', u'-PRON-', u'be', u'not', u'a', u'good', u'idea'], [u'a', u'lynching', u'be', u'always', u'exciting'], [u'vice_president', u'Silvio_Berlusconi', u'quip', u'that', u'tendentiousness', u'be', u'a', u'concept']]\n"
     ]
    }
   ],
   "source": [
    "pt = process_token(None, doc3)\n",
    "string_list = []\n",
    "for sent in doc3.sents:\n",
    "    string_list.append(filter(None, map(pt, sent)))\n",
    "\n",
    "print string_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'lynch', u'-PRON-', u'be', u'not', u'a', u'good', u'idea'], [u'a', u'lynching', u'be', u'always', u'exciting'], [u'vice_president', u'Silvio_Berlusconi', u'quip', u'that', u'tendentiousness', u'be', u'a', u'concept']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "string_list = []\n",
    "entity_counter = 0\n",
    "for sent in doc3.sents:        \n",
    "    string = []\n",
    "    for t in sent:\n",
    "        if t.ent_iob_ == 'B': # if start of named entity                \n",
    "            string.append(doc3.ents[entity_counter].text.replace(\" \", \"_\"))\n",
    "            entity_counter += 1\n",
    "            continue\n",
    "\n",
    "        if t.ent_iob_ == 'I' or t.pos_ in ['PUNCT', 'SPACE']:\n",
    "            continue\n",
    "\n",
    "        if t._.is_input_ent: \n",
    "            string.append(t.text.replace(\" \",\"_\") )\n",
    "        elif t._.is_input_concept:\n",
    "            string.append(t.text.lower().replace(\" \",\"_\") )\n",
    "        else:\n",
    "            string.append(t.lemma_)\n",
    "            \n",
    "    string_list.append(string)\n",
    "    #print t.i, t.idx, t.text.replace(\" \",\"_\"),t.pos_, t.dep_\n",
    "    #print t.i, t.idx, t.text, t.pos_, t.dep_\n",
    "    \n",
    "    #string_list.append()\n",
    "print string_list\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{'LOWER': 'hello'}, {'IS_PUNCT': True}, {'LOWER': 'world'}]\n",
    "matcher.add('HelloWorld', None, pattern)\n",
    "\n",
    "doc = nlp(u'Hello world! Hello; world!')\n",
    "for tok in doc:\n",
    "    print tok.text, tok.pos_\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]  # the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
