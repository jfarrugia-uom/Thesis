{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HypernymEvaluation:\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, feature_extractor, scorer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.scorer = scorer\n",
    "                \n",
    "\n",
    "    def convert_hypernyms_to_one_line(self):\n",
    "        ordered_queries = sorted(list(set(self.dataset[0])))\n",
    "        one_line = {}\n",
    "        for w in ordered_queries:\n",
    "            word_hypernyms = [h for q, h in zip(*self.dataset) if q == w]\n",
    "            one_line[w] = word_hypernyms\n",
    "        return one_line\n",
    "\n",
    "    # taken from task_scorer.py provided with shared task resources\n",
    "    def mean_reciprocal_rank(self, r):\n",
    "        \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "        First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "        Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean reciprocal rank\n",
    "        \"\"\"\n",
    "        r = np.asarray(r).nonzero()[0]\n",
    "        return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "    def precision_at_k(self, r, k, n):\n",
    "        \"\"\"Score is precision @ k\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Precision @ k\n",
    "        Raises:\n",
    "            ValueError: len(r) must be >= k\n",
    "        \"\"\"\n",
    "        assert k >= 1\n",
    "        r = np.asarray(r)[:k] != 0\n",
    "        if r.size != k:\n",
    "            raise ValueError('Relevance score length < k')\n",
    "        return (np.mean(r)*k)/min(k,n)\n",
    "        # Modified from the first version. Now the gold elements are taken into account\n",
    "\n",
    "    def average_precision(self, r,n):\n",
    "        \"\"\"Score is average precision (area under PR curve)\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Average precision\n",
    "        \"\"\"\n",
    "        r = np.asarray(r) != 0\n",
    "        out = [self.precision_at_k(r, k + 1, n) for k in range(r.size)]\n",
    "        #Modified from the first version (removed \"if r[k]\"). All elements (zero and nonzero) are taken into account\n",
    "        if not out:\n",
    "            return 0.\n",
    "        return np.mean(out)\n",
    "\n",
    "    def mean_average_precision(self, r, n):\n",
    "        \"\"\"Score is mean average precision\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean average precision\n",
    "        \"\"\"\n",
    "        return self.average_precision(r,n)\n",
    "\n",
    "    # predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "    def get_evaluation_scores(self, predictions):\n",
    "        all_scores = []    \n",
    "        scores_names = ['MRR', 'MAP', 'P@1', 'P@5', 'P@10']\n",
    "        for query, gold_hyps in self.convert_hypernyms_to_one_line().items():\n",
    "\n",
    "            avg_pat1 = []\n",
    "            avg_pat2 = []\n",
    "            avg_pat3 = []\n",
    "\n",
    "            pred_hyps = predictions[query]\n",
    "            gold_hyps_n = len(gold_hyps)    \n",
    "            r = [0 for i in range(15)]\n",
    "\n",
    "            for j in range(len(pred_hyps)):\n",
    "                # I believe it's not fair to bias evaluation on how many hypernyms were found in gold set\n",
    "                # if anything a shorter list (ex. because a hypernym is very particular) will already make \n",
    "                # it harder for a match to be found but if system returns correct hypernym in second place\n",
    "                # why should it be ignored?\n",
    "                #if j < gold_hyps_n:\n",
    "                pred_hyp = pred_hyps[j]\n",
    "                if pred_hyp in gold_hyps:\n",
    "                    r[j] = 1\n",
    "\n",
    "            avg_pat1.append(self.precision_at_k(r,1,gold_hyps_n))\n",
    "            avg_pat2.append(self.precision_at_k(r,5,gold_hyps_n))\n",
    "            avg_pat3.append(self.precision_at_k(r,10,gold_hyps_n))    \n",
    "\n",
    "            mrr_score_numb = self.mean_reciprocal_rank(r)\n",
    "            map_score_numb = self.mean_average_precision(r,gold_hyps_n)\n",
    "            avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "            avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "            avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "            score_results = [mrr_score_numb, map_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "            all_scores.append(score_results)\n",
    "        return scores_names, all_scores\n",
    "\n",
    "    # return predictions for user-defined list of terms\n",
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings_q = self.feature_extractor.get_layer(name='TermEmbedding_Q').get_weights()[0]\n",
    "        embeddings_h = self.feature_extractor.get_layer(name='TermEmbedding_H').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings_q[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "            \n",
    "            # normalise phi projection\n",
    "            #word_phi /= np.linalg.norm(word_phi)\n",
    "                        \n",
    "            hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n",
    "    \n",
    "    # return predictions for all terms initially passed to class\n",
    "    def predict_ltr_hypernyms(self):\n",
    "        return self.predict_ltr_hypernym(self.dataset[0])\n",
    "\n",
    "\n",
    "class HypernymEvaluation_SquareDiff(HypernymEvaluation):\n",
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        #embeddings_q = self.feature_extractor.get_layer(name='TermEmbedding_Q').get_weights()[0]\n",
    "        #embeddings_h = self.feature_extractor.get_layer(name='TermEmbedding_H').get_weights()[0]\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "                        \n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = self.scorer([(embeddings[1:] - word_phi)**2])            \n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results    \n",
    "    \n",
    "class HypernymEvaluation_MSE(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "        \n",
    "    \n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "            \n",
    "            # normalise phi projection\n",
    "            word_phi /= np.linalg.norm(word_phi)\n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = np.dot(embeddings[1:], word_phi)\n",
    "            #hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores.flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n",
    "    \n",
    "class HypernymEvaluation_CRIM(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "\n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "        \n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "        \n",
    "        cluster_weight = self.feature_extractor.get_layer(name='Prediction').get_weights()[0]\n",
    "        bias = self.feature_extractor.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)                                                \n",
    "    \n",
    "            sim_matrix = np.dot(cluster_weight.T, np.dot(embeddings[1:], word_phi.T).T) + bias\n",
    "            top_words = np.argsort(sim_matrix[0])[::-1][:15] + 1\n",
    "                                                \n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results        \n",
    "    \n",
    "class HypernymEvaluation_CRIM_Max(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "\n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "        \n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "        \n",
    "        cluster_weight = self.feature_extractor.get_layer(name='Prediction').get_weights()[0]\n",
    "        bias = self.feature_extractor.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)\n",
    "            \n",
    "            word_phi /= np.linalg.norm(word_phi, axis=1).reshape(-1,1)\n",
    "    \n",
    "            sim_matrix = np.dot(embeddings[1:], word_phi.T)    \n",
    "            max_sim = np.mean(sim_matrix, 1).reshape(1,-1)\n",
    "            sim_matrix = np.dot(cluster_weight.T, max_sim) + bias        \n",
    "            \n",
    "            top_words = np.argsort(sim_matrix[0])[::-1][:15] + 1\n",
    "                                                \n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)\n",
    "            # normalise phi projection\n",
    "            #word_phi /= np.linalg.norm(word_phi)\n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = self.scorer([(embeddings[1:] - word_phi)**2])\n",
    "            #hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test final MRR score\n",
    "\n",
    "get_score = K.function([s_vi], [rel_score])\n",
    "he = HypernymEvaluation((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "predictions = he.predict_ltr_hypernyms()\n",
    "#predictions = mrr_logger.predictions\n",
    "_, all_scores = he.get_evaluation_scores(predictions)\n",
    "mrr = round(sum([score_list[0] for score_list in all_scores]) / len(all_scores), 5)                                \n",
    "print mrr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he = HypernymEvaluation_MSE((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "he.predict_ltr_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_synyonyms(hyponyms, hypernyms, n=15):\n",
    "    synonyms = {}\n",
    "    \n",
    "    # prepare hypernym lookup dictionary\n",
    "    hyper_lookup = defaultdict(list)\n",
    "    for q, h in zip(hyponyms, hypernyms):\n",
    "        hyper_lookup[q].append(h)\n",
    "                \n",
    "    for term in set(hyponyms):        \n",
    "        synonyms[term] = list(filter(lambda x: x not in hyper_lookup[x], zip(*model.most_similar(term, topn=20))[0]))[:n]\n",
    "        \n",
    "    return synonyms\n",
    "    \n",
    "#get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)    \n",
    "#get_synyonyms(valid_query, valid_hyper)    \n",
    "\n",
    "def get_random(hyponyms, hypernyms, vocab, n = 15):\n",
    "    \n",
    "    random_words = {}\n",
    "    \n",
    "    # prepare hypernym lookup dictionary\n",
    "    hyper_lookup = defaultdict(list)\n",
    "    for q, h in zip(hyponyms, hypernyms):\n",
    "        hyper_lookup[q].append(h)\n",
    "            \n",
    "    for term in set(hyponyms):                \n",
    "        some_words = np.random.choice(vocab, 20, replace=False)        \n",
    "        random_words[term] = list(filter(lambda x: x not in hyper_lookup[x], some_words))[:n]\n",
    "            \n",
    "    return random_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, \n",
    "                 train_query, train_hyper, \n",
    "                 test_query, test_hyper, \n",
    "                 valid_query, valid_hyper, \n",
    "                 vocab, embeddings):\n",
    "        \n",
    "                \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.valid_query = valid_query\n",
    "        self.valid_hyper = valid_hyper\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        #self.synonyms = synonyms\n",
    "                \n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['animal'].shape[0]\n",
    "        \n",
    "        print (\"Tokenising words...\")\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = tokenizer = Tokenizer(num_words = 300000, filters='')\n",
    "        self.tokenizer.fit_on_texts(train_query + test_query + valid_query + vocab)\n",
    "        \n",
    "        print (\"Creating embedding matrix...\")\n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings[word]\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector (already normalised)\n",
    "                    #embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)\n",
    "        \n",
    "        print (\"Creating random words/synonyms...\")\n",
    "        self.random_words = get_random(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper, vocab)  \n",
    "        self.synonyms = get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = Data(train_query, train_hyper, test_query, test_hyper, valid_query, valid_hyper, vocab, model)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('.', 'pickle')\n",
    "#pickle.dump(data, open(os.path.join(dest, 'semeval_data.pkl'), 'wb'), protocol=2)\n",
    "data = pickle.load(open(os.path.join(dest, 'semeval_data.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for fitting RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#term_count = Counter(data.train_query)\n",
    "\n",
    "query = []\n",
    "hyper = []\n",
    "negat = []\n",
    "n_negative = 1\n",
    "for q, h in zip(data.train_query, data.train_hyper):        \n",
    "    # mix of random and synonyms\n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()\n",
    "    \n",
    "    # append query word to negatives    \n",
    "    #rands.append(q)\n",
    "    \n",
    "    query.extend([q] * n_negative)\n",
    "    hyper.extend([h] * n_negative)\n",
    "    negat.extend(rands)\n",
    "                                         \n",
    "\n",
    "query_seq, hyper_seq, neg_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), \n",
    "                                    [query, hyper, negat])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data for fitting RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_query = []\n",
    "v_hyper = []\n",
    "v_negat = []\n",
    "\n",
    "n_negative = 1\n",
    "for q, h in zip(data.valid_query, data.valid_hyper):        \n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()\n",
    "    #rands.append(q)\n",
    "    \n",
    "    v_query.extend([q] * n_negative)\n",
    "    v_hyper.extend([h] * n_negative)\n",
    "    v_negat.extend(rands)\n",
    "                                 \n",
    "\n",
    "v_query_seq, v_hyper_seq, v_neg_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), \n",
    "                                    [v_query, v_hyper, v_negat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(query[:10], hyper[:10], negat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print query_seq[0], hyper_seq[0], neg_seq[0]\n",
    "print map(lambda x: len(x), [query_seq, hyper_seq, neg_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras imports\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding,  Flatten,  Dropout, Subtract, Activation, Lambda, concatenate, Dot\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm, MinMaxNorm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.keras.initializers import Initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor/Pair-wise LTR Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomIdentity(Initializer):\n",
    "    def __init__(self, dtype=dtypes.float32):\n",
    "        self.dtype = dtypes.as_dtype(dtype)\n",
    "\n",
    "    \n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.dtype\n",
    "        \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)        \n",
    "        #identity = K.eye(shape[-1], dtype='float32')        \n",
    "        rident = tf.eye(shape[-1]) * rnorm\n",
    "        return rident\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"dtype\": self.dtype.name}\n",
    "\n",
    "    \n",
    "class RandomPlusIdentity(Initializer):\n",
    "    def __init__(self, dtype=dtypes.float32):\n",
    "        self.dtype = dtypes.as_dtype(dtype)\n",
    "\n",
    "    \n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.dtype\n",
    "        \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)    \n",
    "        rident = tf.eye(shape[-1]) + rnorm\n",
    "        return rident            \n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"dtype\": self.dtype.name}\n",
    "        \n",
    "\n",
    "get_custom_objects().update({'RandomIdentity': RandomIdentity})\n",
    "get_custom_objects().update({'RandomPlusIdentity': RandomPlusIdentity})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_RankNet_model(feature_extractor, dropout_rate):\n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "    s_vi, s_vj = feature_extractor([hypo_input, hyper_input, negative_input])\n",
    "\n",
    "    # now we can pipe our extracted features into a RankNet model\n",
    "    h_1 = Dense(128, activation = \"relu\")\n",
    "    h_2 = Dense(64, activation = \"relu\")\n",
    "    h_3 = Dense(32, activation = \"relu\")\n",
    "    s = Dense(1)\n",
    "\n",
    "    # \"relevant\" document score\n",
    "    h_1_rel = h_1(s_vi)\n",
    "    h_1_rel = Dropout(dropout_rate)(h_1_rel)\n",
    "    h_2_rel = h_2(h_1_rel)\n",
    "    h_2_rel = Dropout(dropout_rate)(h_2_rel)\n",
    "    h_3_rel = h_3(h_2_rel)\n",
    "    h_3_rel = Dropout(dropout_rate)(h_3_rel)\n",
    "    rel_score = s(h_3_rel)\n",
    "\n",
    "    # \"irrelevant\" document score\n",
    "    h_1_irr = h_1(s_vj)\n",
    "    h_1_irr = Dropout(dropout_rate)(h_1_irr)\n",
    "    h_2_irr = h_2(h_1_irr)\n",
    "    h_2_irr = Dropout(dropout_rate)(h_2_irr)\n",
    "    h_3_irr = h_3(h_2_irr)\n",
    "    h_3_irr = Dropout(dropout_rate)(h_3_irr)\n",
    "    irr_score = s(h_3_irr)\n",
    "\n",
    "    # Subtract scores.\n",
    "    diff = Subtract()([rel_score, irr_score])\n",
    "    # Pass difference through sigmoid function.\n",
    "    prob = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "    model = Model(inputs=[hypo_input, hyper_input, negative_input], outputs=prob)\n",
    "    model.compile(optimizer = 'adadelta', loss = \"binary_crossentropy\")\n",
    "\n",
    "    get_score = K.function([s_vi], [rel_score])\n",
    "    return model, get_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard model where we learn phi projection and Ranknet weights together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)    \n",
    "hyper_embedding = embedding_layer(hyper_input)\n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "# first part of the feature extractor\n",
    "phi = Dense(200, activation=None, use_bias=False, \n",
    "            kernel_initializer=RandomIdentity(),\n",
    "            name='Phi') (hypo_embedding)\n",
    "\n",
    "# attempt toL unit norm phi\n",
    "#phi = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='NormPhi')(phi)\n",
    "\n",
    "# flatten outputs\n",
    "phi = Flatten(name='FlattenPhi')(phi)\n",
    "# dropout phi\n",
    "phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "\n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)    \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)\n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "feature_extractor.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "feature_extractor.get_layer(name='TermEmbedding').trainable = False\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-phi approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "query_embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_Q',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hyper_embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_H',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = query_embedding_layer(hypo_input)    \n",
    "hyper_embedding = hyper_embedding_layer(hyper_input)\n",
    "neg_embedding = hyper_embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "phi_k = 1\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomIdentity(),\n",
    "                           name='Phi%d' % (i)) (hypo_embedding))\n",
    "\n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='FlattenPhi')(phi_layer[0])    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "    phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=False))(phi)\n",
    "    \n",
    "        \n",
    "# attempt toL unit norm phi\n",
    "#phi = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='NormPhi')(phi)\n",
    "        \n",
    "# dropout phi\n",
    "phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "        \n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding) \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "feature_extractor.get_layer(name='TermEmbedding_Q').set_weights([data.embedding_matrix])\n",
    "feature_extractor.get_layer(name='TermEmbedding_H').set_weights([data.embedding_matrix])\n",
    "\n",
    "feature_extractor.get_layer(name='TermEmbedding_Q').trainable = False\n",
    "feature_extractor.get_layer(name='TermEmbedding_H').trainable = False\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor initially trained on MSE;\n",
    "\n",
    "After training we will create a new model (the feature extractor proper), set the embeddings and projection matrix weight; set all layers to untrainable and have a go on the LTR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_similarity is the dot product between the negative term and the hyponym projection\n",
    "# model should strive to minimise this value.\n",
    "# c is a regularisation weight\n",
    "def custom_loss(random_similarity, c):\n",
    "    def mse(y_true, y_pred):                        \n",
    "        return K.mean(K.square(y_pred - y_true), axis=-1) + (c * K.mean(K.square(random_similarity)))\n",
    "        #return (y_pred - y_true) + (c * K.square(random_similarity))\n",
    "                                                    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input  = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "hyper_embedding = embedding_layer(hyper_input)  \n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "phi_k = 1\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomNormal(mean=0., stddev=0.01),\n",
    "                           name='Phi%d' % (i)) (hypo_embedding))\n",
    "\n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='FlattenPhi')(phi_layer[0])    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "    phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=False))(phi)\n",
    "                    \n",
    "# dropout phi\n",
    "#phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)\n",
    "\n",
    "hyper_similarity = Dot(axes=-1, normalize=True, name='DotProductHyper')([phi, hyper_embedding])\n",
    "random_similarity = Dot(axes=-1, normalize=True, name='DotProductRand')([phi, neg_embedding])\n",
    "\n",
    "\n",
    "# initialise custom loss function\n",
    "mse = custom_loss(random_similarity, c=1.)\n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor = Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=hyper_similarity)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = mse)\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict hypoynym projection directly; minimise by computing MSE with target hypernym vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_similarity is the dot product between the negative term and the hyponym projection\n",
    "# model should strive to minimise this value.\n",
    "# c is a regularisation weight\n",
    "def custom_loss(similarities, c):\n",
    "    def mse(y_true, y_pred):                        \n",
    "        return K.mean(K.sum(K.square(y_pred - y_true), axis=-1)) +\\\n",
    "    (c[0] * K.mean(K.square(similarities[0]))) +\\\n",
    "    (c[1] * K.mean(K.square(similarities[1])))\n",
    "        #return (y_pred - y_true) + (c * K.square(random_similarity))\n",
    "                                                    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "\n",
    "phi = Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomNormal(mean=0., stddev=0.01),\n",
    "                           name='Phi') (hypo_embedding)\n",
    "\n",
    "phi = Flatten(name='FlattenPhi')(phi)    \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "\n",
    "random_similarity = Dot(axes=-1, normalize=True, name='DotProductRand')([phi, neg_embedding])\n",
    "query_similarity = Dot(axes=-1, normalize=True, name='DotProductRand2')([phi, hypo_embedding])\n",
    "\n",
    "# initialise custom loss function\n",
    "mse = custom_loss([random_similarity, query_similarity], c=[1., 0.1])\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, negative_input], outputs=phi)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = mse)\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRIM model - multi-phi + logistic regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "phi_k = 10\n",
    "max_or_combine = True\n",
    "\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "hyper_embedding = embedding_layer(hyper_input)\n",
    "\n",
    "hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "phi_layer = []\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(data.embedding_matrix.shape[1], activation=None, use_bias=False, \n",
    "                           activity_regularizer=None,\n",
    "                           kernel_initializer=RandomIdentity(),                               \n",
    "                           name='Phi%d' % (i)) (hypo_embedding))            \n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='Flatten_Phi')(phi_layer[0])\n",
    "    hyper_embedding = Flatten(name='Flatten_Hyper')(hyper_embedding)    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "\n",
    "# this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct1')([phi, hyper_embedding])                    \n",
    "\n",
    "if phi_k > 1:\n",
    "    if max_or_combine:\n",
    "        phi_hyper = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi_hyper)\n",
    "    phi_hyper = Flatten(name='Flatten_PhiHyper')(phi_hyper)\n",
    "\n",
    "predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                    use_bias=True,                    \n",
    "                    kernel_initializer='random_normal',\n",
    "                    kernel_constraint= None,                        \n",
    "                    bias_initializer=Zeros(),                                            \n",
    "                    kernel_regularizer=None,                        \n",
    "                    bias_regularizer=None\n",
    "                   ) (phi_hyper)\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = 'binary_crossentropy')\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create copy of model but with separate hypo and hyper embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "phi_k = 10\n",
    "max_or_combine = True\n",
    "\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer_q = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_Q',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "embedding_layer_h = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_H',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "\n",
    "hypo_embedding = embedding_layer_q(hypo_input)  \n",
    "hyper_embedding = embedding_layer_h(hyper_input)\n",
    "\n",
    "hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "phi_layer = []\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(data.embedding_matrix.shape[1], activation=None, use_bias=False, \n",
    "                           activity_regularizer=None,\n",
    "                           kernel_initializer=RandomIdentity(),                               \n",
    "                           name='Phi%d' % (i)) (hypo_embedding))            \n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='Flatten_Phi')(phi_layer[0])\n",
    "    hyper_embedding = Flatten(name='Flatten_Hyper')(hyper_embedding)    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "\n",
    "# this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct1')([phi, hyper_embedding])                    \n",
    "\n",
    "if phi_k > 1:\n",
    "    if max_or_combine:\n",
    "        phi_hyper = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi_hyper)\n",
    "    phi_hyper = Flatten(name='Flatten_PhiHyper')(phi_hyper)\n",
    "\n",
    "predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                    use_bias=True,                    \n",
    "                    kernel_initializer='random_normal',\n",
    "                    kernel_constraint= None,                        \n",
    "                    bias_initializer=Zeros(),                                            \n",
    "                    kernel_regularizer=None,                        \n",
    "                    bias_regularizer=None\n",
    "                   ) (phi_hyper)\n",
    "\n",
    "# create projection model\n",
    "projection_model_fine =  Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "#projection_model_fine.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "#projection_model_fine.get_layer(name='TermEmbedding').trainable = False\n",
    "#adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "#projection_model.compile(optimizer = adam, loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write callback that returns MRR at the end of every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class MRRLogger(Callback):\n",
    "    def set_evaluator(self, hypernym_evaluator):\n",
    "        self.he = hypernym_evaluator        \n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.mrr = []\n",
    "        self.map = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # no need to invoke validation data passed into fit function;\n",
    "        # validation data are encapsulated in hypernym_evaluator\n",
    "        self.predictions = self.he.predict_ltr_hypernyms()\n",
    "        _, all_scores = self.he.get_evaluation_scores(self.predictions)        \n",
    "        epoch_mrr = round(sum([score_list[0] for score_list in all_scores]) / len(all_scores), 5)  \n",
    "        epoch_map = round(sum([score_list[1] for score_list in all_scores]) / len(all_scores), 5)  \n",
    "        self.mrr.append(epoch_mrr)\n",
    "        self.map.append(epoch_map)\n",
    "        print (\"; MRR: %.4f; MAP: %.4f\" % (epoch_mrr, epoch_map)) \n",
    "\n",
    "        \n",
    "class BestModelWeightSaver(Callback):\n",
    "    def set_mrr_logger(self, mrr_logger):\n",
    "        self.mrr_logger = mrr_logger\n",
    "        \n",
    "    def set_filepath(self, filepath):\n",
    "        # file path should include placeholders for epoch and validation MRR\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.best_metric = 0.\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # decide on whether we're going to overwrite weights or ignore this epoch\n",
    "        # because of inferior results\n",
    "        test_metric = np.sqrt(mrr_logger.map[::-1][0] * mrr_logger.mrr[::-1][0])\n",
    "        if test_metric > self.best_metric:\n",
    "            # we have new highest MRR: save weights to disc\n",
    "            self.best_metric = test_metric\n",
    "            np.savez_compressed(self.filepath % (epoch, self.best_metric), self.model.get_weights())            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.ones((len(query_seq), 1))\n",
    "v_y = np.ones((len(v_query), 1))\n",
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# get RankNet model and scorer\n",
    "model, get_score = get_RankNet_model(feature_extractor, dropout_rate=0.2)\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                    validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                    batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                    callbacks=[mrr_logger, weight_saver]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MSE-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare to run MSE model\n",
    "y = [1.] * len(hyper_seq)\n",
    "v_y = [1.] * len(v_hyper_seq)\n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_MSE((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "history = projection_model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                               validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run proper MSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare y vectors\n",
    "y = np.zeros((len(hyper_seq),200))\n",
    "for idx, h in enumerate(hyper_seq):\n",
    "    y[idx] = data.embedding_matrix[h[0]]\n",
    "\n",
    "v_y = np.zeros((len(v_hyper_seq),200))\n",
    "for idx, h in enumerate(v_hyper_seq):\n",
    "    v_y[idx] = data.embedding_matrix[h[0]]\n",
    "\n",
    "    \n",
    "# train model\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "he = HypernymEvaluation_MSE((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model.fit([query_seq, neg_seq], y, \n",
    "                               validation_data = ([v_query_seq, v_neg_seq], v_y),\n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CRIM model using general fit function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first prepare training set composed of separate positive and negative instances\n",
    "neg_crim_query = []\n",
    "neg_crim_hyper = []\n",
    "\n",
    "n_negative = 10\n",
    "for q, h in zip(data.train_query, data.train_hyper):        \n",
    "    # mix of random and synonyms\n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()            \n",
    "    neg_crim_query.extend([q] * n_negative)    \n",
    "    neg_crim_hyper.extend(rands)\n",
    "                                         \n",
    "query = data.train_query + neg_crim_query\n",
    "hyper = data.train_hyper + neg_crim_hyper\n",
    "y = [1.] * len(data.train_query) + [0.] * len(neg_crim_query)\n",
    "\n",
    "query_seq, hyper_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), [query, hyper])\n",
    "v_query_seq, v_hyper_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), [data.valid_query, data.valid_hyper])\n",
    "\n",
    "v_y = [1.] * len(v_query_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model.fit([query_seq, hyper_seq], y, validation_data = ([v_query_seq, v_hyper_seq], v_y), \n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('models/best_ltr_e%s_mrr%.4f' % (10, 0.1044), projection_model.get_weights())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dirham': [u'monetary_unit',\n",
       "  u'sale',\n",
       "  u'the_treasury',\n",
       "  u'amount_of_money',\n",
       "  u'natural_person',\n",
       "  u'paid',\n",
       "  u'sum_of_money',\n",
       "  u'stamp_duty',\n",
       "  u'legal_tender',\n",
       "  u'ration_card',\n",
       "  u'market_price',\n",
       "  u'net_worth',\n",
       "  u'person',\n",
       "  u'payment',\n",
       "  u'gold_coin']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "he.predict_ltr_hypernym(['dirham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projection_model_fine.get_layer(name='TermEmbedding_Q').set_weights([data.embedding_matrix])\n",
    "projection_model_fine.get_layer(name='TermEmbedding_H').set_weights([data.embedding_matrix])\n",
    "\n",
    "projection_model_fine.get_layer(name='TermEmbedding_H').trainable = True\n",
    "projection_model_fine.get_layer(name='TermEmbedding_Q').trainable = True\n",
    "\n",
    "# get projection matrices\n",
    "dense = map(lambda x: x.get_weights()[0], [l for l in projection_model.layers if l.name.startswith('Phi')])\n",
    "dense = np.asarray(dense)\n",
    "# get sigmoid weights\n",
    "lr_weights = projection_model.get_layer(name='Prediction').get_weights()\n",
    "    \n",
    "# inject pre-trained embedding weights into Embedding layer\n",
    "        \n",
    "phi_projections = [l for l in projection_model_fine.layers if l.name.startswith('Phi')]    \n",
    "for idx, phi_projection in enumerate(phi_projections):\n",
    "    phi_projection.set_weights([dense[idx]])\n",
    "    phi_projection.trainable = False\n",
    "\n",
    "projection_model_fine.get_layer(name='Prediction').set_weights(lr_weights)\n",
    "projection_model_fine.get_layer(name='Prediction').trainable = True\n",
    "                \n",
    "projection_model_fine.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune model, but keeping Phi frozen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jfarrugia/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 129569 samples, validate on 200 samples\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model_fine)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model_fine.fit([query_seq, hyper_seq], y, validation_data = ([v_query_seq, v_hyper_seq], v_y), \n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RankNet model from already trained feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ranknet on trained model\n",
    "# first set feature_extractor Phi to non-trainable\n",
    "feature_extractor.get_layer(name='Phi0').trainable = False\n",
    "\n",
    "y = np.ones((len(query_seq), 1))\n",
    "v_y = np.ones((len(v_query), 1))\n",
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# get RankNet model and scorer\n",
    "model, get_score = get_RankNet_model(feature_extractor, dropout_rate=0.2)\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_SquareDiff((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                    validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                    batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                    callbacks=[mrr_logger, weight_saver]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model from Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved weights\n",
    "model_weights = np.load('models/best_ltr_e3_mrr0.1257.npz') \n",
    "model_weights = model_weights['arr_0'].tolist()\n",
    "projection_model.set_weights(model_weights)\n",
    "                  \n",
    "# refresh scorer\n",
    "#get_score = K.function([s_vi], [rel_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_extractor.summary()\n",
    "for idx, (loss, val_loss, mrr, mean_prec) in enumerate(zip(history.history['loss'], history.history['val_loss'], mrr_logger.mrr, mrr_logger.map)):\n",
    "    print idx+1, loss, val_loss, mrr, mean_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "he.predict_ltr_hypernym(['rod_laver'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_logger.predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "print (\"Generating predictions...\")\n",
    "\n",
    "he_test = HypernymEvaluation_CRIM_Max((data.test_query, data.test_hyper), data.tokenizer, projection_model)\n",
    "ltr_predictions = he_test.predict_ltr_hypernyms()\n",
    "\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = he_test.get_evaluation_scores(ltr_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print projection_model.test_on_batch([query_seq[0], neg_seq[0]], y[0].reshape(1,-1))\n",
    "print np.sum(np.square(projection_model.predict([query_seq[0], neg_seq[0]]) - y[0]))\n",
    "phi = projection_model.get_layer(name='Phi').get_weights()[0]\n",
    "\n",
    "proj = np.dot(data.embedding_matrix[query_seq[0][0]], phi)\n",
    "negative_ex =  data.embedding_matrix[neg_seq[0][0]]\n",
    "pos_ex = y[0]\n",
    "\n",
    "print np.sum(np.square(proj - pos_ex))\n",
    "print np.sum(np.square(proj - negative_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.tokenizer.word_index['dog']\n",
    "b = data.tokenizer.word_index['animal']\n",
    "proj = np.dot(data.embedding_matrix[a], phi)\n",
    "\n",
    "\n",
    "np.sum(np.square(proj - data.embedding_matrix[a]))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(proj[np.newaxis,:], data.embedding_matrix[b][np.newaxis,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_matrix = [l.get_weights()[0] for l in projection_model.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "phi_matrix = np.asarray(phi_matrix)\n",
    "\n",
    "word = data.embedding_matrix[data.tokenizer.word_index['rod_laver']]\n",
    "pos = data.embedding_matrix[data.tokenizer.word_index['athlete']]\n",
    "neg = data.embedding_matrix[data.tokenizer.word_index['tennis_player']]\n",
    "\n",
    "cluster_weight = projection_model.get_layer(name='Prediction').get_weights()[0]\n",
    "bias = projection_model.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "#proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = np.dot(word, phi_matrix)\n",
    "proj = np.mean(proj, axis=0)\n",
    "\n",
    "proj /= np.linalg.norm(proj)\n",
    "\n",
    "print np.dot(proj, neg) * cluster_weight + bias\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
