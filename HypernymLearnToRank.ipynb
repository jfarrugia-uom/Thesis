{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [0,0,0,1,0]\n",
    "r = np.asarray(r) != 0\n",
    "r = np.asarray(r)[:4] != 0\n",
    "(np.mean(r) * 4)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HypernymEvaluation:\n",
    "    \n",
    "    def __init__(self, dataset, tokenizer, feature_extractor, scorer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.scorer = scorer\n",
    "                \n",
    "\n",
    "    def convert_hypernyms_to_one_line(self):\n",
    "        ordered_queries = sorted(list(set(self.dataset[0])))\n",
    "        one_line = {}\n",
    "        for w in ordered_queries:\n",
    "            word_hypernyms = [h for q, h in zip(*self.dataset) if q == w]\n",
    "            one_line[w] = word_hypernyms\n",
    "        return one_line\n",
    "\n",
    "    # taken from task_scorer.py provided with shared task resources\n",
    "    def mean_reciprocal_rank(self, r):\n",
    "        \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "        First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "        Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean reciprocal rank\n",
    "        \"\"\"\n",
    "        r = np.asarray(r).nonzero()[0]\n",
    "        return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "    def precision_at_k(self, r, k, n):\n",
    "        \"\"\"Score is precision @ k\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Precision @ k\n",
    "        Raises:\n",
    "            ValueError: len(r) must be >= k\n",
    "        \"\"\"\n",
    "        assert k >= 1\n",
    "        r = np.asarray(r)[:k] != 0\n",
    "        if r.size != k:\n",
    "            raise ValueError('Relevance score length < k')\n",
    "        return (np.mean(r)*k)/min(k,n)\n",
    "        # Modified from the first version. Now the gold elements are taken into account\n",
    "\n",
    "    def average_precision(self, r,n):\n",
    "        \"\"\"Score is average precision (area under PR curve)\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Average precision\n",
    "        \"\"\"\n",
    "        r = np.asarray(r) != 0\n",
    "        out = [self.precision_at_k(r, k + 1, n) for k in range(r.size)]\n",
    "        #Modified from the first version (removed \"if r[k]\"). All elements (zero and nonzero) are taken into account\n",
    "        if not out:\n",
    "            return 0.\n",
    "        return np.mean(out)\n",
    "\n",
    "    def mean_average_precision(self, r, n):\n",
    "        \"\"\"Score is mean average precision\n",
    "        Relevance is binary (nonzero is relevant).\n",
    "        Args:\n",
    "            r: Relevance scores (list or numpy) in rank order\n",
    "                (first element is the first item)\n",
    "        Returns:\n",
    "            Mean average precision\n",
    "        \"\"\"\n",
    "        return self.average_precision(r,n)\n",
    "\n",
    "    # predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "    def get_evaluation_scores(self, predictions):\n",
    "        all_scores = []    \n",
    "        scores_names = ['MRR', 'MAP', 'P@1', 'P@5', 'P@10']\n",
    "        for query, gold_hyps in self.convert_hypernyms_to_one_line().items():\n",
    "\n",
    "            avg_pat1 = []\n",
    "            avg_pat2 = []\n",
    "            avg_pat3 = []\n",
    "\n",
    "            pred_hyps = predictions[query]\n",
    "            gold_hyps_n = len(gold_hyps)    \n",
    "            r = [0 for i in range(15)]\n",
    "\n",
    "            for j in range(len(pred_hyps)):\n",
    "                # I believe it's not fair to bias evaluation on how many hypernyms were found in gold set\n",
    "                # if anything a shorter list (ex. because a hypernym is very particular) will already make \n",
    "                # it harder for a match to be found but if system returns correct hypernym in second place\n",
    "                # why should it be ignored?\n",
    "                if j < gold_hyps_n:\n",
    "                    pred_hyp = pred_hyps[j]\n",
    "                    if pred_hyp in gold_hyps:\n",
    "                        r[j] = 1\n",
    "\n",
    "            avg_pat1.append(self.precision_at_k(r,1,gold_hyps_n))\n",
    "            avg_pat2.append(self.precision_at_k(r,5,gold_hyps_n))\n",
    "            avg_pat3.append(self.precision_at_k(r,10,gold_hyps_n))    \n",
    "            \n",
    "            mrr_score_numb = self.mean_reciprocal_rank(r)\n",
    "            map_score_numb = self.mean_average_precision(r,gold_hyps_n)            \n",
    "            avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "            avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "            avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "            score_results = [mrr_score_numb, map_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "            all_scores.append(score_results)\n",
    "        return scores_names, all_scores\n",
    "\n",
    "    # return predictions for user-defined list of terms\n",
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings_q = self.feature_extractor.get_layer(name='TermEmbedding_Q').get_weights()[0]\n",
    "        embeddings_h = self.feature_extractor.get_layer(name='TermEmbedding_H').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings_q[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "            \n",
    "            # normalise phi projection\n",
    "            #word_phi /= np.linalg.norm(word_phi)\n",
    "                        \n",
    "            hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n",
    "    \n",
    "    # return predictions for all terms initially passed to class\n",
    "    def predict_ltr_hypernyms(self):\n",
    "        return self.predict_ltr_hypernym(self.dataset[0])\n",
    "\n",
    "\n",
    "class HypernymEvaluation_SquareDiff(HypernymEvaluation):\n",
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        #embeddings_q = self.feature_extractor.get_layer(name='TermEmbedding_Q').get_weights()[0]\n",
    "        #embeddings_h = self.feature_extractor.get_layer(name='TermEmbedding_H').get_weights()[0]\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "                        \n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = self.scorer([(embeddings[1:] - word_phi)**2])            \n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results    \n",
    "    \n",
    "class HypernymEvaluation_MSE(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "        \n",
    "    \n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        #phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)            \n",
    "            word_phi = np.mean(word_phi, axis=0)\n",
    "            \n",
    "            # normalise phi projection\n",
    "            word_phi /= np.linalg.norm(word_phi)\n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = np.dot(embeddings[1:], word_phi)\n",
    "            #hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores.flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n",
    "    \n",
    "class HypernymEvaluation_CRIM(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "\n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "        \n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "        \n",
    "        cluster_weight = self.feature_extractor.get_layer(name='Prediction').get_weights()[0]\n",
    "        bias = self.feature_extractor.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)                                                \n",
    "    \n",
    "            sim_matrix = np.dot(cluster_weight.T, np.dot(embeddings[1:], word_phi.T).T) + bias\n",
    "            top_words = np.argsort(sim_matrix[0])[::-1][:15] + 1\n",
    "                                                \n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results        \n",
    "    \n",
    "class HypernymEvaluation_CRIM_Mean(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "\n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "        \n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "        \n",
    "        cluster_weight = self.feature_extractor.get_layer(name='Prediction').get_weights()[0]\n",
    "        bias = self.feature_extractor.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)\n",
    "            \n",
    "            word_phi /= np.linalg.norm(word_phi, axis=1).reshape(-1,1)            \n",
    "                                    \n",
    "            sim_matrix = np.dot(embeddings[1:], word_phi.T)    \n",
    "            max_sim = np.mean(sim_matrix, 1).reshape(1,-1)\n",
    "            sim_matrix = np.dot(cluster_weight.T, max_sim) + bias        \n",
    "            \n",
    "            top_words = np.argsort(sim_matrix[0])[::-1][:15] + 1\n",
    "                                                \n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results        \n",
    "    \n",
    "class HypernymEvaluation_CRIM_Max(HypernymEvaluation):\n",
    "    def __init__(self, dataset, tokenizer, feature_extractor):\n",
    "        HypernymEvaluation.__init__(self, dataset, tokenizer, feature_extractor, None)\n",
    "\n",
    "    def predict_ltr_hypernym(self, queries):\n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "        \n",
    "        phi_matrix = [l.get_weights()[0] for l in self.feature_extractor.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "        phi_matrix = np.asarray(phi_matrix)\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]        \n",
    "        \n",
    "        cluster_weight = self.feature_extractor.get_layer(name='Prediction').get_weights()[0]\n",
    "        bias = self.feature_extractor.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)\n",
    "            \n",
    "            word_phi /= np.linalg.norm(word_phi, axis=1).reshape(-1,1)            \n",
    "                                    \n",
    "            sim_matrix = np.dot(embeddings[1:], word_phi.T)    \n",
    "            max_sim = np.max(sim_matrix, 1).reshape(1,-1)\n",
    "            sim_matrix = np.dot(cluster_weight.T, max_sim) + bias        \n",
    "            \n",
    "            top_words = np.argsort(sim_matrix[0])[::-1][:15] + 1\n",
    "                                                \n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def predict_ltr_hypernym(self, queries):        \n",
    "        ordered_queries = sorted(list(set(queries)))\n",
    "        results = {}\n",
    "\n",
    "        phi_matrix = self.feature_extractor.get_layer(name='Phi').get_weights()[0]\n",
    "        embeddings = self.feature_extractor.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "        for idx, word in enumerate(ordered_queries):        \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print (\"Done\", idx + 1)\n",
    "\n",
    "            q_idx = self.tokenizer.word_index[word]        \n",
    "            word_phi = np.dot(embeddings[q_idx], phi_matrix)\n",
    "            # normalise phi projection\n",
    "            #word_phi /= np.linalg.norm(word_phi)\n",
    "            \n",
    "            # square vector different as per model\n",
    "            hyp_scores = self.scorer([(embeddings[1:] - word_phi)**2])\n",
    "            #hyp_scores = self.scorer([embeddings[1:] - word_phi])\n",
    "            top_words = np.argsort(hyp_scores[0].flatten())[::-1][:15] + 1\n",
    "            results[word] = self.tokenizer.sequences_to_texts(top_words.reshape(-1,1))                        \n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test final MRR score\n",
    "\n",
    "get_score = K.function([s_vi], [rel_score])\n",
    "he = HypernymEvaluation((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "predictions = he.predict_ltr_hypernyms()\n",
    "#predictions = mrr_logger.predictions\n",
    "_, all_scores = he.get_evaluation_scores(predictions)\n",
    "mrr = round(sum([score_list[0] for score_list in all_scores]) / len(all_scores), 5)                                \n",
    "print mrr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rod_laver': [u'athlete',\n",
       "  u'athletes',\n",
       "  u'team_sport',\n",
       "  u'sports',\n",
       "  u'bodybuilding',\n",
       "  u'basketball',\n",
       "  u'kickboxing',\n",
       "  u'sports_organization',\n",
       "  u'olympic_sports',\n",
       "  u'sportsperson',\n",
       "  u'sports_league',\n",
       "  u'sportsmanship',\n",
       "  u'floor_hockey',\n",
       "  u'basketball_player',\n",
       "  u'student_athlete']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "he.predict_ltr_hypernym(['rod_laver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_synyonyms(hyponyms, hypernyms, n=15):\n",
    "    synonyms = {}\n",
    "    \n",
    "    # prepare hypernym lookup dictionary\n",
    "    hyper_lookup = defaultdict(list)\n",
    "    for q, h in zip(hyponyms, hypernyms):\n",
    "        hyper_lookup[q].append(h)\n",
    "                \n",
    "    for term in set(hyponyms):        \n",
    "        synonyms[term] = list(filter(lambda x: x not in hyper_lookup[x], zip(*model.most_similar(term, topn=20))[0]))[:n]\n",
    "        \n",
    "    return synonyms\n",
    "    \n",
    "#get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)    \n",
    "#get_synyonyms(valid_query, valid_hyper)    \n",
    "\n",
    "def get_random(hyponyms, hypernyms, vocab, n = 15):\n",
    "    \n",
    "    random_words = {}\n",
    "    \n",
    "    # prepare hypernym lookup dictionary\n",
    "    hyper_lookup = defaultdict(list)\n",
    "    for q, h in zip(hyponyms, hypernyms):\n",
    "        hyper_lookup[q].append(h)\n",
    "            \n",
    "    for term in set(hyponyms):                \n",
    "        some_words = np.random.choice(vocab, 20, replace=False)        \n",
    "        random_words[term] = list(filter(lambda x: x not in hyper_lookup[x], some_words))[:n]\n",
    "            \n",
    "    return random_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, \n",
    "                 train_query, train_hyper, \n",
    "                 test_query, test_hyper, \n",
    "                 valid_query, valid_hyper, \n",
    "                 vocab, embeddings):\n",
    "        \n",
    "                \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.valid_query = valid_query\n",
    "        self.valid_hyper = valid_hyper\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        #self.synonyms = synonyms\n",
    "                \n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['animal'].shape[0]\n",
    "        \n",
    "        print (\"Tokenising words...\")\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = tokenizer = Tokenizer(num_words = 300000, filters='')\n",
    "        self.tokenizer.fit_on_texts(train_query + test_query + valid_query + vocab)\n",
    "        \n",
    "        print (\"Creating embedding matrix...\")\n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings[word]\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector (already normalised)\n",
    "                    #embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)\n",
    "        \n",
    "        print (\"Creating random words/synonyms...\")\n",
    "        self.random_words = get_random(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper, vocab)  \n",
    "        self.synonyms = get_synyonyms(train_query + test_query + valid_query, train_hyper + test_hyper + valid_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = Data(train_query, train_hyper, test_query, test_hyper, valid_query, valid_hyper, vocab, model)\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('.', 'pickle')\n",
    "#pickle.dump(data, open(os.path.join(dest, 'semeval_data.pkl'), 'wb'), protocol=2)\n",
    "data = pickle.load(open(os.path.join(dest, 'semeval_data.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for fitting RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#term_count = Counter(data.train_query)\n",
    "\n",
    "query = []\n",
    "hyper = []\n",
    "negat = []\n",
    "n_negative = 1\n",
    "for q, h in zip(data.train_query, data.train_hyper):        \n",
    "    # mix of random and synonyms\n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()\n",
    "    \n",
    "    # append query word to negatives    \n",
    "    #rands.append(q)\n",
    "    \n",
    "    query.extend([q] * n_negative)\n",
    "    hyper.extend([h] * n_negative)\n",
    "    negat.extend(rands)\n",
    "                                         \n",
    "\n",
    "query_seq, hyper_seq, neg_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), \n",
    "                                    [query, hyper, negat])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data for fitting RankNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_query = []\n",
    "v_hyper = []\n",
    "v_negat = []\n",
    "\n",
    "n_negative = 1\n",
    "for q, h in zip(data.valid_query, data.valid_hyper):        \n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()\n",
    "    #rands.append(q)\n",
    "    \n",
    "    v_query.extend([q] * n_negative)\n",
    "    v_hyper.extend([h] * n_negative)\n",
    "    v_negat.extend(rands)\n",
    "                                 \n",
    "\n",
    "v_query_seq, v_hyper_seq, v_neg_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), \n",
    "                                    [v_query, v_hyper, v_negat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip(query[:10], hyper[:10], negat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print query_seq[0], hyper_seq[0], neg_seq[0]\n",
    "print map(lambda x: len(x), [query_seq, hyper_seq, neg_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keras imports\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding,  Flatten,  Dropout, Subtract, Activation, Lambda, concatenate, Dot\n",
    "from tensorflow.keras.models import Model, save_model, load_model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm, MinMaxNorm\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.keras.initializers import Initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor/Pair-wise LTR Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomIdentity(Initializer):\n",
    "    def __init__(self, dtype=dtypes.float32):\n",
    "        self.dtype = dtypes.as_dtype(dtype)\n",
    "\n",
    "    \n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.dtype\n",
    "        \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)        \n",
    "        #identity = K.eye(shape[-1], dtype='float32')        \n",
    "        rident = tf.eye(shape[-1]) * rnorm\n",
    "        return rident\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"dtype\": self.dtype.name}\n",
    "\n",
    "    \n",
    "class RandomPlusIdentity(Initializer):\n",
    "    def __init__(self, dtype=dtypes.float32):\n",
    "        self.dtype = dtypes.as_dtype(dtype)\n",
    "\n",
    "    \n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        if dtype is None:\n",
    "            dtype = self.dtype\n",
    "        \n",
    "        rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.01)    \n",
    "        rident = tf.eye(shape[-1]) + rnorm\n",
    "        return rident            \n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"dtype\": self.dtype.name}\n",
    "        \n",
    "\n",
    "get_custom_objects().update({'RandomIdentity': RandomIdentity})\n",
    "get_custom_objects().update({'RandomPlusIdentity': RandomPlusIdentity})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_RankNet_model(feature_extractor, dropout_rate):\n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "    s_vi, s_vj = feature_extractor([hypo_input, hyper_input, negative_input])\n",
    "\n",
    "    # now we can pipe our extracted features into a RankNet model\n",
    "    h_1 = Dense(128, activation = \"relu\")\n",
    "    h_2 = Dense(64, activation = \"relu\")\n",
    "    h_3 = Dense(32, activation = \"relu\")\n",
    "    s = Dense(1)\n",
    "\n",
    "    # \"relevant\" document score\n",
    "    h_1_rel = h_1(s_vi)\n",
    "    h_1_rel = Dropout(dropout_rate)(h_1_rel)\n",
    "    h_2_rel = h_2(h_1_rel)\n",
    "    h_2_rel = Dropout(dropout_rate)(h_2_rel)\n",
    "    h_3_rel = h_3(h_2_rel)\n",
    "    h_3_rel = Dropout(dropout_rate)(h_3_rel)\n",
    "    rel_score = s(h_3_rel)\n",
    "\n",
    "    # \"irrelevant\" document score\n",
    "    h_1_irr = h_1(s_vj)\n",
    "    h_1_irr = Dropout(dropout_rate)(h_1_irr)\n",
    "    h_2_irr = h_2(h_1_irr)\n",
    "    h_2_irr = Dropout(dropout_rate)(h_2_irr)\n",
    "    h_3_irr = h_3(h_2_irr)\n",
    "    h_3_irr = Dropout(dropout_rate)(h_3_irr)\n",
    "    irr_score = s(h_3_irr)\n",
    "\n",
    "    # Subtract scores.\n",
    "    diff = Subtract()([rel_score, irr_score])\n",
    "    # Pass difference through sigmoid function.\n",
    "    prob = Activation(\"sigmoid\")(diff)\n",
    "\n",
    "    model = Model(inputs=[hypo_input, hyper_input, negative_input], outputs=prob)\n",
    "    model.compile(optimizer = 'adadelta', loss = \"binary_crossentropy\")\n",
    "\n",
    "    get_score = K.function([s_vi], [rel_score])\n",
    "    return model, get_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard model where we learn phi projection and Ranknet weights together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)    \n",
    "hyper_embedding = embedding_layer(hyper_input)\n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "# first part of the feature extractor\n",
    "phi = Dense(200, activation=None, use_bias=False, \n",
    "            kernel_initializer=RandomIdentity(),\n",
    "            name='Phi') (hypo_embedding)\n",
    "\n",
    "# attempt toL unit norm phi\n",
    "#phi = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='NormPhi')(phi)\n",
    "\n",
    "# flatten outputs\n",
    "phi = Flatten(name='FlattenPhi')(phi)\n",
    "# dropout phi\n",
    "phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "\n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)    \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)\n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "feature_extractor.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "feature_extractor.get_layer(name='TermEmbedding').trainable = False\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-phi approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "query_embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_Q',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hyper_embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding_H',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = query_embedding_layer(hypo_input)    \n",
    "hyper_embedding = hyper_embedding_layer(hyper_input)\n",
    "neg_embedding = hyper_embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "phi_k = 1\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomIdentity(),\n",
    "                           name='Phi%d' % (i)) (hypo_embedding))\n",
    "\n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='FlattenPhi')(phi_layer[0])    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "    phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=False))(phi)\n",
    "    \n",
    "        \n",
    "# attempt toL unit norm phi\n",
    "#phi = Lambda(lambda x: K.l2_normalize(x, axis=-1), name='NormPhi')(phi)\n",
    "        \n",
    "# dropout phi\n",
    "phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "        \n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding) \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "feature_extractor.get_layer(name='TermEmbedding_Q').set_weights([data.embedding_matrix])\n",
    "feature_extractor.get_layer(name='TermEmbedding_H').set_weights([data.embedding_matrix])\n",
    "\n",
    "feature_extractor.get_layer(name='TermEmbedding_Q').trainable = False\n",
    "feature_extractor.get_layer(name='TermEmbedding_H').trainable = False\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor initially trained on MSE;\n",
    "\n",
    "After training we will create a new model (the feature extractor proper), set the embeddings and projection matrix weight; set all layers to untrainable and have a go on the LTR;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_similarity is the dot product between the negative term and the hyponym projection\n",
    "# model should strive to minimise this value.\n",
    "# c is a regularisation weight\n",
    "def custom_loss(random_similarity, c):\n",
    "    def mse(y_true, y_pred):                        \n",
    "        return K.mean(K.square(y_pred - y_true), axis=-1) + (c * K.mean(K.square(random_similarity)))\n",
    "        #return (y_pred - y_true) + (c * K.square(random_similarity))\n",
    "                                                    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input  = Input(shape=(1,), name='Hypernym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "hyper_embedding = embedding_layer(hyper_input)  \n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "# dropout 0.3 of the embeddings parameters\n",
    "hypo_embedding = Dropout(0.3, name='DropHypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(0.3, name='DropHyper')(hyper_embedding)\n",
    "neg_embedding = Dropout(0.3, name='DropNeg')(neg_embedding)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "phi_k = 1\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomNormal(mean=0., stddev=0.01),\n",
    "                           name='Phi%d' % (i)) (hypo_embedding))\n",
    "\n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='FlattenPhi')(phi_layer[0])    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "    phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=False))(phi)\n",
    "                    \n",
    "# dropout phi\n",
    "#phi = Dropout(0.3, name='DropoutPhi')(phi)\n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "hyper_embedding = Flatten(name='FlattenHyper')(hyper_embedding)\n",
    "\n",
    "hyper_similarity = Dot(axes=-1, normalize=True, name='DotProductHyper')([phi, hyper_embedding])\n",
    "random_similarity = Dot(axes=-1, normalize=True, name='DotProductRand')([phi, neg_embedding])\n",
    "\n",
    "\n",
    "# initialise custom loss function\n",
    "mse = custom_loss(random_similarity, c=1.)\n",
    "\n",
    "# extract features from query (hyponym) and doc i (relevant hypernym), doc j (irrelevant hypernym)\n",
    "vi = Subtract(name='Sub1')([hyper_embedding, phi])\n",
    "# square the subtraction\n",
    "vi = Lambda(lambda x: K.square(x))(vi)\n",
    "\n",
    "vj = Subtract(name='Sub2')([neg_embedding, phi])\n",
    "# square the substraction vector\n",
    "vj = Lambda(lambda x: K.square(x))(vj)\n",
    "\n",
    "feature_extractor = Model(inputs=[hypo_input, hyper_input, negative_input], outputs=[vi, vj])\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, hyper_input, negative_input], outputs=hyper_similarity)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = mse)\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict hypoynym projection directly; minimise by computing MSE with target hypernym vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random_similarity is the dot product between the negative term and the hyponym projection\n",
    "# model should strive to minimise this value.\n",
    "# c is a regularisation weight\n",
    "def custom_loss(similarities, c):\n",
    "    def mse(y_true, y_pred):                        \n",
    "        return K.mean(K.sum(K.square(y_pred - y_true), axis=-1)) +\\\n",
    "    (c[0] * K.mean(K.square(similarities[0]))) +\\\n",
    "    (c[1] * K.mean(K.square(similarities[1])))\n",
    "        #return (y_pred - y_true) + (c * K.square(random_similarity))\n",
    "                                                    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# inputs\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "negative_input = Input(shape=(1,), name='Negative')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "neg_embedding = embedding_layer(negative_input)\n",
    "\n",
    "phi_layer = []\n",
    "\n",
    "# build k projection matrices\n",
    "\n",
    "phi = Dense(200, activation=None, use_bias=False, \n",
    "                           kernel_initializer=RandomNormal(mean=0., stddev=0.01),\n",
    "                           name='Phi') (hypo_embedding)\n",
    "\n",
    "phi = Flatten(name='FlattenPhi')(phi)    \n",
    "neg_embedding = Flatten(name='FlattenNeg')(neg_embedding)        \n",
    "\n",
    "random_similarity = Dot(axes=-1, normalize=True, name='DotProductRand')([phi, neg_embedding])\n",
    "query_similarity = Dot(axes=-1, normalize=True, name='DotProductRand2')([phi, hypo_embedding])\n",
    "\n",
    "# initialise custom loss function\n",
    "mse = custom_loss([random_similarity, query_similarity], c=[1., 0.1])\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, negative_input], outputs=phi)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = mse)\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRIM model - multi-phi + logistic regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "phi_k = 20\n",
    "# modes = 0 combine; 1 mean; 2, max\n",
    "mode = 1\n",
    "\n",
    "\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "hyper_embedding = embedding_layer(hyper_input)\n",
    "\n",
    "hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "phi_layer = []\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(data.embedding_matrix.shape[1], activation=None, use_bias=False, \n",
    "                           activity_regularizer=None,\n",
    "                           kernel_initializer=RandomIdentity(),                               \n",
    "                           name='Phi%d' % (i)) (hypo_embedding))            \n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='Flatten_Phi')(phi_layer[0])\n",
    "    hyper_embedding = Flatten(name='Flatten_Hyper')(hyper_embedding)    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "    #phi = Lambda(lambda x: K.mean(x, axis=1, keepdims=False))(phi)\n",
    "    #hyper_embedding = Flatten(name='Flatten_Hyper')(hyper_embedding)    \n",
    "\n",
    "phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "\n",
    "# this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct1')([phi, hyper_embedding])                    \n",
    "\n",
    "if phi_k > 1:\n",
    "    if mode == 1:\n",
    "        phi_hyper = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi_hyper)\n",
    "    if mode == 2:\n",
    "        phi_hyper = Lambda(lambda x: K.max(x, axis=1, keepdims=True))(phi_hyper)\n",
    "        \n",
    "    phi_hyper = Flatten(name='Flatten_PhiHyper')(phi_hyper)\n",
    "\n",
    "predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                    use_bias=True,                    \n",
    "                    kernel_initializer='random_normal',\n",
    "                    kernel_constraint= None,                        \n",
    "                    bias_initializer=Zeros(),                                            \n",
    "                    kernel_regularizer=None,                        \n",
    "                    bias_regularizer=None\n",
    "                   ) (phi_hyper)\n",
    "\n",
    "# create projection model\n",
    "projection_model =  Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "projection_model.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "projection_model.get_layer(name='TermEmbedding').trainable = False\n",
    "adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "projection_model.compile(optimizer = adam, loss = 'binary_crossentropy')\n",
    "\n",
    "########################### END OF FEATURE EXTRACTOR DEFINITION #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Hyponym (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "TermEmbedding (Embedding)       (None, 1, 200)       43904800    Hyponym[0][0]                    \n",
      "                                                                 Hypernym[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Hypo (Dropout)          (None, 1, 200)       0           TermEmbedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Phi0 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi1 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi2 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi3 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi4 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi5 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi6 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi7 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi8 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi9 (Dense)                    (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi10 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi11 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi12 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi13 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi14 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi15 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi16 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi17 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi18 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Phi19 (Dense)                   (None, 1, 200)       40000       Dropout_Hypo[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Hypernym (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20, 200)      0           Phi0[0][0]                       \n",
      "                                                                 Phi1[0][0]                       \n",
      "                                                                 Phi2[0][0]                       \n",
      "                                                                 Phi3[0][0]                       \n",
      "                                                                 Phi4[0][0]                       \n",
      "                                                                 Phi5[0][0]                       \n",
      "                                                                 Phi6[0][0]                       \n",
      "                                                                 Phi7[0][0]                       \n",
      "                                                                 Phi8[0][0]                       \n",
      "                                                                 Phi9[0][0]                       \n",
      "                                                                 Phi10[0][0]                      \n",
      "                                                                 Phi11[0][0]                      \n",
      "                                                                 Phi12[0][0]                      \n",
      "                                                                 Phi13[0][0]                      \n",
      "                                                                 Phi14[0][0]                      \n",
      "                                                                 Phi15[0][0]                      \n",
      "                                                                 Phi16[0][0]                      \n",
      "                                                                 Phi17[0][0]                      \n",
      "                                                                 Phi18[0][0]                      \n",
      "                                                                 Phi19[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Phi (Dropout)           (None, 20, 200)      0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_Hyper (Dropout)         (None, 1, 200)       0           TermEmbedding[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "DotProduct1 (Dot)               (None, 20, 1)        0           Dropout_Phi[0][0]                \n",
      "                                                                 Dropout_Hyper[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1, 1)         0           DotProduct1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Flatten_PhiHyper (Flatten)      (None, 1)            0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Prediction (Dense)              (None, 1)            2           Flatten_PhiHyper[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 44,704,802\n",
      "Trainable params: 800,002\n",
      "Non-trainable params: 43,904,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "projection_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create copy of model but with separate hypo and hyper embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "phi_k = 10\n",
    "max_or_combine = True\n",
    "\n",
    "hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "\n",
    "# lookup word embedding from word index\n",
    "embedding_layer = Embedding(data.embedding_matrix.shape[0], \n",
    "                           data.embedding_matrix.shape[1], name='TermEmbedding',\n",
    "                           embeddings_constraint = UnitNorm(axis=1))\n",
    "\n",
    "\n",
    "hypo_embedding = embedding_layer(hypo_input)  \n",
    "hyper_embedding = embedding_layer(hyper_input)\n",
    "\n",
    "hypo_embedding = Dropout(dropout_rate, name='Dropout_Hypo')(hypo_embedding)\n",
    "hyper_embedding = Dropout(dropout_rate, name='Dropout_Hyper')(hyper_embedding)\n",
    "    \n",
    "phi_layer = []\n",
    "for i in range(phi_k):\n",
    "    phi_layer.append(Dense(data.embedding_matrix.shape[1], activation=None, use_bias=False, \n",
    "                           activity_regularizer=None,\n",
    "                           kernel_initializer=RandomIdentity(),                               \n",
    "                           name='Phi%d' % (i)) (hypo_embedding))            \n",
    "if phi_k == 1:\n",
    "    # flatten tensors\n",
    "    phi = Flatten(name='Flatten_Phi')(phi_layer[0])\n",
    "    hyper_embedding = Flatten(name='Flatten_Hyper')(hyper_embedding)    \n",
    "else:\n",
    "    phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "phi = Dropout(dropout_rate, name='Dropout_Phi')(phi)\n",
    "\n",
    "# this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "phi_hyper = Dot(axes=-1, normalize=True, name='DotProduct1')([phi, hyper_embedding])                    \n",
    "\n",
    "if phi_k > 1:\n",
    "    if max_or_combine:\n",
    "        phi_hyper = Lambda(lambda x: K.mean(x, axis=1, keepdims=True))(phi_hyper)\n",
    "    phi_hyper = Flatten(name='Flatten_PhiHyper')(phi_hyper)\n",
    "\n",
    "predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                    use_bias=True,                    \n",
    "                    kernel_initializer='random_normal',\n",
    "                    kernel_constraint= None,                        \n",
    "                    bias_initializer=Zeros(),                                            \n",
    "                    kernel_regularizer=None,                        \n",
    "                    bias_regularizer=None\n",
    "                   ) (phi_hyper)\n",
    "\n",
    "# create projection model\n",
    "projection_model_fine =  Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "#projection_model_fine.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "#projection_model_fine.get_layer(name='TermEmbedding').trainable = False\n",
    "#adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.9, clipnorm=1.)\n",
    "#projection_model.compile(optimizer = adam, loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write callback that returns MRR at the end of every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class MRRLogger(Callback):\n",
    "    def set_evaluator(self, hypernym_evaluator):\n",
    "        self.he = hypernym_evaluator        \n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.mrr = []\n",
    "        self.map = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # no need to invoke validation data passed into fit function;\n",
    "        # validation data are encapsulated in hypernym_evaluator\n",
    "        self.predictions = self.he.predict_ltr_hypernyms()\n",
    "        _, all_scores = self.he.get_evaluation_scores(self.predictions)        \n",
    "        epoch_mrr = round(sum([score_list[0] for score_list in all_scores]) / len(all_scores), 5)  \n",
    "        epoch_map = round(sum([score_list[1] for score_list in all_scores]) / len(all_scores), 5)  \n",
    "        self.mrr.append(epoch_mrr)\n",
    "        self.map.append(epoch_map)\n",
    "        print (\"; MRR: %.4f; MAP: %.4f\" % (epoch_mrr, epoch_map)) \n",
    "\n",
    "        \n",
    "class BestModelWeightSaver(Callback):\n",
    "    def set_mrr_logger(self, mrr_logger):\n",
    "        self.mrr_logger = mrr_logger\n",
    "        \n",
    "    def set_filepath(self, filepath):\n",
    "        # file path should include placeholders for epoch and validation MRR\n",
    "        self.filepath = filepath\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.best_metric = 0.\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # decide on whether we're going to overwrite weights or ignore this epoch\n",
    "        # because of inferior results\n",
    "        test_metric = np.sqrt(mrr_logger.map[::-1][0] * mrr_logger.mrr[::-1][0])\n",
    "        if test_metric > self.best_metric:\n",
    "            # we have new highest MRR: save weights to disc\n",
    "            self.best_metric = test_metric\n",
    "            np.savez_compressed(self.filepath % (epoch, self.best_metric), self.model.get_weights())            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = np.ones((len(query_seq), 1))\n",
    "v_y = np.ones((len(v_query), 1))\n",
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# get RankNet model and scorer\n",
    "model, get_score = get_RankNet_model(feature_extractor, dropout_rate=0.2)\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                    validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                    batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                    callbacks=[mrr_logger, weight_saver]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run MSE-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare to run MSE model\n",
    "y = [1.] * len(hyper_seq)\n",
    "v_y = [1.] * len(v_hyper_seq)\n",
    "\n",
    "# train model\n",
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_MSE((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "history = projection_model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                               validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run proper MSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare y vectors\n",
    "y = np.zeros((len(hyper_seq),200))\n",
    "for idx, h in enumerate(hyper_seq):\n",
    "    y[idx] = data.embedding_matrix[h[0]]\n",
    "\n",
    "v_y = np.zeros((len(v_hyper_seq),200))\n",
    "for idx, h in enumerate(v_hyper_seq):\n",
    "    v_y[idx] = data.embedding_matrix[h[0]]\n",
    "\n",
    "    \n",
    "# train model\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "he = HypernymEvaluation_MSE((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model.fit([query_seq, neg_seq], y, \n",
    "                               validation_data = ([v_query_seq, v_neg_seq], v_y),\n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CRIM model using general fit function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first prepare training set composed of separate positive and negative instances\n",
    "neg_crim_query = []\n",
    "neg_crim_hyper = []\n",
    "\n",
    "n_negative = 10\n",
    "for q, h in zip(data.train_query, data.train_hyper):        \n",
    "    # mix of random and synonyms\n",
    "    rands = np.random.choice(data.random_words[q], n_negative, replace=False).tolist()            \n",
    "    neg_crim_query.extend([q] * n_negative)    \n",
    "    neg_crim_hyper.extend(rands)\n",
    "                                         \n",
    "query = data.train_query + neg_crim_query\n",
    "hyper = data.train_hyper + neg_crim_hyper\n",
    "y = [1.] * len(data.train_query) + [0.] * len(neg_crim_query)\n",
    "\n",
    "query_seq, hyper_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), [query, hyper])\n",
    "v_query_seq, v_hyper_seq = map(lambda x: data.tokenizer.texts_to_sequences(x), [data.valid_query, data.valid_hyper])\n",
    "\n",
    "v_y = [1.] * len(v_query_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 129569 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "129536/129569 [============================>.] - ETA: 0s - loss: 0.3972; MRR: 0.0772; MAP: 0.0396\n",
      "129569/129569 [==============================] - 75s 583us/step - loss: 0.3972 - val_loss: 1.9780\n",
      "Epoch 2/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.2574; MRR: 0.1175; MAP: 0.0533\n",
      "129569/129569 [==============================] - 72s 559us/step - loss: 0.2574 - val_loss: 1.5933\n",
      "Epoch 3/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.2281; MRR: 0.1332; MAP: 0.0582\n",
      "129569/129569 [==============================] - 73s 564us/step - loss: 0.2282 - val_loss: 1.2448\n",
      "Epoch 4/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.2050; MRR: 0.1207; MAP: 0.0560\n",
      "129569/129569 [==============================] - 74s 573us/step - loss: 0.2049 - val_loss: 1.1213\n",
      "Epoch 5/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1860; MRR: 0.1149; MAP: 0.0513\n",
      "129569/129569 [==============================] - 75s 579us/step - loss: 0.1860 - val_loss: 0.9302\n",
      "Epoch 6/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1695; MRR: 0.1233; MAP: 0.0489\n",
      "129569/129569 [==============================] - 75s 581us/step - loss: 0.1695 - val_loss: 0.8901\n",
      "Epoch 7/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1563; MRR: 0.1140; MAP: 0.0472\n",
      "129569/129569 [==============================] - 76s 584us/step - loss: 0.1563 - val_loss: 0.7906\n",
      "Epoch 8/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1447; MRR: 0.1050; MAP: 0.0467\n",
      "129569/129569 [==============================] - 75s 577us/step - loss: 0.1447 - val_loss: 0.7495\n",
      "Epoch 9/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1353; MRR: 0.1277; MAP: 0.0576\n",
      "129569/129569 [==============================] - 75s 579us/step - loss: 0.1353 - val_loss: 0.7282\n",
      "Epoch 10/10\n",
      "129472/129569 [============================>.] - ETA: 0s - loss: 0.1282; MRR: 0.1315; MAP: 0.0556\n",
      "129569/129569 [==============================] - 79s 607us/step - loss: 0.1283 - val_loss: 0.7198\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model.fit([query_seq, hyper_seq], y, validation_data = ([v_query_seq, v_hyper_seq], v_y), \n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.468273"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(projection_model.get_layer(name='Phi6').get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('models/best_ltr_e%s_mrr%.4f' % (10, 0.1044), projection_model.get_weights())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model)\n",
    "he.predict_ltr_hypernym(['dirham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projection_model_fine.get_layer(name='TermEmbedding').set_weights([data.embedding_matrix])\n",
    "#projection_model_fine.get_layer(name='TermEmbedding_H').set_weights([data.embedding_matrix])\n",
    "\n",
    "projection_model_fine.get_layer(name='TermEmbedding').trainable = True\n",
    "#projection_model_fine.get_layer(name='TermEmbedding_Q').trainable = True\n",
    "\n",
    "# get projection matrices\n",
    "dense = map(lambda x: x.get_weights()[0], [l for l in projection_model.layers if l.name.startswith('Phi')])\n",
    "dense = np.asarray(dense)\n",
    "# get sigmoid weights\n",
    "lr_weights = projection_model.get_layer(name='Prediction').get_weights()\n",
    "    \n",
    "# inject pre-trained embedding weights into Embedding layer\n",
    "        \n",
    "phi_projections = [l for l in projection_model_fine.layers if l.name.startswith('Phi')]    \n",
    "for idx, phi_projection in enumerate(phi_projections):\n",
    "    phi_projection.set_weights([dense[idx]])\n",
    "    phi_projection.trainable = False\n",
    "\n",
    "projection_model_fine.get_layer(name='Prediction').set_weights(lr_weights)\n",
    "projection_model_fine.get_layer(name='Prediction').trainable = True\n",
    "                \n",
    "projection_model_fine.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune model, but keeping Phi frozen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model_fine)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = projection_model_fine.fit([query_seq, hyper_seq], y, validation_data = ([v_query_seq, v_hyper_seq], v_y), \n",
    "                               batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                               callbacks=[mrr_logger, weight_saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RankNet model from already trained feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run ranknet on trained model\n",
    "# first set feature_extractor Phi to non-trainable\n",
    "feature_extractor.get_layer(name='Phi0').trainable = False\n",
    "\n",
    "y = np.ones((len(query_seq), 1))\n",
    "v_y = np.ones((len(v_query), 1))\n",
    "# train model\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# get RankNet model and scorer\n",
    "model, get_score = get_RankNet_model(feature_extractor, dropout_rate=0.2)\n",
    "\n",
    "# initialise MRR callback\n",
    "\n",
    "# initialise evaluator\n",
    "he = HypernymEvaluation_SquareDiff((data.valid_query, data.valid_hyper), data.tokenizer, feature_extractor, get_score)\n",
    "mrr_logger = MRRLogger()\n",
    "mrr_logger.set_evaluator(he)\n",
    "\n",
    "weight_saver = BestModelWeightSaver()\n",
    "weight_saver.set_mrr_logger(mrr_logger)\n",
    "weight_saver.set_filepath('models/best_ltr_e%s_mrr%.4f')\n",
    "\n",
    "\n",
    "history = model.fit([query_seq, hyper_seq, neg_seq], y, \n",
    "                    validation_data = ([v_query_seq, v_hyper_seq, v_neg_seq], v_y),\n",
    "                    batch_size = BATCH_SIZE, epochs = NUM_EPOCHS, verbose = 1,\n",
    "                    callbacks=[mrr_logger, weight_saver]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model from Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load saved weights\n",
    "model_weights = np.load('models/best_ltr_e3_mrr0.1257.npz') \n",
    "model_weights = model_weights['arr_0'].tolist()\n",
    "projection_model.set_weights(model_weights)\n",
    "                  \n",
    "# refresh scorer\n",
    "#get_score = K.function([s_vi], [rel_score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature_extractor.summary()\n",
    "for idx, (loss, val_loss, mrr, mean_prec) in enumerate(zip(history.history['loss'], history.history['val_loss'], mrr_logger.mrr, mrr_logger.map)):\n",
    "    print idx+1, loss, val_loss, mrr, mean_prec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "he = HypernymEvaluation_CRIM_Max((data.valid_query, data.valid_hyper), data.tokenizer, projection_model_fine)\n",
    "he.predict_ltr_hypernym(['editorial']).values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bierstadt': [u'landscape_painting',\n",
       "  u'landscape_architect',\n",
       "  u'natural_landscape',\n",
       "  u'cultural_landscape',\n",
       "  u'work_of_art',\n",
       "  u'landscape_design',\n",
       "  u'sculptor',\n",
       "  u'printmaker',\n",
       "  u'urban_planner',\n",
       "  u'fine_art',\n",
       "  u'craftsman',\n",
       "  u'art_collector',\n",
       "  u'art_historian',\n",
       "  u'painter',\n",
       "  u'folk_art'],\n",
       " u'bloodguilt': [u'person',\n",
       "  u'television',\n",
       "  u'infringement_of_copyright',\n",
       "  u'tv_program',\n",
       "  u'television_production',\n",
       "  u'television_program',\n",
       "  u'mass_media',\n",
       "  u'good_person',\n",
       "  u'film',\n",
       "  u'truthfulness',\n",
       "  u'film_criticism',\n",
       "  u'court_of_law',\n",
       "  u'history_of_film',\n",
       "  u'media_studies',\n",
       "  u'bad_person'],\n",
       " u'boatlift': [u'sense_of_responsibility',\n",
       "  u'dramatic_art',\n",
       "  u'world_affairs',\n",
       "  u'literary_movement',\n",
       "  u'political_activist',\n",
       "  u'playwright',\n",
       "  u'journalism',\n",
       "  u'betterment',\n",
       "  u'drama',\n",
       "  u'yaro',\n",
       "  u'dramatist',\n",
       "  u'citizens',\n",
       "  u'popular_culture',\n",
       "  u'professional_life',\n",
       "  u'urban_planning'],\n",
       " u'burger_king': [u'prepackaged',\n",
       "  u'distributor',\n",
       "  u'retail_store',\n",
       "  u'retail_outlet',\n",
       "  u'promotional',\n",
       "  u'fast_food',\n",
       "  u'fast-food',\n",
       "  u'frozen_food',\n",
       "  u'retail',\n",
       "  u'grocery',\n",
       "  u'bakery',\n",
       "  u'grocery_store',\n",
       "  u'distributing',\n",
       "  u'product',\n",
       "  u'vending_machine'],\n",
       " u'burgh': [u'parcel_of_land',\n",
       "  u'public_building',\n",
       "  u'seat_of_government',\n",
       "  u'piece_of_land',\n",
       "  u'municipal_government',\n",
       "  u'residential_district',\n",
       "  u'the_city',\n",
       "  u'township',\n",
       "  u'building_site',\n",
       "  u'independent_city',\n",
       "  u'city_block',\n",
       "  u'city',\n",
       "  u'government_office',\n",
       "  u'borough',\n",
       "  u'owner'],\n",
       " u'calmodulin': [u'molecule',\n",
       "  u'molecular',\n",
       "  u'living_systems',\n",
       "  u'chemical_reaction',\n",
       "  u'nucleic',\n",
       "  u'electrical_circuit',\n",
       "  u'cellular',\n",
       "  u'macromolecular',\n",
       "  u'biomolecular',\n",
       "  u'biomolecule',\n",
       "  u'protein_structure',\n",
       "  u'chemical_bond',\n",
       "  u'structural_biology',\n",
       "  u'biochemical',\n",
       "  u'interfacing'],\n",
       " u'catechumen': [u'sainthood',\n",
       "  u'religious_leader',\n",
       "  u'spiritual',\n",
       "  u'devout',\n",
       "  u'person',\n",
       "  u'celibacy',\n",
       "  u'good_person',\n",
       "  u'religion',\n",
       "  u'nun',\n",
       "  u'priest',\n",
       "  u'religious',\n",
       "  u'religious_conversion',\n",
       "  u'religious_order',\n",
       "  u'saintly',\n",
       "  u'celibate'],\n",
       " u'cingulate_gyrus': [u'human_body',\n",
       "  u'living_systems',\n",
       "  u'human_physiology',\n",
       "  u'nervous_system',\n",
       "  u'human_brain',\n",
       "  u'brain',\n",
       "  u'bodily',\n",
       "  u'mental',\n",
       "  u'neurophysiology',\n",
       "  u'information_processing_system',\n",
       "  u'mind-body',\n",
       "  u'physical',\n",
       "  u'circuitry',\n",
       "  u'sensory_system',\n",
       "  u'metabolism'],\n",
       " u'cloister': [u'living_quarters',\n",
       "  u'place_of_worship',\n",
       "  u'demolished',\n",
       "  u'public_building',\n",
       "  u'mausoleum',\n",
       "  u'cloister',\n",
       "  u'floor_plan',\n",
       "  u'multi-story',\n",
       "  u'house_of_worship',\n",
       "  u'religious_service',\n",
       "  u'depicting',\n",
       "  u'building_site',\n",
       "  u'palace',\n",
       "  u'worship',\n",
       "  u'demolition'],\n",
       " u'cromford_canal': [u'roads',\n",
       "  u'waterway',\n",
       "  u'building_site',\n",
       "  u'body_of_water',\n",
       "  u'construction_work',\n",
       "  u'construction_site',\n",
       "  u'local_road',\n",
       "  u'rail_transportation',\n",
       "  u'roadway',\n",
       "  u'right-of-way',\n",
       "  u'transportation',\n",
       "  u'navigable',\n",
       "  u'vehicle_traffic',\n",
       "  u'transportation_system',\n",
       "  u'vehicular_traffic'],\n",
       " u'dirham': [u'legal_tender',\n",
       "  u'public_treasury',\n",
       "  u'bullion',\n",
       "  u'sum_of_money',\n",
       "  u'monetary_unit',\n",
       "  u'the_treasury',\n",
       "  u'paper_money',\n",
       "  u'fiat_money',\n",
       "  u'gold_coin',\n",
       "  u'silver_coin',\n",
       "  u'paper_currency',\n",
       "  u'dirham',\n",
       "  u'remuneration',\n",
       "  u'natural_person',\n",
       "  u'minister_of_finance'],\n",
       " u'don_mckellar': [u'actor',\n",
       "  u'scriptwriter',\n",
       "  u'entertainer',\n",
       "  u'playwright',\n",
       "  u'film_producer',\n",
       "  u'entertainment_industry',\n",
       "  u'film_director',\n",
       "  u'feature_film',\n",
       "  u'filmmaker',\n",
       "  u'screenplay',\n",
       "  u'production_company',\n",
       "  u'television_producer',\n",
       "  u'film_industry',\n",
       "  u'screenwriter',\n",
       "  u'comedian'],\n",
       " u'dragonfly': [u'animal',\n",
       "  u'bird',\n",
       "  u'mammal',\n",
       "  u'body_structure',\n",
       "  u'reptiles',\n",
       "  u'aquatic',\n",
       "  u'computer_graphic',\n",
       "  u'birds',\n",
       "  u'reptile',\n",
       "  u'wild_animal',\n",
       "  u'marine_animal',\n",
       "  u'nonliving',\n",
       "  u'zoological',\n",
       "  u'animal_group',\n",
       "  u'scientific_classification'],\n",
       " u'drug_of_abuse': [u'biopsychosocial',\n",
       "  u'mental',\n",
       "  u'information_processor',\n",
       "  u'physical',\n",
       "  u'information_processing',\n",
       "  u'medical_model',\n",
       "  u'medical_science',\n",
       "  u'practice_of_medicine',\n",
       "  u'psychoactive_substance',\n",
       "  u'predisposition',\n",
       "  u'scientific_discipline',\n",
       "  u'medical_practice',\n",
       "  u'interrelate',\n",
       "  u'mental_disease',\n",
       "  u'psychotropic'],\n",
       " u'emmeline_pankhurst': [u'person',\n",
       "  u'public_servant',\n",
       "  u'delegate',\n",
       "  u'educational_institution',\n",
       "  u'elected_official',\n",
       "  u'political_activist',\n",
       "  u'member',\n",
       "  u'important_person',\n",
       "  u'respected',\n",
       "  u'educator',\n",
       "  u'honorary',\n",
       "  u'official',\n",
       "  u'honorarium',\n",
       "  u'parliamentarian',\n",
       "  u'professional_person'],\n",
       " u'entente': [u'governing',\n",
       "  u'military_organization',\n",
       "  u'government',\n",
       "  u'armed_forces',\n",
       "  u'respect',\n",
       "  u'memorandum_of_understanding',\n",
       "  u'delegation',\n",
       "  u'govern',\n",
       "  u'united_states_government',\n",
       "  u'country',\n",
       "  u'respective',\n",
       "  u'establish',\n",
       "  u'institutionalize',\n",
       "  u'civil_authority',\n",
       "  u'reaffirm'],\n",
       " u'fertility_rate': [u'calculating',\n",
       "  u'denominator',\n",
       "  u'computed',\n",
       "  u'statistics',\n",
       "  u'calculate',\n",
       "  u'calculation',\n",
       "  u'corresponding',\n",
       "  u'numerator',\n",
       "  u'total_fertility_rate',\n",
       "  u'equivalent',\n",
       "  u'defined',\n",
       "  u'i.e.',\n",
       "  u'basic',\n",
       "  u'central_tendency',\n",
       "  u'population_growth'],\n",
       " u'funen': [u'city',\n",
       "  u'roads',\n",
       "  u'territory',\n",
       "  u'town',\n",
       "  u'city_limits',\n",
       "  u'new_town',\n",
       "  u'body_of_water',\n",
       "  u'construction_work',\n",
       "  u'scenic',\n",
       "  u'traveling',\n",
       "  u'means_of_transportation',\n",
       "  u'birthplace',\n",
       "  u'travel_by',\n",
       "  u'public_road',\n",
       "  u'the_city'],\n",
       " u'gamesmanship': [u'motivating',\n",
       "  u'public_office',\n",
       "  u'motivate',\n",
       "  u'political_party',\n",
       "  u'value_system',\n",
       "  u'respect',\n",
       "  u'accountable',\n",
       "  u'political_arena',\n",
       "  u'social_environment',\n",
       "  u'role_model',\n",
       "  u'political_contribution',\n",
       "  u'necessarily',\n",
       "  u'elected_official',\n",
       "  u'decision_maker',\n",
       "  u'self-confidence'],\n",
       " u'griselda': [u'female',\n",
       "  u'woman',\n",
       "  u'male',\n",
       "  u'young_girl',\n",
       "  u'girl',\n",
       "  u'mary_jane',\n",
       "  u'adult_male',\n",
       "  u'taxidermist',\n",
       "  u'important_person',\n",
       "  u'person',\n",
       "  u'apprentice',\n",
       "  u'adult',\n",
       "  u'laborer',\n",
       "  u'prostitute',\n",
       "  u'bird'],\n",
       " u'heming': [u'nominator',\n",
       "  u'camera_operator',\n",
       "  u'videographer',\n",
       "  u'photographer',\n",
       "  u'photograph',\n",
       "  u'meteorologist',\n",
       "  u'professional_person',\n",
       "  u'title_page',\n",
       "  u'bylined',\n",
       "  u'researcher',\n",
       "  u'covering_letter',\n",
       "  u'faculty_member',\n",
       "  u'reviewer',\n",
       "  u'publisher',\n",
       "  u'investigator'],\n",
       " u'honorable_discharge': [u'military_service',\n",
       "  u'armed_forces',\n",
       "  u'duty_assignment',\n",
       "  u'military_unit',\n",
       "  u'stipulate',\n",
       "  u'honorable_discharge',\n",
       "  u'responsibility',\n",
       "  u'military_installation',\n",
       "  u'official_document',\n",
       "  u'entitled',\n",
       "  u'branch_of_service',\n",
       "  u'accordance',\n",
       "  u'statutorily',\n",
       "  u'family_unit',\n",
       "  u'obligated'],\n",
       " u'hurricane_iniki': [u'the_storm',\n",
       "  u'thunderstorm',\n",
       "  u'natural_event',\n",
       "  u'winter_storm',\n",
       "  u'high_wind',\n",
       "  u'torrential',\n",
       "  u'tornado',\n",
       "  u'storm_surge',\n",
       "  u'extreme_weather',\n",
       "  u'landfall',\n",
       "  u'tropical_storm',\n",
       "  u'windstorm',\n",
       "  u'property_damage',\n",
       "  u'storm',\n",
       "  u'miles_per_hour'],\n",
       " u'in-joke': [u'music_video',\n",
       "  u'comic_book',\n",
       "  u'comics',\n",
       "  u'pop_culture',\n",
       "  u'live-action',\n",
       "  u'movie',\n",
       "  u'tv_show',\n",
       "  u'live_action',\n",
       "  u'films',\n",
       "  u'cartoon',\n",
       "  u'relatable',\n",
       "  u'film',\n",
       "  u'video_game',\n",
       "  u'computer_game',\n",
       "  u'anime'],\n",
       " u'jurmo': [u'cultural_landscape',\n",
       "  u'teacher',\n",
       "  u'geographer',\n",
       "  u'natural_landscape',\n",
       "  u'geographical',\n",
       "  u'educator',\n",
       "  u'learner',\n",
       "  u'concept_map',\n",
       "  u'classroom',\n",
       "  u'lesson_plan',\n",
       "  u'thematic_map',\n",
       "  u'convey',\n",
       "  u'classroom_project',\n",
       "  u'assigning',\n",
       "  u'graphic_organizer'],\n",
       " u'konstantin_tsiolkovsky': [u'person',\n",
       "  u'educational_institution',\n",
       "  u'mechanical_engineer',\n",
       "  u'aerospace_engineering',\n",
       "  u'mechanical_engineering',\n",
       "  u'engineer',\n",
       "  u'physicist',\n",
       "  u'engineering_school',\n",
       "  u'engineering',\n",
       "  u'astronautics',\n",
       "  u'communication_device',\n",
       "  u'rocket_propulsion',\n",
       "  u'mechatronics',\n",
       "  u'mechanics',\n",
       "  u'physics'],\n",
       " u'kralendijk': [u'body_of_water',\n",
       "  u'mountain_range',\n",
       "  u'vehicle_traffic',\n",
       "  u'waterway',\n",
       "  u'overland',\n",
       "  u'roadway',\n",
       "  u'roads',\n",
       "  u'vehicular_traffic',\n",
       "  u'natural_landscape',\n",
       "  u'road_vehicle',\n",
       "  u'corridor',\n",
       "  u'kilometre',\n",
       "  u'mineral_extraction',\n",
       "  u'shoreline',\n",
       "  u'drainage_basin'],\n",
       " u'lemongrass': [u'edible',\n",
       "  u'plant_part',\n",
       "  u'vegetable',\n",
       "  u'flavoring',\n",
       "  u'stevia',\n",
       "  u'nutritive',\n",
       "  u'starches',\n",
       "  u'plant',\n",
       "  u'medicinal',\n",
       "  u'starch',\n",
       "  u'condiment',\n",
       "  u'fermented',\n",
       "  u'juices',\n",
       "  u'plant_food',\n",
       "  u'quinoa'],\n",
       " u'marquee': [u'computer_graphic',\n",
       "  u'artwork',\n",
       "  u'musical_group',\n",
       "  u'musical_performance',\n",
       "  u'video_production',\n",
       "  u'visual_art',\n",
       "  u'graphic_design',\n",
       "  u'computer_software',\n",
       "  u'logo',\n",
       "  u'commercial_art',\n",
       "  u'television_production',\n",
       "  u'performing_arts',\n",
       "  u'billboard',\n",
       "  u'graphic',\n",
       "  u'mural'],\n",
       " u'matthew_murphy': [u'person',\n",
       "  u'computer_system',\n",
       "  u'e-mailing',\n",
       "  u'software_program',\n",
       "  u'uploading',\n",
       "  u'copy',\n",
       "  u'computer_user',\n",
       "  u'full_name',\n",
       "  u'e-mail_address',\n",
       "  u'faxed',\n",
       "  u'psychologist',\n",
       "  u'administrator',\n",
       "  u'assumed_name',\n",
       "  u'service_provider',\n",
       "  u'downloaded'],\n",
       " u'minnamurra': [u'body_of_water',\n",
       "  u'natural_landscape',\n",
       "  u'park',\n",
       "  u'building_site',\n",
       "  u'roads',\n",
       "  u'shoreline',\n",
       "  u'roadway',\n",
       "  u'waterway',\n",
       "  u'mountain_range',\n",
       "  u'parcel_of_land',\n",
       "  u'flood_plain',\n",
       "  u'topography',\n",
       "  u'public_park',\n",
       "  u'drainage_basin',\n",
       "  u'scenic'],\n",
       " u'neon_genesis_evangelion': [u'interactive_entertainment',\n",
       "  u'video_game',\n",
       "  u'computer_game',\n",
       "  u'edutainment',\n",
       "  u'entertainment_industry',\n",
       "  u'film_making',\n",
       "  u'interactive_media',\n",
       "  u'game_industry',\n",
       "  u'game_design',\n",
       "  u'television_production',\n",
       "  u'live-action',\n",
       "  u'video_game_development',\n",
       "  u'electronic_game',\n",
       "  u'live_action',\n",
       "  u'movie_making'],\n",
       " u'nsanje': [u'west-central',\n",
       "  u'body_of_water',\n",
       "  u'drainage_basin',\n",
       "  u'rivers',\n",
       "  u'south-central',\n",
       "  u'floodwaters',\n",
       "  u'lahar',\n",
       "  u'flooding',\n",
       "  u'rainfall',\n",
       "  u'geographical',\n",
       "  u'roads',\n",
       "  u'farming_area',\n",
       "  u'river_catchment',\n",
       "  u'water_supply',\n",
       "  u'mountainous'],\n",
       " u'palisade': [u'city_block',\n",
       "  u'building_site',\n",
       "  u'multi-story',\n",
       "  u'earthworks',\n",
       "  u'public_building',\n",
       "  u'perimeter',\n",
       "  u'earthwork',\n",
       "  u'floor_plan',\n",
       "  u'adjoining',\n",
       "  u'square',\n",
       "  u'vacant_lot',\n",
       "  u'public_park',\n",
       "  u'demolition',\n",
       "  u'square_block',\n",
       "  u'construction_site'],\n",
       " u'pinny': [u'bodybuilder',\n",
       "  u'athlete',\n",
       "  u'role_model',\n",
       "  u'substitute_teacher',\n",
       "  u'gymnast',\n",
       "  u'videographer',\n",
       "  u'soccer_player',\n",
       "  u'construction_worker',\n",
       "  u'basketball_player',\n",
       "  u'actor',\n",
       "  u'production_company',\n",
       "  u'trainer',\n",
       "  u'skin_color',\n",
       "  u'movie_star',\n",
       "  u'laborer'],\n",
       " u'pinot_blanc': [u'plant',\n",
       "  u'plant_part',\n",
       "  u'grape',\n",
       "  u'varietal',\n",
       "  u'chardonnay',\n",
       "  u'plant_family',\n",
       "  u'red_raspberry',\n",
       "  u'ornamental_plant',\n",
       "  u'edible',\n",
       "  u'cabernet_sauvignon',\n",
       "  u'pinot_noir',\n",
       "  u'succulent',\n",
       "  u'viticulture',\n",
       "  u'horticulturally',\n",
       "  u'wine_maker'],\n",
       " u'polysemy': [u'human_language',\n",
       "  u'conceptual',\n",
       "  u'concepts',\n",
       "  u'animal_communication',\n",
       "  u'lexical_semantics',\n",
       "  u'cognitive_psychology',\n",
       "  u'cognitive_science',\n",
       "  u'as_language',\n",
       "  u'metalinguistic',\n",
       "  u'general_knowledge',\n",
       "  u'grammatical',\n",
       "  u'mental_representation',\n",
       "  u'psycholinguistics',\n",
       "  u'abstract_thought',\n",
       "  u'word_formation'],\n",
       " u'quebec': [u'territory',\n",
       "  u'territorial',\n",
       "  u'statehood',\n",
       "  u'administrative_region',\n",
       "  u'self-governing',\n",
       "  u'local_government',\n",
       "  u'locale',\n",
       "  u'unorganized_territory',\n",
       "  u'cultural_landscape',\n",
       "  u'state',\n",
       "  u'legal_entity',\n",
       "  u'provincial',\n",
       "  u'geographical',\n",
       "  u'territorially',\n",
       "  u'tribal'],\n",
       " u'relcom': [u'telecommunication',\n",
       "  u'computer_networks',\n",
       "  u'data_communication',\n",
       "  u'local_area_network',\n",
       "  u'computer_network',\n",
       "  u'input/output',\n",
       "  u'computer_networking',\n",
       "  u'telecommunications_network',\n",
       "  u'graphical_user_interface',\n",
       "  u'data_processing',\n",
       "  u'telecommunication_equipment',\n",
       "  u'computer',\n",
       "  u'communications_technology',\n",
       "  u'personal_computer',\n",
       "  u'data_transmission'],\n",
       " u'roundhouse': [u'construction_work',\n",
       "  u'public_building',\n",
       "  u'building_site',\n",
       "  u'adaptive_reuse',\n",
       "  u'demolition',\n",
       "  u'construction_site',\n",
       "  u'public_park',\n",
       "  u'amusement_ride',\n",
       "  u'cultural_landscape',\n",
       "  u'public_art',\n",
       "  u'roundhouse',\n",
       "  u'asbestos_abatement',\n",
       "  u'demolished',\n",
       "  u'city_block',\n",
       "  u'fairground'],\n",
       " u'sedative': [u'self-medicate',\n",
       "  u'self-medication',\n",
       "  u'drug',\n",
       "  u'pain',\n",
       "  u'therapeutic',\n",
       "  u'nondrug',\n",
       "  u'predisposition',\n",
       "  u'medical',\n",
       "  u'doctor',\n",
       "  u'chronic_pain',\n",
       "  u'medicine',\n",
       "  u'illness',\n",
       "  u'ailment',\n",
       "  u'alertness',\n",
       "  u'painkillers'],\n",
       " u'shambles': [u'law_and_order',\n",
       "  u'responsibility',\n",
       "  u'business_establishment',\n",
       "  u'respect',\n",
       "  u'regard',\n",
       "  u'laws',\n",
       "  u'sense_of_responsibility',\n",
       "  u'minister',\n",
       "  u'value_system',\n",
       "  u'musical_work',\n",
       "  u'legislations',\n",
       "  u'so_long',\n",
       "  u'happening',\n",
       "  u'bring_about',\n",
       "  u'necessarily'],\n",
       " u'shellac': [u'manufacture',\n",
       "  u'manufacturing_process',\n",
       "  u'manufactured',\n",
       "  u'product',\n",
       "  u'end_product',\n",
       "  u'packaging',\n",
       "  u'plastics',\n",
       "  u'label',\n",
       "  u'manufacturer',\n",
       "  u'labels',\n",
       "  u'raw_material',\n",
       "  u'chemical_process',\n",
       "  u'embossed',\n",
       "  u'musical_composition',\n",
       "  u'plastics_industry'],\n",
       " u'showroom': [u'retail_store',\n",
       "  u'retail_outlet',\n",
       "  u'showroom',\n",
       "  u'retail_chain',\n",
       "  u'telecommunication_equipment',\n",
       "  u'manufacturing_plant',\n",
       "  u'office_furniture',\n",
       "  u'distributor',\n",
       "  u'resale',\n",
       "  u'retailer',\n",
       "  u'business_establishment',\n",
       "  u'warehouse',\n",
       "  u'retail',\n",
       "  u'manufacturing',\n",
       "  u'selling'],\n",
       " u'sociology_department': [u'college',\n",
       "  u'university',\n",
       "  u'academic',\n",
       "  u'curriculum',\n",
       "  u'student',\n",
       "  u'educational_program',\n",
       "  u'department',\n",
       "  u'advisory_board',\n",
       "  u'school',\n",
       "  u'mission_statement',\n",
       "  u'educator',\n",
       "  u'committee',\n",
       "  u'society',\n",
       "  u'educational_institution',\n",
       "  u'honors'],\n",
       " u'starcraft': [u'computer_game',\n",
       "  u'video_game',\n",
       "  u'interactive_entertainment',\n",
       "  u'computer',\n",
       "  u'video_game_development',\n",
       "  u'interactive',\n",
       "  u'online_game',\n",
       "  u'edutainment',\n",
       "  u'software_application',\n",
       "  u'computer_software',\n",
       "  u'computer_hardware',\n",
       "  u'software',\n",
       "  u'video',\n",
       "  u'personal_computer',\n",
       "  u'computer_network'],\n",
       " u'tahini': [u'edible',\n",
       "  u'food_product',\n",
       "  u'foodstuff',\n",
       "  u'vegetable',\n",
       "  u'starches',\n",
       "  u'flavoring',\n",
       "  u'nutritive',\n",
       "  u'plant',\n",
       "  u'starch',\n",
       "  u'juices',\n",
       "  u'fermented',\n",
       "  u'yoghurt',\n",
       "  u'plant_part',\n",
       "  u'cereal',\n",
       "  u'potato'],\n",
       " u'tim_tebow': [u'athlete',\n",
       "  u'athletes',\n",
       "  u'basketball_player',\n",
       "  u'team_sport',\n",
       "  u'hockey_player',\n",
       "  u'sport',\n",
       "  u'conditioning',\n",
       "  u'student_athlete',\n",
       "  u'weightlifting',\n",
       "  u'bodybuilding',\n",
       "  u'gymnast',\n",
       "  u'tennis_player',\n",
       "  u'sports',\n",
       "  u'tennis_ball',\n",
       "  u'type_of_sport'],\n",
       " u'timepiece': [u'musical_composition',\n",
       "  u'game_designer',\n",
       "  u'musical_instrument',\n",
       "  u'piece_of_music',\n",
       "  u'multimedia_system',\n",
       "  u'computer_game',\n",
       "  u'animator',\n",
       "  u'digital_audio_workstation',\n",
       "  u'piano_keyboard',\n",
       "  u'software_program',\n",
       "  u'video_game',\n",
       "  u'musician',\n",
       "  u'musical_performance',\n",
       "  u'animation',\n",
       "  u'computer_processor'],\n",
       " u'tussle': [u'sport',\n",
       "  u'athlete',\n",
       "  u'team_sport',\n",
       "  u'actor',\n",
       "  u'wrestler',\n",
       "  u'sports',\n",
       "  u'professional_boxing',\n",
       "  u'skateboarding',\n",
       "  u'sportsmanship',\n",
       "  u'olympic_sports',\n",
       "  u'gymnastics',\n",
       "  u'athletes',\n",
       "  u'dancer',\n",
       "  u'gymnast',\n",
       "  u'soccer_player']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr_logger.predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "('Done', 100)\n",
      "('Done', 200)\n",
      "('Done', 300)\n",
      "('Done', 400)\n",
      "('Done', 500)\n",
      "('Done', 600)\n",
      "('Done', 700)\n",
      "('Done', 800)\n",
      "('Done', 900)\n",
      "('Done', 1000)\n",
      "('Done', 1100)\n",
      "('Done', 1200)\n",
      "('Done', 1300)\n",
      "('Done', 1400)\n",
      "CRIM evaluation:\n",
      "MRR: 0.16373\n",
      "MAP: 0.05272\n",
      "P@1: 0.10474\n",
      "P@5: 0.05724\n",
      "P@10: 0.04083\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "print (\"Generating predictions...\")\n",
    "\n",
    "he_test = HypernymEvaluation_CRIM_Max((data.test_query, data.test_hyper), data.tokenizer, projection_model)\n",
    "ltr_predictions = he_test.predict_ltr_hypernyms()\n",
    "\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = he_test.get_evaluation_scores(ltr_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print projection_model.test_on_batch([query_seq[0], neg_seq[0]], y[0].reshape(1,-1))\n",
    "print np.sum(np.square(projection_model.predict([query_seq[0], neg_seq[0]]) - y[0]))\n",
    "phi = projection_model.get_layer(name='Phi').get_weights()[0]\n",
    "\n",
    "proj = np.dot(data.embedding_matrix[query_seq[0][0]], phi)\n",
    "negative_ex =  data.embedding_matrix[neg_seq[0][0]]\n",
    "pos_ex = y[0]\n",
    "\n",
    "print np.sum(np.square(proj - pos_ex))\n",
    "print np.sum(np.square(proj - negative_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = data.tokenizer.word_index['dog']\n",
    "b = data.tokenizer.word_index['animal']\n",
    "proj = np.dot(data.embedding_matrix[a], phi)\n",
    "\n",
    "\n",
    "np.sum(np.square(proj - data.embedding_matrix[a]))\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(proj[np.newaxis,:], data.embedding_matrix[b][np.newaxis,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi_matrix = [l.get_weights()[0] for l in projection_model.layers if type(l) == Dense and l.name.startswith('Phi') ]\n",
    "phi_matrix = np.asarray(phi_matrix)\n",
    "\n",
    "word = data.embedding_matrix[data.tokenizer.word_index['rod_laver']]\n",
    "pos = data.embedding_matrix[data.tokenizer.word_index['athlete']]\n",
    "neg = data.embedding_matrix[data.tokenizer.word_index['tennis_player']]\n",
    "\n",
    "cluster_weight = projection_model.get_layer(name='Prediction').get_weights()[0]\n",
    "bias = projection_model.get_layer(name='Prediction').get_weights()[1]\n",
    "\n",
    "#proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proj = np.dot(word, phi_matrix)\n",
    "proj = np.mean(proj, axis=0)\n",
    "\n",
    "proj /= np.linalg.norm(proj)\n",
    "\n",
    "print np.dot(proj, neg) * cluster_weight + bias\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
